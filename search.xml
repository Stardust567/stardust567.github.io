<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[被动读写锁]]></title>
    <url>%2Fpost%2F9271.html</url>
    <content type="text"><![CDATA[Scalable Read-mostly Synchronization Using Passive Reader-Writer Locks 被动读写锁 prwlock 为TSO架构提供了可扩展的读者侧性能以及较小的写入器延迟。 prwlock的关键是多个非通信读者和未决写者之间的基于版本的共识协议。 Prwlock 利用内存一致性的有限陈旧性来避免阅读器通用路径中的原子指令和内存障碍，并使用消息传递（例如IPI）来散布读者，以便可以限制写锁获取延迟。 背景知识读写锁 rwlockrwlock 把对共享资源的访问者划分成读者和写者，读者只对共享资源进行读访问，写者则需要对共享资源进行写操作。这种锁能提高并发性，因为在多处理器系统中，它允许同时有多个读者来访问共享资源，最大可能的读者数为实际的逻辑CPU数。写者是排他性的，一个读写锁同时只能有一个写者或多个读者，但不能同时既有读者又有写者。 如果读写锁当前没有读者，也没有写者，那么写者可以立刻获得读写锁，否则它必须自旋在那里，直到没有任何写者或读者。如果读写锁没有写者，那么读者可以立即获得该读写锁，否则读者必须自旋在那里，直到写者释放该读写锁。 大读者锁 brlock大读者锁是读写锁的高性能版，读者可以非常快地获得锁，但写者获得锁的开销比较大。这种锁适合于读多写少的情况，它在这种情况下远好于读写锁。大读者锁的实现机制有两种： brlock1：每个线程都有互斥锁，一个读者仅需要获取本线程锁，但写者需要获得所有锁 brlock2：使用一个由读者和写者共享的读者flags数组 当一个线程被抢占然后移到另一个core的时候，brlock容易发生死锁，所以brlock常用在禁止抢占的环境中。 设计框架设计原理rwlock 的基本设计目标是，读者写者同时进行，因此不应共享任何内容。但典型的rwlock依赖于原子指令来协调读者和写者。在多处理器上，一条原子指令暗含了一次内存屏障，这样可以保证读者读到的是最后一位写者的最新数据。但是，当不存在任何写程序时，这样的内存屏障是不必要的，读程序之间没有内存顺序依赖性，此类不必要的内存屏障可能会导致大量开销。 Message passing is not prohibitively expensive: Bounded staleness without memory barriers：在rwlock中，一个写者需要与所有读者达成 consensus 才能获取该锁。因此，写者必须让所有读者看到其当前状态才能继续进行。典型的rwlock使用显式内存屏障或等待屏障以确保读者写者的版本更新按顺序对彼此可见。但这样要么需要昂贵的内存屏障来限制读者的可伸缩性，要么大幅延长写者的延迟时间（例如，等待宽限期）。一般处理器中如x86-64，通常在很短的时间内其他内核就可以看到多个内存更新。基于此采取微基准测试，重复写入一个内存位置，并在随机延迟后读取该位置on another core。然后收集看到过时值的读者的时间间隔，发现大多数读者可以在很短的时间内看到写者的更新。这是因为处理器由于其有限的大小而将主动刷新其存储缓冲区。合理的做法是，仅等待一小段时间，直到读者看到常见情况的更新版本，同时使用重量稍大的机制来确保正确性。 Memory barrier not essential for mutual exclusion:现代处理器为了性能考虑不会按序执行指令，存在一个存储缓冲区，以使处理器在write cache未命中后继续执行，但这也导致了较弱的内存一致性。为了实现正确的互斥，通常使用性能开销较大的同步机制（例如内存屏障）来对管道进行序列化并刷新存储缓冲区。事实证明，在TSO机器上的所有执行中，都不可能构建一种满足互斥，无死锁且避免原子指令和内存屏障的算法。尽管prwlock读者不包含显式的内存屏障，因此似乎违反了该“顺序法则”，但prwlock使用 IPI 序列化了针对写者的读者执行，并且IPI处理具有与内存屏障相同的效果。 IPI 处理器间中断（Inter-Processor Interrupt）是种特殊类型的中断，即在多核系统中，允许一个CPU向系统其他的CPU发送中断信号，可能要求采取的行动：刷新其它CPU的内存管理单元缓存、停机（当系统被一个处理器关闭时） 基本设计Consensus using bounded staleness: Prwlock 将 64-bit version variable（ver）引入锁结构。每个写者都会增加version，并等待所有读者看到该version，写者会等待所有读者都看到了新的version之后才继续写。 这样可以确保具有总存储顺序（TSO）一致性的计算机上正确的rwlock语义，因为只有在所有先前的内存操作可见后才能看到某个内存存储。但是，这种方法仍然存在一些问题。 首先，如果假定的读者再也不会进入读取侧临界区，那么写者可能永远无法进入写入侧临界区。 其次，读者可能会从一个core迁移到另一个core，因此可能不会更新离开的core。 因此，这种方法可能导致写入侧任意长的等待时间甚至饥饿。 Handling straggling readers: 为解决上述问题，prwlock引入了基于消息的共识协议，以使写者在必要时主动向读者发送共识请求。这种设计是受当代处理器中消息传递的相对较低成本的推动。因此，prwlock使用IPI来请求散乱的读者立即报告其状态。但如果允许读者在读取侧临界区中睡眠的话，睡眠状态的读者可能会错过共识请求，因此可能会无限阻塞写者。支持睡眠阅读器：为了解决睡眠阅读器问题，prwlock通过将上述机制与传统的基于计数器的rwlocks结合使用了一种混合设计。 Supporting sleeping readers: 为了解决读者睡眠问题，prwlock通过将上述机制与传统的基于计数器的rwlocks结合使用了一种混合设计。Prwlock跟踪两种类型的读者：被动读者和主动读者。读者开始时为passive并且不与其他同步，因此不需要任何存储障碍。读者在睡眠前会从passive转换为active。在此转换期间，共享的counter会自增。当active读者释放其锁定后，counter自减。像传统的rwlocks一样，写者使用该counter来确定是否有任何活动的读者。由于在读取器侧临界区睡眠很少，因此prwlock在通常情况下具有良好的性能，且在有读者睡眠的罕见情况下仍保持正确性。 算法设计为了简化说明，我们假设这些功能中只有一个锁，并且禁止抢占，以便它们可以安全地使用每个CPU状态。 Read-side algorithmpassive读者由每个内核读取器状态结构（st）进行分布式跟踪，该结构会记住最新查看的version以及每个内核上prwlock的被动状态。读者应在检查写者锁之前，将状态设置为PASSIVE，否则会存在一个时间窗口，读者明明观测到写者锁处于FREE，但并不能获得读者锁。如果在此时间窗口中传递了共识消息（例如IPI），则写者将成功获取锁并进入临界区，这将违反rwlock的语义。如果读者发现此锁已被写者锁定，则应将其状态重新设置为FREE，等待写者解锁并重试。根据预期的写者持续时间，prwlock可以选择旋转写者状态，也可以使当前线程进入睡眠态。在后一种情况下，阅读器性能很大程度上取决于唤醒机制。如果读者在被调度时正在PASSIVE模式持有锁，则应通过增加主动计数器（ScheduleOut）将其转换为ACTIVE锁。要解锁读者锁，只需检查该锁是否处于PASSIVE并相应地将其解锁（ReadUnlock）。因此，在TSO体系结构上的读者公共路径中，不需要原子指令/内存障碍。此外，只要读者保持PASSIVE，它们就不会相互通信，从而保证了读者的可伸缩性和较低的延迟。 12345678910Function ReadLock(lock)st ← PerCorePtr(lock.rstatus, CoreID); st.reader ← PASSIVE; while lock.writer != FREE do &#123; st.reader ← FREE; st.version ← lock.version; WaitUntil(lock.writer == FREE); st ← PerCorePtr(lock.rstatus, CoreID); st.reader ← PASSIVE; &#125; /* Barrier needed here on non-TSO architecture */; 123456Function ReadUnlock(lock) st ← PerCorePtr(lock.rstatus, CoreID);if st.reader = PASSIVE then st.reader ← FREE;else AtomicDec(lock.active);/* Barrier needed here on non-TSO architecture */;st.version ← lock.version; 12345Function ScheduleOut(lock)st ← PerCorePtr(lock.rstatus, CoreID); if st.reader = PASSIVE then AtomicInc(lock.active);st.reader ← FREE;st.version ← lock.version; Write-side algorithm写者锁的获取可分为两个阶段。一个写者首先锁定writer mutex，然后增加version以进入阶段1（第6-20行）。然后，它将检查当前域中的所有联机cores，以检查cores是否已查看最新version。如果是这样，则意味着读者知道写者的意图，并且在写者释放锁之前不会获得读者锁。对于看不到最新version的内核，写者发送IPI并询问其状态。收到IPI后，未锁定的读者将通过更新其本地version（报告）向写者报告。锁定的读者在离开读取侧临界区或进入睡眠状态后将稍后报告。在报告了所有核心后，所有passive读者都达成了共识。然后，写者进入阶段2（第21-23行）。在此阶段，写者只需等待所有活动的读者退出即可。对于写者优先锁来说，写者可以将锁直接传递给未决的writer，而无需再次达成共识（WriteUnlock中的1-2行和WriteLock中的2-4行）。 1234567891011121314Function WriteLock(lock)lastState ← Lock(lock.writer);if lastState = PASS then return;/* Lock passed from another writer */newVersion ← AtomicInc(lock.version);coresWait ← /0;for ID ∈ AllCores do if Online(lock.domain, ID) ∧ ID != CoreID then if PerCorePtr(lock.rstatus, CoreID).version != newVersion then AskForReport(ID); Add(ID, coresWait);for ID ∈ coresWait dowhile PerCorePtr(lock.rstatus, CoreID).version != newVersion do Relax();while lock.active != 0 do Schedule(); 123Function WriteUnlock(lock)if SomeoneWaiting(lock.writer) then Unlock(lock.writer, PASS);else Unlock(lock.writer, FREE); 123Function Report(lock)st ← PerCorePtr(lock.rstatus, CoreID);if st.reader != PASSIVE then st.version ← lock.version; Example如果阅读器进入睡眠状态，则处于被动模式的阅读器可能会切换到主动模式。 在读取器释放锁定之前，无法将其直接切换回被动模式。 以下获取的锁将再次处于被动模式 在编写者开始获取锁之前，reader2已完成其读取关键部分，而reader3由于等待某个事件而处于其读取关键部分中。 Reader1和Reader4刚开始阅读关键部分，但尚未完成。在阶段1中，有一个写程序试图以写模式获取该锁，这将增加该锁的版本并阻止所有即将到来的读者。它将IPI发送给尚未看到最新锁版本的当前活动读取器。如果core2中的reader2完成了上下文切换，并且现在正在运行另一个线程，则core2不需要IPI。 core4中的Reader4可能会进入睡眠状态以等待某个事件，该事件将切换为活动的读取器。 core4不需要IPI，因为那时core4中没有读取器。在第一阶段结束时，所有被动阅读器都离开了关键部分。因此，在阶段2中，写程序等待所有活动的读程序完成执行，最后可以在写模式下授予锁定。对于写者偏好的prwlock，写者可以将锁直接传递给下一个写者，这可以避免连续写者的读者之间不必要的共识。 Correctness on TSO architecturerwlocks与其他较弱的同步原语之间的主要区别在于，rwlocks在读写器之间具有强大的可见性保证。借助TSO一致性模型，可以在prwlock中确保这一点。一旦读者看到了免费的prwlock，我们可以确定：1）该FREE由直接的前一个作者设置，因为作者将始终确保所有读者在继续之前都看到其LOCKED状态； 2）由于在TSO架构下按顺序可见内存写入，因此以前的写入器所做的更新也应对该读取器可见。早期的作家也是如此。 3）写作者必须等到所有读者都可以看到它，以便在该读者退出之前，没有其他写作者可以进入关键部分。因此，prwlock确保了共享状态的一致视图。这三个属性共同确保读者始终可以看到受prwlock保护的共享数据的最新一致版本。此外，由于所有读取器在获取写入器锁期间都明确报告了最新版本，因此还可以保证写入器查看读取器对其他数据结构所做的所有更新（如果有）。在非TSO架构上，读取器算法中需要两个额外的存储屏障，如图3所示。第一个确保读取器在快速路径中获得锁定后，可以看到共享数据的最新版本。第二个方法是在释放读取器锁之前使写入器可以看到读取器的内存更新。]]></content>
      <categories>
        <category>Sys</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>OS</tag>
        <tag>rwlock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FSCQ文件系统]]></title>
    <url>%2Fpost%2F3d2c.html</url>
    <content type="text"><![CDATA[FSCQ：可以应对crash的文件系统，with a machine-checkable proof (using the Coq proof assistant) FSCQ论文基础背景文件系统cp /path/src /path/dst，文件系统首先需要从目录树中定位源文件的位置，为 此，它在根目录条目中找到“path”（通常已经缓存在数据缓存区里），拿到 path 的索引值 ipath，将path的目录条目加载到数据缓存区中，定位和名称“src”相对应的索引值isrc， 然后文件系统通过读取 isrc 索引节点的文件内容。要创建目标文件，文件系统将分配一个新的索引节点 idst，并在 inode 的分配位图中标记为已经使用，然后根据 src 文件的大小来给 dst 文件分配新的磁盘块，在 idst 中记录分配的块号，并且在块的的数据位图中将其标记为已使用，再将 src 的内容写入到新分配的块中。最后，文件系统将一个新的目录条目 （“dst”，idst）添加到 path 的目录条目中，并将目录的新内容写回到磁盘里。注意：此处的顺序并不能保证崩溃安全性。 崩溃安全性应用完成某个操作可能涉及到多次写磁盘，该过程中系统可能随时崩溃，有可能一个写操作只完成一部分，崩溃后磁盘会处于一个中间状态，这个状态下数据可能不一致，因此再重启之后通常需要运行一个恢复程序，检查磁盘的状态，将其恢复到一个数据一致的状态，以便将来能继续正确执行程序，文件系统的这种属性称为崩溃安全性。 写屏障（磁盘同步）现代硬盘驱动通常会增加一层缓存来优化 I/O 实际写入磁盘的顺序，即发给磁盘的写入不会立即持久化，最近的写往往会被缓存在内部缓存中，并由硬盘驱动器决定什么时候会将这些未完成的写刷到磁盘。同时，硬盘驱动器可以对这些写重新排序进行写入以提高磁盘吞吐量。假如系统没有崩溃，一个读总是能够读到最新写得值，但假如系统崩溃并重启，可能会发现崩溃之前稍后发出的写持久到了磁盘，但是之前的写却没有。 硬盘驱动器提供了写屏障操作，称为磁盘同步，这会强制将写的缓存全部刷到磁盘上，磁盘同步操作可以用来保证不会有未完成的写操作，之前所有的写都会保留在磁盘上。但是磁盘同步非常的耗时，应当尽可能的避免，而这种异步的 I/O 使得论证崩溃安全性变得更加复杂。 预写式日志通过写操作的重排序来优化写的性能，文件系统的开发人员必须在适当的位置植入屏障，这个屏障会保证之前代码里面所有的写操作持久化到磁盘，以此来保证顺序性，但屏障是非常耗时的操作，需要尽量的避免， 现在比较通用的实现崩溃安全性的方法是使用预写式日志。这时写操作不会直接修改文件系统相应部分的数据结构，而是会先将修改记录在磁盘上的日志区域，当所有操作记录完成后，会进行标志，然后再将数据从日志区域写到实际的磁盘区域，再把日志区域的记录擦 除。假如系统在中间发生了崩溃，恢复程序会检查标志是否存在，假如存在，恢复程序会先把所有数据重新写一遍到磁盘上，这样就可以解决之前数据可能只写了一部分的问题。假如 不存在，说明之前的数据并没有往磁盘上写，因此可以直接将日志区域的数据擦除，仿佛这个操作完全没有发生一样，写标志这个时间点成为了操作是否发生的时间点，即使在运行恢复程序的过程中系统再次发生崩溃，重启之后再次运行恢复程序依然能正确的恢复磁盘状态。 虽然概念上很简单，但预写式日志会增加许多额外的磁盘写入和磁盘屏障操作，因此实 际的文件系统会实现很多优化来尽量增加磁盘吞吐量，这些优化包括将多个事务合并成一 个事务以及延迟将缓存中的数据写入磁盘。 日志协议文件系统一般会实现预写日志来解决这个问题，以确保系统崩溃重新启动的时候，磁盘 上的数据结构处于一致的状态。磁盘通常会分配一块固定的区域给日志，日志通常由两部分构成，一部分是连续的日志条目，每个日志条目记录希望对磁盘进行的写操作，另一部分是一个单一的提交快，记录着日志条目的数量。 一个基本的日志协议工作如下：（1）日志记录系统为文件系统的一个操作开启一个事务（2）对于该操作引发的每一个磁盘写入的地址和值，日志记录系统向日志条目附加一个新的地址/值对（3）当日志记录系统提交事务时，它会发出一个磁盘同步来保存磁盘上的日志条目，然后更新提交块以反映日志的长度，然后再次同步磁盘以完成这个事务（4）日志系统将日志中的修改应用到实际的磁盘位置，并发出磁盘同步将更改持久化到磁盘上（5）最后，将零写入提交块来截断日志，并同步到磁盘上（6）无论何时系统崩溃并重新启动，都会运行恢复过程。它根据提交块的长度读取日志；如果日志不为空，则跳到步骤（4）重新执行。 上述协议的正确性依赖于一个重要的假设：更新提交块是原子的同步的操作。原子性确保了提交块上要么是原长度，要么是新长度，同步则保证了磁盘控制器不会对磁盘的写操作进行重新排序，将提交块的写和其他的写调换顺序，这是通过在更新提交块之前和之后发出 两个屏障（磁盘同步操作）来强制执行。在这个协议下，在事务中途，还没有更新提交块之前系统崩溃的话，提交块的长度为零，此时磁盘的数据没有被写，这个事务就像没有执行一 样，数据处于一致的状态；提交之后但在应用完成之前的系统崩溃将导致提交块长度非零， 并且日志中包含相同数目的有效条目，这时就只需要重新执行提交操作，会达到事务执行完的状态。 这个基本的日志协议会增加磁盘写操作以及耗时的磁盘同步操作。例如，写入单个磁盘块的操作将需要完成四次磁盘写入（两次为提交块，两次为块的内容）和四个磁盘同步（上文加粗处）。 日志优化 延迟应用日志：日志系统提交事务后，并不立即应用日志，而是等日志填满后，再一次性将日志内容应用。可以将每个事务的磁盘同步由四个 降为两个，同时将写磁盘的次数也由四次降为两次。 组提交：在内存中累积多个系统调用的写入，并将其合并到单个事物当中。 …… CHL框架CHL (Crash Hoare Logic) 框架是用 Coq 实现的用来书写规范的框架，它允许文件系统开发人员编写包含崩溃条件和恢复过程的规范，并提供了较好的证明自动化的支持。这涉及到了形式化验证的知识，我也只是刚有个模糊的认识。FSCQ 是从 Coq 提取出 Haskell 代码，再进一步获得可执行的文件系统，因此具有较高的 CPU 开销。 Haskell是一门纯粹的函数式编程语言。 Coq 是一种基于构造演算的证明助手，被用于形式化各种领域的证明，包括数学和编程语言。如果对其感兴趣想了解更多，可以参照此网站进一步学习 -&gt; https://github.com/coq/coq/wiki#coq-tutorials 磁盘模型依照分离逻辑的假设，磁盘被定义成从整数块号a到内容v的偏函数a → v，其中超过磁盘范围的块号不会映射到任何东西。 而根据我们所知，I/O写入并不是同步的，中间会经过cash异步写入磁盘。为了捕获这种异步的性质，CHL 用一个值集（v,vs）来描述一个块的内容，而不是一个单一的值，其中 v 代表着这个地址最近写入的值， 而 vs 是一系列的之前写入的值，假如系统突然崩溃，v 和 vs 中的任何一个值都可能出现在磁盘上。而磁盘同步操作意味着对应的缓存已经全部应用到该地址上，先前写入的值已经全部处理，即 vs 为 ∅，最新的值会出现在崩溃之后，否则，这个磁盘块就是未同步的。 CHL 的磁盘模型假设块是原子写的，也就是说，在系统崩溃之后，块必须包含最后写入 的值或者是先前值中的一个，并且不允许部分块写入。 崩溃条件给定了异步磁盘模型，我们还需要一种手段来描述可能崩溃的中间状态，CHL 将霍尔逻辑扩展成一个具有崩溃条件的四元组， 写为：{P}C{Q}{R} 其中 Q 依然是后置条件，描述 C 在没有崩溃情况下的执行结束时的状态，而 R 是描述 C 执行过程中可能发生的中间状态的崩溃条件。 下面我们以disk_write(a, v)举例： 前置条件：F * a -&gt; (v0, vs)后置条件：F * a -&gt; (v, {v0} U vs)崩溃条件：F * a -&gt; (v0, vs) ∨ F * a -&gt; (v, {v0} U vs) 规范由四个部分组成，首先是我们推理的过程名称disk_write(a, v)，把值v写到地址a上。其前置条件声明的是磁盘上地址 a 的地方指向的值是一个集合，v0是最新写入的值，vs则表示之前写入的值的序列，除 a 以外的地址满足的谓词是 F；其后置条件说的是 a 以外的值依旧满足谓词 F，没有修改，而地址 a 指向的值的集合中，最新的元素是v0，旧的值则是由vs加上v0组成的序列；由于磁盘写是一个原子操作，因此它要么完成了值的写入，要么没有完成，因此可能发生系统崩溃时有两种状态，一是前置条件的状态，二是后置条件的状态，通过将前置条件和后置条件并起来则表示了系统崩溃时可能的所有状态，即系统崩溃时必须满足的谓词。 磁盘写的规范说明还表明了真实磁盘的两个重要行为，保证了磁盘的性能： 磁盘实际 I/O 可以异步发生 磁盘的写可以重新排序 因为一个地址a会指向一组值而不是一个值，指向一组值表明系统崩溃恢复的时候，磁盘上的值可能是之前的某个值，而不同地址之间可能会呈现出不一致的状态，这也表明了磁盘对写进行了重新排序。 CHL 的崩溃条件有一个微妙的地方，他们描述的是系统崩溃之前磁盘的状态，而不是系统崩溃之后。在崩溃后，CHL 的磁盘模型认为，每个块将从崩溃之前的值的集合中选择一个值，例如disk_write(a, v)的前置条件中表明，磁盘的 a 地址指向的可能是一系列值的集合，而不是一个值。这样的好处就是，崩溃条件会跟前置条件和后置条件类似，有利于证明自动化。 逻辑地址空间文件系统的许多地方涉及到逻辑地址空间。例如，磁盘是从磁盘块号到磁盘块内容的地址空间；文件索引层是从文件索引号到文件索引的地址空间；每个文件里，是从该文件中的偏移量到数据的地址空间；目录是从文件名到索引节点号的地址空间。基于这个观察，CHL 扩展了关于磁盘推理的分离逻辑， 使其以同样的方式表达诸如文件，目录或逻辑文件内容的更高级的地址空间。 作为逻辑地址空间的实例，我们考虑 inc_two 的简单过程。inc_two 表达在一个操作中需要同时更新两个块的文件系统调用，我们先读取两个块的内容，然后将这两个块的内容加一，再写回。该过程使用日志系统提供的 log_begin，开始一个事务，用 log_read 来读取数据，然后用 log_write 来写数据，最后用 log_commit 来提交这个事 务。假如在执行 inc_two 的过程中发生了系统崩溃并且重新启动，首先系统会运行 log_recover 这个恢复程序，如果在恢复过程中依然发生系统崩溃，则重新启动后，恢复过程将再次运行。原则上来说恢复过程可能运行若干次。尽管如此，只要 log_recover 完成， 日志记录系统就能保证两个写入要么都发生，要么都没有发生，不管经历了多少次系统崩溃。 为了简化说明，我们假设现在的日志系统只是采用前面所述的基础的日志协议: SPEC inc_two(a1, a2) PRE disk: log_rep(NoTxn, start_state)start_state: F ⋆ a1 → (v_x, vs_x) ⋆ a2 → (v_y, vs_y) POST disk: log_rep(NoTxn, new_state)new_state: F ⋆ a1 → (v_x+1, ∅) ⋆ a2 → (v_y+1, ∅) CRASH disk: log_intact(start_state, new_state) 前置条件中，a1 → (v_x, vs_x) 适用于日志系统提供的逻辑磁盘的地址空间，后置条件中，a1 → (v_x+1, ∅) 适用于逻辑磁盘的新的内容。就像物理磁盘一样，这个地址空间也是从地址到值的偏函数。与物理磁盘不同的是，通过日志系统写入逻辑磁盘的操作展现的是一个同步的接口，即整个操作完成之后，能保证要写入地址的值被刷到磁盘上，因此在后置条件中，我们看到的是单个同步的值，这层抽象做的具体的事情就是隐藏更下层的异步的接口，这也是引入地址空间的好处。 我们在 log_rep 将逻辑地址空间和实际的物理磁盘连接起来。例如，我们指定起始状态如何存储在磁盘上，以及如何逻辑的构造新的状态，比如将日志内容应用到起始状态。通过 log_rep 的这层转化，对逻辑地址空间的描述将会变成对物理磁盘的描述，并且隐藏了一 些琐碎的细节。log_rep 将会把逻辑地址空间作为参数，结合日志系统的状态和逻辑磁盘状态准确的反映出物理磁盘上的状态。日志系统中有好几个可能的状态： NoTxn ：表示当前磁盘没有事务活动， ActiveTxn ：表示活跃状态中的事务所有的修改只会发生在内存中 FlushedUnsync ：表示此时开始进入提交事务的过程，首先把内存中的修改刷到数据缓冲区的日志记录的区域中，未同步表示还没有刷到磁盘上 Flushed ：表示此时已经有了一次磁盘同步，我们将日志的数据区的内容刷到了磁盘上 CommittedUnsync ：表示我们现在将提交块写到了数据缓冲区里，同时还没有同步到磁盘里 Committed ：表示提交块被同步到了磁盘里，这个点之前和之后作为事务是否完成的分界点 AppliedUnsync ：表示现在数据缓存区里将日志的内容实际应用到数据区 AppliedSync ：表示日志应用的工作已经完全反映到了磁盘上，整个事务至此完成 inc_two 的崩溃条件使用 log_would_recover 描述了所有可能的崩溃状态从这些崩溃状态要么恢复到 start_state，要么恢复到 cur_state，使用 log_would_recover，我们考虑的所有状态即前文所述的日志系统的八种状态，这些状态的产生源于这个操作深层次的调用中任何可能的崩溃点（比如，在 log_commit 过程里面的崩溃情况）。 恢复执行语义计算机遇到崩溃并重新启动后，它会在恢复正常操作之前先运行恢复程序（如 log_recover），崩溃条件和地址空间允许我们指定计算机在执行过程中可能会崩溃的状态， 但我们还需要一种证明恢复过程正确性的方法，包括在恢复期间的系统崩溃。日志系统必须在每次崩溃之后运行 log_recover 来将已经提交的事务继续做完，包括在运行 log_recover 本身发生崩溃之后。 log_recover 指出，从与 log_would_recover(last_state, committed_state) 相匹配的任何状态开始，log_recover 会将事务回滚到 last_state 或将提交的事务继续执行到 committed_state。此外，log_recover 的规范是幂等的，因此它的崩溃条件意味着前置条件，这将允许 log_recover 在执行的过程中崩溃多次。 SPEC log_recover()PRE disk: log_intact(last_state, committed_state)POST disk: log_rep(NoTxn, last_state) ∨ log_rep(NoTxn, committed_state)CRASH disk: log_intact(last_state, committed_state) 为了表达 log_recover 在遇到崩溃之后运行，CHL 提供了恢复执行语义，一般的执行语义下，一个程序的执行要么产生错误状态，崩溃状态或者正常完成状态，恢复执行语义则考虑的是两个程序执行，其中一个是正常程序，另一个是恢复程序，并且要么产生一个错误，一个完成状态（正常程序完成执行）或者一个恢复状态（恢复程序完成执行）。这种模型下，首先正常程序尝试完成，但如果系统崩溃，它将开始运行恢复程序，如果再回复过程中系统崩溃，有可能需要多次运行恢复程序，最终达到恢复状态的过程。 扩展后{P}C≫R{Q}表示，假如 C 在满足前置条件 P 下执行，且每当系统在 C 崩溃或 R 自身崩溃时执行恢复过程 R，则当 C 或 R 终止时，条件 Q 将成 立。 我们可以由此扩展 inc_two 规范以包括恢复执行： SPEC inc_two(a1, a2) ≫ log_recover PRE disk: log_rep(NoTxn, start_state)start_state: F ⋆ a1 → (v_x, vs_x) ⋆ a2 → (v_y, vs_y) POST disk: log_rep(NoTxn, new_state) ∨ (status = recovered ∧ log_rep(NoTxn, start_state))new_state: F ⋆ a1 → (v_x+1, ∅) ⋆ a2 → (v_y+1, ∅) 后置条件表示，如果 inc_two 在没有崩溃的情况下完成，则两个块都已经更新，或者两者都没有更新，这时一定是因为发生了崩溃，运行恢复程序之后达到了原状态。状态变量 status 指示系统当前状态。 证明规范证明自动化为证明这样一个程序 C 是正确的，我们需证明如果 C 执行，且执行前 C 前置条件成立，那么，C 的后置条件的正常运行之后成立，且恢复条件在恢复程序运行之后成立。 正常执行下，我们必须证明顶层程序的前置条件蕴含第一个子过程的前置条件，第一个子程序的后置条件蕴含着接下来要执行程序的前置条件，并这样一直下去。 对于崩溃，我们则必须证明每个过程的崩溃条件都蕴含这恢复程序的前置条件，恢复程序的后置条件蕴含着顶层程序的恢复条件。 在这两种情况下，逻辑蕴涵关系都按着控制流的方向传递，在 CHL 框架里面，有用策略语言写好的证明搜索的 Ltac 程序，假如前置条件很容易被之前程序的后置条件蕴涵，那么 Ltac 程序可以自动完成这个过程，开发人员不需要进行手动的证明，在实际中，通常开发人员只需要证明程序的不变量的成立。 证明崩溃条件规范程序分解CHL 的第一个阶段是使用组合规则将证明目标分解成一系列的证明义务。具体来说，CHL 会考虑 C 中的每个步骤，并查看每个步骤的前置条件和后置条件，假设每个小的步骤已经有一个证明好了的规范，比如最基本的磁盘读写，同步操作。CHL 首先假设 C 前置条件成立，并为每个步骤产生两个证明义务，（1）当前所能给定的条件，要么是 C 的前置条件（即当前步骤为第一步骤），要么是前一个步骤的后置条件，不管是那种情况，都要求给定条件能蕴涵当前步骤的前置条件；（2） 当前步骤的崩溃条件蕴涵 C 的崩溃条件（小范围推出大范围）。而对程序 C 的最后一个小步骤，会产生最后一个步骤蕴涵 C 的后置条件的证明义务。 组合规则：{P1}C1{P2}{R1} {P2}C2{P3}{R2}B -&gt; {P1}C1; C2{P3}{R1∨R2}正常运行，P1→C1→P2→C2→P3；如若崩溃，可能在C1可能在C2，故用∨ 谓词蕴涵有些程序分解之后产生的证明义务很容易证明，CHL 根据一阶逻辑会立即证明这个蕴涵成立。 对更复杂的情况，CHL 会依赖于分离逻辑的结构规则来证明。当 inc_two 第一次调用 log_write 时，我们需要证明F1 ⋆ a1 → (v_x, vs_x) ⋆ a2 → (v_y, vs_y) 蕴涵 F ⋆ a → (v0, vs0)，CHL 会自动将 a1 → (v_x, vs_x) 与a → (v0, vs0) 匹配，并将这些项从证明义务的两边同时去掉，然后将 F 匹配为F1 ⋆ a2 → (v_y, vs_y)，这样会将这个证明义务完成，然后将a2 → (v_y, vs_y) 的信息带入到接下来的证明义务中。 蕴涵：如果P，那么Q，表示为P→Q。蕴涵式中，只有当P真Q假时判断为假，其余均为真。结构规则：{P}C{Q}{R} -&gt; {P⋆S}C{Q⋆S}{R⋆S} 证明恢复语义规范如果恢复程序是幂等的，CHL 可以使用崩溃条件的规范直接来证明具有恢复条件的规范，证明恢复规范的规则称为恢复规则：对程序C有 {P}C{Q}{T} 当崩溃条件T触发时，启用恢复语义 {T}R{S}{T} 即可推出 {P}C≫R{Q∨S} 恢复规则说的是假如恢复过程的前置条件既是C的崩溃条件又是自身的崩溃条件，如果 C 和 R 的后置条件分别是 Q 和 S，那么将C和R一起运行的结果就是Q∨S 文件系统构建FSCQ参照xv6文件系统设计，但不支持多处理器，同时采用单独的位图来分配索引节点。 系统结构FscqLog提供了一个简单的预写日志，FSCQ使用它来原子地更新几个磁盘块。其他组件提供标准文件系统抽象的简单实现。缓存模块提供缓冲区缓存。 Balloc实现了一个位图分配器，用于块和inode分配。索引节点实现索引节点层；这里最有趣的逻辑是将直接块和间接块组合在一起，形成单个块地址列表。 Inode调用Balloc分配间接块。 BFile实现了一个块级文件接口，在更高级别上公开了一个接口，其中每个文件都是一个块列表。 BFile调用Balloc分配文件数据块。 Dir在块级文件的顶部实现目录。 ByteFile实现文件的字节级接口，其中每个文件都是字节数组。 DirTree将目录和字节级文件组合为分层目录树结构；在创建/删除文件或子目录时，它调用Balloc来分配/取消分配索引节点。最后，FSCQ在事务中实现完整的系统调用。 磁盘模型FSCQ 崩溃后，并非每次发出的写入都将反映在磁盘状态中。 CHL模型的另外两个磁盘操作是读取磁盘块（disk_read），并等待所有写操作到达非易失性内存（disk_sync）disk_sync将块地址作为附加参数，以指示必须同步的块。 disk_sync的按地址变体会丢弃该块的所有先前值，从而使最后写入的值成为崩溃后可能出现的唯一可能值。在执行时，每个地址disk_syncs的连续调用被折叠为单个全局disk_sync操作，以实现所需的性能。 POSIX接口FSCQ在顶层提供类似于POSIX的接口。 与POSIX的主要区别是：FSCQ不支持硬链接，以及 FSCQ不实现文件描述符，而是需要按索引节点编号命名打开的文件。 FSCQ依靠FUSE驱动程序来维护打开文件描述符和inode编号之间的映射。 FscqLogFscqLog一次仅允许一个事务，并假定内存足够大以容纳当前事务的内存日志。但即使对于这种简单的设计，日志记录协议也很复杂，因为disk_write是异步的。崩溃后，日志记录系统可以处于七个状态之一：NoTxn（无活动事务），ActiveTxn（事务已启动），LogFlushed（log_flush已经发生），CommittedUnsync（已写入提交块），CommittedSync（提交块）已同步），AppliedUnsync（已应用日志）和AppliedSync（已应用并同步日志）。崩溃后，FSCQ的恢复过程fs_recover读取布局块以确定日志的位置，然后调用FscqLog的log_recover将磁盘置于一致状态。FSCQ仅通过log_write更新磁盘，并将这些写入以系统调用的粒度包装到事务中，以实现崩溃安全。此外，它为上级软件提供了一个更简单的同步接口。因为事务API公开了一个逻辑地址空间，该逻辑地址空间将每个块映射到唯一的块内容，即使物理磁盘将每个块映射到最后写入值和一组先前值的元组。因此，在FscqLog之上编写的软件不必担心异步写入。]]></content>
      <categories>
        <category>Sys</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>CHL</tag>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统]]></title>
    <url>%2Fpost%2Fc51c.html</url>
    <content type="text"><![CDATA[结合 MIT-6.824-2018 的各个 lab 来简单写下学习分布式系统的学习心得。 目前主要有：MapReduce, Raft（期待后续更新中） MapReduceMapReduce PaperMapReduce Example我们用最经典的words count来举例介绍MapReduce模型： 对于一个三行的input file来说，我们按行进行splitting操作。每行分给不同的线程/节点，各节点单独进行mapping任务（单词分割）。各节点在shuffing阶段会进行相互通信，完成“洗牌”工作。然后各节点单独完成reducing任务并将结果输出在各节点本地文件上。最后将这些文件进行归并得到final result。 MapReduce Execution 用户程序调用 MapReduce 库将 Input files 切成 M 个数据片度，然后用户程序在机群中 fork 大量程序副本。 Fork 的副本中有一个特殊的程序 master，其它都是 worker 程序，由 master 分配任务。有 M 个 Map Task 和 R 个 Reduce Task 将被分配，master 将一个 Map/Reduce Task 分配给一个空闲的 worker。 被分配 map task 的 worker 读取相关的输入数据片段并解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入本地磁盘。缓存的 key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker。 当 Reduce worker 接收到 master 发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，对 key 排序后使具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。 Reduce worker 遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 将这 个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。此时在用户程序里对 MapReduce 的调用才返回。 在成功完成任务之后，MapReduce 的输出存放在 R 个输出文件中（对应每个 Reduce 任务产生一个输出文件，文件名由用户指定） MapReduce RefinementsPartitioning Function使用 MapReduce 时通常会指定 Reduce 任务和 Reduce 任务输出文件的数量（R）。我们在中间 key 上使用分区函数来对数据进行分区，再输入到后续任务执行进程。缺省的分区函数是使用 hash 方法，比如 hash(key) mod R 进行分区。 Ordering Guarantees确保在给定的分区中，中间 key/value pair 数据的处理顺序是按照 key 值增量顺序处理的。这样的顺序保证对每个分成生成一个有序的输出文件，这对于需要对输出文件按 key 值随机存取的应用非常有意义， 对在排序输出的数据集也很有帮助。 Combiner Function在某些情况下，Map 函数产生的中间 key 值的重复数据会占很大的比重，并且，用户自定义的 Reduce 函数满足结合律和交换律。MapReduce 允许用户指定一个可选的 combiner 函数，combiner 函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出 去。 Combiner 函数在每台执行 Map 任务的机器上都会被执行一次。一般情况下，Combiner 和 Reduce 函数是 一样的，唯一区别是 Reduce 函数的输出被保存在最终的输出文件里，而 Combiner 函数的输出被写到中间文件里，然后被发送给 Reduce 任务。 部分的合并中间结果可以显著的提高一些 MapReduce 操作的速度。 Input and Output TypesMapReduce 库支持几种不同的格式的输入数据。比如，文本模式的输入数据的每一行被视为是一个 key/value pair。key 是文件的偏移量，value 是那一行的内容。另外一种常见的格式是以 key 进行排序来存储的 key/value pair 的序列。每种输入类型的实现都必须能够把输入数据分割成数据片段，该数据片段能够由单独的 Map 任务来进行后续处理。同时也可以通过提供一个简单的 Reader 接口实现就能够支持一个新的输入类型。 MapReduce LabPart I: Map/Reduce input and outputdoMap()doMap manages one map task：读取一个输入文件inFile，并为改文件内容调用用户自定义的mapF()，最后将mapF()的输出拆分到nReduce个中间文件中。同时要保证reduce的输出为json格式以方便后续操作。 1234567891011121314151617181920212223242526272829303132func doMap( jobName string, // the name of the MapReduce job mapTask int, // which map task this is inFile string, nReduce int, // the number of reduce task that will be run ("R" in the paper) mapF func(filename string, contents string) []KeyValue,) &#123; data, err := ioutil.ReadFile(inFile) if nil != err &#123; log.Fatal(err) &#125; // mapF() returns a slice containing the key/value pairs for reduce kvs := mapF(inFile, string(data)) var outFiles []*os.File defer func() &#123; for _, file := range outFiles &#123; file.Close() &#125; &#125;() for i:=0; i&lt;nReduce; i++ &#123; //the filename as the intermediate file for reduce task r. name := reduceName(jobName, mapTask, i) file, err := os.Create(name) if nil != err &#123; log.Fatal(err) &#125; outFiles = append(outFiles, file) &#125; for _, kv := range kvs &#123; index := ihash(kv.Key) % nReduce //enc opens IO stream of outFiles[index] enc := json.NewEncoder(outFiles[index]) //writes kv in stream enc.Encode(kv) &#125;&#125; doReduce()doReduce manages one reduce task：读取map tasks产生的中间文件，将其根据键值对的key排序后，为每个key调用用户自定义的reduceF()函数，最后将reduceF()的结果写到磁盘outFile文件上。因为doMap()产生的是json格式，要注意解码。 123456789101112131415161718192021222324252627282930313233343536func doReduce( jobName string, // the name of the whole MapReduce job reduceTask int, // which reduce task this is outFile string, // write the output here nMap int, // the number of map tasks that were run ("M" in the paper) reduceF func(key string, values []string) string,) &#123; //set of having the same key kvsMap := make(map[string]([]string)) for i := 0; i &lt; nMap; i++ &#123; //yield the filename from map task m name := reduceName(jobName, i, reduceTask) file, err := os.Open(name) if nil != err &#123; log.Fatal(err) &#125; dec := json.NewDecoder(file) for &#123; var kv KeyValue err := dec.Decode(&amp;kv) if err != nil &#123; break &#125; kvsMap[kv.Key] = append(kvsMap[kv.Key], kv.Value) &#125; file.Close() &#125; reduceFile, err := os.Create(outFile) if nil != err &#123; log.Fatal(err) &#125; enc := json.NewEncoder(reduceFile) for key, value := range kvsMap &#123; //reduceF() returns the reduced value for that key data := reduceF(key, value) kv := KeyValue&#123;key, data&#125; enc.Encode(kv) &#125; reduceFile.Close()&#125; Part II: Single-worker word countmapF()向mapF()传递一个文件名以及该文件的内容，mapF()将文件内容拆分为单词（毕竟对于字数统计，仅将单词用作键才有意义），并返回KeyValue类型的Go切片。 Tips: strings.FieldsFunc()可以用来拆分string，strconv.Itoa()可以将int转换成string类型。 123456789101112131415161718192021222324func mapF(filename string, contents string) []mapreduce.KeyValue &#123; //单词频率结果 wordsKv := make(map[string]int) //分词并遍历 f := func(r rune) bool &#123; return !unicode.IsLetter(r) &#125; //splits contents at each run of Unicode code points c satisfying f(c) words := strings.FieldsFunc(contents, f) for _, word := range words &#123; //统计单词 _, ok := wordsKv[word] if ok &#123; wordsKv[word]++ &#125; else &#123; wordsKv[word] = 1 &#125; &#125; //转换为输出格式 var rst []mapreduce.KeyValue for key, value := range wordsKv &#123; kv := mapreduce.KeyValue&#123; key, strconv.Itoa(value) &#125; rst = append(rst, kv) &#125; return rst&#125; reduceF()每个key都会调用一次reduceF()计算这个key的出现总数，并以string类型return。 Tips: strconv.Atoi()可以将string类型转换成int类型。 12345678910func reduceF(key string, values []string) string &#123; cnt := 0 //合并统计结果 for _, value := range values &#123; num, err := strconv.Atoi(value) if err != nil &#123; break &#125; cnt += num &#125; return strconv.Itoa(cnt)&#125; Part III: Distributing MapReduce tasksschedule()在一次mapReduce中会调用两次schedule()，分别在map阶段和reduce阶段。schedule()将任务分发给可用的worker并等待所有任务完成后返回。registerChan参数channel为每个worker程序生成一个字符串，其中包含工作程序的RPC地址。schedule()通过call(worker, &quot;Worker.DoTask&quot;, args, nil)（worker := &lt;-registerChan）将Worker.DoTask RPC 发送给worker程序来通知其执行任务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func schedule(jobName string, mapFiles []string, nReduce int, phase jobPhase, registerChan chan string) &#123; var ntasks int var n_other int // number of inputs (for reduce) or outputs (for map) switch phase &#123; case mapPhase: ntasks = len(mapFiles) n_other = nReduce case reducePhase: ntasks = nReduce n_other = len(mapFiles) &#125; fmt.Printf("Schedule: %v %v tasks (%d I/Os)\n", ntasks, phase, n_other) // All ntasks tasks have to be scheduled on workers. // Once all tasks have completed successfully, // schedule() should return. var wg sync.WaitGroup wg.Add(ntasks) for i := 0; i &lt; ntasks; i++ &#123; go func(index int) &#123; //rpc call args var args DoTaskArgs args.Phase = phase args.JobName = jobName args.NumOtherPhase = n_other args.TaskNumber = index if phase == mapPhase &#123; args.File = mapFiles[index] &#125; done := false for !done &#123; // registerChan yields a stream of registered workers worker := &lt;-registerChan done = call(worker, "Worker.DoTask", args, nil) go func()&#123; registerChan &lt;- worker &#125;() &#125; wg.Done() &#125;(i) &#125; wg.Wait() fmt.Printf("Schedule: %v done\n", phase)&#125; RaftRaft是一种共识算法，它在容错和性能上与Paxos等效，但更容易理解学习。可以通过 http://thesecretlivesofdata.com/raft/ 简单认知下Raft共识算法。 共识是容错分布式系统中的一个基本问题，共识涉及多个服务器就价值达成一致。共识通常出现在复制状态机的背景下，每个服务器都有一个状态机和一个日志，每个状态机都从其日志中获取输入命令。以哈希表为例，日志将包含诸如 set x=3 之类的命令，而共识算法要确保所有状态机都 x=3 作为第n个命令应用。即每个状态机处理相同系列的命令，从而产生相同系列的结果并到达相同系列的状态。 Raft PaperRaft 选举出一个leader来管理复制日志：leader从客户端接收日志条目，将其复制到其他服务器上，并在保证安全的时候应用日志条目到他们的状态机中。由此，Raft 将一致性问题分解成三个子问题：Leader Election, Log Replication, Safety（所有状态机都应执行相同的命令序列） Raft PropertiesRaft 在任何时候都保证以下的各个特性。 Election Safety：一个给定term最多能选出一位leader Leader Append-Only：leader不能删除或修改自身log中entries，只能追加新entries Log Matching：如果两个log在某一index处的entries具有相同term，则这两个log在该index前的所有entries相同。 Leader Completeness：一个committed的entry会始终存在于之后terms的leaders的log中。 State Machine Safety：若一个给定index的entry已经被某台服务器apply，则不会有服务器会在其log的index位置apply别的entry的。 Raft Basics在任何时刻，每一个服务器节点都处于这三个状态之一：leader、followers、candidates。通常系统中只有一个leader且其他节点全都是followers，在一个任期内，leader一直保持，直到自己宕机。Followers都是被动的：他们不会发送任何请求，只是简单的响应来自leader / candidates的请求。Leader处理所有的客户端请求（如果一个客户端和followers联系，那followers会把请求重定向给leader） Raft 把时间分割成任意长度的任期terms。任期用连续的整数标记。每一段任期从一次选举开始。 每一个节点存储一个当前任期号current term，这一编号在整个时期内单调的增长。当服务器之间通信的时候会交换current term；如果一个服务器的current term比其他人小，那么他会更新到较大的编号值。Raft 算法中服务器节点之间通信使用远程过程调用（RPCs），并且基本的一致性算法只需要两种类型的 RPCs。请求投票（RequestVote） RPCs 由候选人在选举期间发起，然后附加条目（AppendEntries）RPCs 由领导人发起，用来复制日志和提供一种心跳机制。 Leader electionRaft 使用一种heartbeat机制来触发领导人选举。当服务器程序启动时，各自都是followers。Leader会周期性的向所有followers发送心跳包 (AppendEntries RPCs that carry no log entries) 来维持自己的权威。Followers只要收到了来自 leader / candidates 的有效RPCs就会一直保持followers。但如果一个follower在一段时间（random，一般在150ms-300ms）里没有接收到任何消息，即选举超时，那他会认为系统中无可用leader，并发起选举以选出新的leader。 要开始一次选举过程，followers先要current term++且转换到candidates，然后并行的向集群中的其他服务器节点发送RequestVote的 RPCs 来给自己投票。Candidates会继续保持当前状态直到以下三件事情之一发生： (a) it wins the election：candidate从整个集群的大多数服务器节点获得了针对同一term的选票。每个服务器按先来先服务原则在每个term最多投一张选票。一旦candidate赢得选举，则立即成为leader并向其他的服务器发送heartbeat来建立自己的权威并且阻止产生别的leader。 (b) another server establishes itself as leader：等待投票时candidate收到别的服务器leader声明的AppendEntries RPC，如果该RPC中leader的term &gt;= candidate的term，则candidate承认该leader并回到follower；否则candidate会拒绝该RPC并保持candidate 。 (c) a period of time goes by with no winner：多个followers同时成为candidates来瓜分选票，导致无人能赢得大多数人的支持。为了避免瓜分次数过多，每个candidate在发RequestVote RPCs时都会设置一个随机的等待时间，若超时则重新发送RequestVote RPCs 。这样每次瓜分后，所有candidates都会超时，current term++开启新一轮选举。 Log replication客户端只和leader交互，leader把客户端指令作为一条log entry附加到自己log中，然后并行的发起 AppendEntries RPCs 给 followers，让他们复制这条entry。当这其被安全的复制，leader会将这条entry内容应用在自身状态机上并将其commit，同时return result给客户端。接下来leader会通知followers该entry已经committed，followers将其应用到各自状态机，完成各自log上该entry的commit。leader会不断的重复尝试 AppendEntries RPCs （尽管已经回复了客户端）直到所有的followers都最终存储了所有的日志条目。 日志由有序序号标记的条目组成。每个条目都包含创建时的任期号（图中框中的数字）和一个状态机需要执行的指令。一个条目当可以安全的被应用到状态机中去的时候，就认为是可以提交了。 在leader将创建的日志条目复制到大多数的服务器上的时候，此时可以认为该 log entry 应用到状态机中是安全的，即该 log entry is committed（如图中条目 7）。同时，leader的日志中之前的所有日志条目也都会被提交，包括之前所有terms里的条目。Leader跟踪 committed entries 的 highest index（例如图中为7），并将其写进未来所有的 AppendEntries RPCs 好让 followers 清楚。Followers会将 committed entries 按 log 顺序应用到本地的状态机中。 Raft 的日志机制维护了不同服务器的日志之间的高层次的一致性，同时也组成了上文中的日志匹配特性： 不同log中的两个entry拥有相同index&amp;term -&gt; 他们存了相同的指令且之前的所有entries也全部相同。 理由：leader会把前一个entry的index和term写进当前AppendEntries RPC中，如果followers发现在自己的log中找不到符合的entry，就会拒收这个新entry以保证其log和leader的log顺序一致。 正常操作中，logs肯定会保持一致性。但存在这种情况，leader在自己log中写入了一堆entries但在这些entries还没全部committed时leader就先自己崩溃了，下一term它作为follower，就会出现和当前leader的log不一致情况。Raft的解决办法是直接把followers冲突的那部分entries覆盖成leader的对应entries。 Leader针对每一个follower维护了一个 nextIndex（即将要发给followers的entry的index）当一个leader刚获得权力的时候，初始化所有的 nextIndex = leader log highest index + 1。如果出现AppendEntries RPC被followers拒绝的情况，leader会让 nextIndex– 并重试。最终 nextIndex 会停在两者log一致的点，此时的AppendEntries RPC将不再被拒绝，即可followers冲突的entries全部删除并逐个追加leader的entries。通过这种机制，leader在获得权力的时候就不需要任何特殊的操作来恢复一致性。 Safety我们需要保证每个状态机都会按照相同的顺序执行相同的指令。但如果出现，一些entries被大部分followers接受并commit，但存在一个follower处于unavailable没有接受这些entries，但偏偏该follower在下一term成为了leader，这就会导致那些committed entries明明执行了却被强制删除，即不同状态机执行不同的指令序列。 Raft通过在leader election中增加一些限制来保证：任何leader对于给定的term，都拥有之前terms里所有的committed entries (即上文提到的 the Leader Completeness Property) 在raft的投票环节，每个candidates发出的RequestVote RPC都会包含其last log entry的term和index，followers会根据这些信息拒绝掉没有自己新（term越大越新，term相同index越大约新）的投票请求。这样选出来的leader至少和大部分followers一样新，而entries是在大部分followers都复制后才会commit，即committed entries存在于大部分followers上，如果能和大部分followers一样新，就能保证存储了所有的committed entries。 Leader不能断定之前term中保存到大部分服务器的entry是否已经commit，如图，S1作leader，开始复制entry_2，复制了小部分followers后换leader了。S5作leader，没有entry_2且刚接受到客户端的entry_3，继续换leader。S1作leader，继续复制那条entry_2并成功复制到大部分followers上，但在entry_2即将commit前，又被换leader。若S5作leader（因为entry_3的term&gt;entry_2的term，所以能收到S2,S3,S4的选票），此时entry_3会覆盖所有的entry_2；若S1继续作leader，则S1的entries都会被提交。 leader复制之前term的entries时，这些entries的term不变（图c复制entry_2时，其term依旧为2 ） 由此，对于之前term中的entries，leader不会通过计算其副本数目的方式去commit，事实上leader只会对当前term的entries用计算副本数目的方式来commit。而当前term的entries被commit，由于日志匹配特性，之前的entries都会被间接commit。 Condenced Summary AppendEntries RPC由leader负责调用来复制日志指令；也会用作heartbeat，receiver实现： 如果 term &lt; currentTerm 就返回 false 如果日志在 prevLogIndex 位置处的 entry 和 prevLogTerm 不匹配，则返回 false 如果已经存在的日志条目和新的产生冲突（index相同但term不同），删除这一条和之后所有的 entries 附加日志中尚未存在的任何新条目 如果 leaderCommit &gt; commitIndex，令 commitIndex = MIN(leaderCommit, 新日志条目索引值) RequestVote RPC由candidates负责调用来征集选票，receiver实现： 如果term &lt; currentTerm返回 false 如果 votedFor 为 null / candidateId，且candidate的日志至少和receiver一样新，那就投票给他 Rules for ServersAll Servers 如果commitIndex &gt; lastApplied，那就 lastApplied ++，并把log[lastApplied]应用到状态机中 如果接收到的RPC请求或响应中，termT &gt; currentTerm，则令currentTerm = T，并切换状态为followers Followers 响应来自leader和candidates的RPCs 如果在选举超时前一直没有收到leader或candidates的RPCs，就自己变成candidate Candidates 在转变成candidates后就立即开始选举过程 自增当前的任期号（currentTerm） 给自己投票 重置选举计时器 发送请求投票的 RPC 给其他所有服务器 如果接收到大多数服务器的选票，那么就变成领导人 如果接收到来自新的领导人的 AppendEntries RPC，转变成跟随者 如果选举过程超时，再次发起一轮选举 Leader 一旦成为领导人：发送空的AppendEntries RPC (heartbeat) 给其他所有的服务器；在一定的空余时间之后不停的重复发送，以阻止followers超时 如果接收到来自客户端的请求：附加条目到本地日志中，在条目被应用到状态机后响应客户端 如果对于一个跟随者，最后日志条目的index &gt;= nextIndex，则发送从 nextIndex 开始的所有日志条目： 如果成功：更新相应 followers 的 nextIndex 和 matchIndex 如果因为日志不一致而失败，减少 nextIndex 重试 如果存在一个N满足N &gt; commitIndex，且大多数的matchIndex[i] ≥ N成立，并且log[N].term == currentTerm成立，则令commitIndex = N Cluster membership changes实际操作中偶尔会改变集群的配置，但服务器直接从旧配置转换到新配置很容易出现分歧。为避免存在新旧配置在同一时间下出现各自leader，即同一时刻新旧配置同时生效的情况，配置更改必须使用两阶段方法。在 Raft 中，集群先切换到一个过渡的配置，称之为共同一致；一旦共同一致被commit，那系统就切换到新配置上。共同一致是老配置和新配置的结合： 日志条目会被复制给集群中新、老配置的所有服务器。 新、旧配置的服务器都可以成为leader。 达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。 当leader接收到一个改变配置从 C-old 到 C-new 的请求，他会创建configuration entry: C-old,new来存储配置，并复制给followers。各个服务器总是用latest configuration entry作为自己的配置，无论他是否committed。 一旦 C-old,new 被提交（C-old，C-new双方大部分都复制成功），那么无论是 C-old 还是 C-new，在没有经过对方批准的情况下都不可能做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。 在关于重新配置还有三个问题需要提出： 新的服务器可能初始化没有存储任何的日志条目。此时他们需要时间来更新追赶，而没法提交新的条目。Raft 在配置更新之前使用了一种额外的阶段：该阶段里，新的服务器加入，接收leader的entries但无投票权，直到追赶上了集群中的其他机器。 当前leader不在C-new中，即当前leader节点即将下线。在C-new被commit之后Leader实际已经从集群中脱离，会退位到follower状态，此时可以对Leader节点进行下线操作，而新集群则会在C-new的配置下重新选举出一个Leader。 移除不在 C-new 中的服务器可能会扰乱集群。这些服务器因为不在C-new里，所以leader根本不会给他们发心跳，之后这些节点就会选举超时，发送term++的请求投票 RPCs，这样会导致当前leader回退成follower。为了避免这种情况，除非leader超时，不然各节点拒收投票请求RPCs。同时在每次选举前等待一个选举超时时间，这样每次旧节点在发起选举前需要等待一段时间，那这段时间新Leader可以发送心跳，减少影响。 Log compactionRaft 需要一定的机制去清除日志里积累的陈旧的信息，快照Snapshotting是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃。每个服务器独立的创建快照，快照只包括committed entries。Raft 包含状态机状态和少量元数据（last index&amp;term）到快照中，保留元数据是为了支持快照后第一个条目的附加日志请求时的一致性检查，因为这个条目需要前一日志条目的索引值和任期号。为了支持集群成员更新，快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。 但当followers运行缓慢或作为C-new刚加入集群时，需要leader发送快照让他们更新到最新状态。这种情况下leader使用 InstallSnapshot RPCs 来发送快照。当followers通过这种 RPC 接收到快照时，会决定如何处理log：通常快照会含当前log没有的信息，此时会丢弃整个log，用快照代替；也可能接收到的快照是自己日志的前面部分（网络重传或者错误），那么被快照包含的entries会被全部删除，但快照后面的entries仍然有效，必须保留。 Client interactionRaft 要求客户端只和leader交互，所以当客户端请求到follower时，如果集群里存在leader，follower会将leader地址发给客户端；如果此时集群里无leader，客户端会等待、重试。 Raft 的目标是要实现线性化语义(each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response)。但如果，leader在commit该entry之后，回复客户端之前crash，那么客户端会和新leader重试这条指令，导致这条命令就被再次执行。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。 只读操作可以直接处理而不用写进日志。但在不增加任何限制的情况下，这么做可能会返回脏数据，因为leader可能在response客户端时被退回成follower，此时原先leader的信息已经变脏。线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。 Leader必须有关于被提交日志的最新信息。在leader任期开始的时候，他可能不知道哪些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条日志条目。Raft 中通过leader在任期开始时commit一个空白entry到日志中去来实现。 Leader在处理只读的请求前必须检查自己是否已经被废黜了。Raft 中通过leader在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。]]></content>
      <categories>
        <category>Sys</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>Go</tag>
        <tag>6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统]]></title>
    <url>%2Fpost%2Fc51c.html</url>
    <content type="text"><![CDATA[结合 MIT-6.824-2018 的各个 lab 来简单写下学习分布式系统的学习心得。 目前主要有：MapReduce, Raft（期待后续更新中） MapReduceMapReduce PaperMapReduce Example我们用最经典的words count来举例介绍MapReduce模型： 对于一个三行的input file来说，我们按行进行splitting操作。每行分给不同的线程/节点，各节点单独进行mapping任务（单词分割）。各节点在shuffing阶段会进行相互通信，完成“洗牌”工作。然后各节点单独完成reducing任务并将结果输出在各节点本地文件上。最后将这些文件进行归并得到final result。 MapReduce Execution 用户程序调用 MapReduce 库将 Input files 切成 M 个数据片度，然后用户程序在机群中 fork 大量程序副本。 Fork 的副本中有一个特殊的程序 master，其它都是 worker 程序，由 master 分配任务。有 M 个 Map Task 和 R 个 Reduce Task 将被分配，master 将一个 Map/Reduce Task 分配给一个空闲的 worker。 被分配 map task 的 worker 读取相关的输入数据片段并解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入本地磁盘。缓存的 key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker。 当 Reduce worker 接收到 master 发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，对 key 排序后使具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。 Reduce worker 遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 将这 个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。此时在用户程序里对 MapReduce 的调用才返回。 在成功完成任务之后，MapReduce 的输出存放在 R 个输出文件中（对应每个 Reduce 任务产生一个输出文件，文件名由用户指定） MapReduce RefinementsPartitioning Function使用 MapReduce 时通常会指定 Reduce 任务和 Reduce 任务输出文件的数量（R）。我们在中间 key 上使用分区函数来对数据进行分区，再输入到后续任务执行进程。缺省的分区函数是使用 hash 方法，比如 hash(key) mod R 进行分区。 Ordering Guarantees确保在给定的分区中，中间 key/value pair 数据的处理顺序是按照 key 值增量顺序处理的。这样的顺序保证对每个分成生成一个有序的输出文件，这对于需要对输出文件按 key 值随机存取的应用非常有意义， 对在排序输出的数据集也很有帮助。 Combiner Function在某些情况下，Map 函数产生的中间 key 值的重复数据会占很大的比重，并且，用户自定义的 Reduce 函数满足结合律和交换律。MapReduce 允许用户指定一个可选的 combiner 函数，combiner 函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出 去。 Combiner 函数在每台执行 Map 任务的机器上都会被执行一次。一般情况下，Combiner 和 Reduce 函数是 一样的，唯一区别是 Reduce 函数的输出被保存在最终的输出文件里，而 Combiner 函数的输出被写到中间文件里，然后被发送给 Reduce 任务。 部分的合并中间结果可以显著的提高一些 MapReduce 操作的速度。 Input and Output TypesMapReduce 库支持几种不同的格式的输入数据。比如，文本模式的输入数据的每一行被视为是一个 key/value pair。key 是文件的偏移量，value 是那一行的内容。另外一种常见的格式是以 key 进行排序来存储的 key/value pair 的序列。每种输入类型的实现都必须能够把输入数据分割成数据片段，该数据片段能够由单独的 Map 任务来进行后续处理。同时也可以通过提供一个简单的 Reader 接口实现就能够支持一个新的输入类型。 MapReduce LabPart I: Map/Reduce input and outputdoMap()doMap manages one map task：读取一个输入文件inFile，并为改文件内容调用用户自定义的mapF()，最后将mapF()的输出拆分到nReduce个中间文件中。同时要保证reduce的输出为json格式以方便后续操作。 1234567891011121314151617181920212223242526272829303132func doMap( jobName string, // the name of the MapReduce job mapTask int, // which map task this is inFile string, nReduce int, // the number of reduce task that will be run ("R" in the paper) mapF func(filename string, contents string) []KeyValue,) &#123; data, err := ioutil.ReadFile(inFile) if nil != err &#123; log.Fatal(err) &#125; // mapF() returns a slice containing the key/value pairs for reduce kvs := mapF(inFile, string(data)) var outFiles []*os.File defer func() &#123; for _, file := range outFiles &#123; file.Close() &#125; &#125;() for i:=0; i&lt;nReduce; i++ &#123; //the filename as the intermediate file for reduce task r. name := reduceName(jobName, mapTask, i) file, err := os.Create(name) if nil != err &#123; log.Fatal(err) &#125; outFiles = append(outFiles, file) &#125; for _, kv := range kvs &#123; index := ihash(kv.Key) % nReduce //enc opens IO stream of outFiles[index] enc := json.NewEncoder(outFiles[index]) //writes kv in stream enc.Encode(kv) &#125;&#125; doReduce()doReduce manages one reduce task：读取map tasks产生的中间文件，将其根据键值对的key排序后，为每个key调用用户自定义的reduceF()函数，最后将reduceF()的结果写到磁盘outFile文件上。因为doMap()产生的是json格式，要注意解码。 123456789101112131415161718192021222324252627282930313233343536func doReduce( jobName string, // the name of the whole MapReduce job reduceTask int, // which reduce task this is outFile string, // write the output here nMap int, // the number of map tasks that were run ("M" in the paper) reduceF func(key string, values []string) string,) &#123; //set of having the same key kvsMap := make(map[string]([]string)) for i := 0; i &lt; nMap; i++ &#123; //yield the filename from map task m name := reduceName(jobName, i, reduceTask) file, err := os.Open(name) if nil != err &#123; log.Fatal(err) &#125; dec := json.NewDecoder(file) for &#123; var kv KeyValue err := dec.Decode(&amp;kv) if err != nil &#123; break &#125; kvsMap[kv.Key] = append(kvsMap[kv.Key], kv.Value) &#125; file.Close() &#125; reduceFile, err := os.Create(outFile) if nil != err &#123; log.Fatal(err) &#125; enc := json.NewEncoder(reduceFile) for key, value := range kvsMap &#123; //reduceF() returns the reduced value for that key data := reduceF(key, value) kv := KeyValue&#123;key, data&#125; enc.Encode(kv) &#125; reduceFile.Close()&#125; Part II: Single-worker word countmapF()向mapF()传递一个文件名以及该文件的内容，mapF()将文件内容拆分为单词（毕竟对于字数统计，仅将单词用作键才有意义），并返回KeyValue类型的Go切片。 Tips: strings.FieldsFunc()可以用来拆分string，strconv.Itoa()可以将int转换成string类型。 123456789101112131415161718192021222324func mapF(filename string, contents string) []mapreduce.KeyValue &#123; //单词频率结果 wordsKv := make(map[string]int) //分词并遍历 f := func(r rune) bool &#123; return !unicode.IsLetter(r) &#125; //splits contents at each run of Unicode code points c satisfying f(c) words := strings.FieldsFunc(contents, f) for _, word := range words &#123; //统计单词 _, ok := wordsKv[word] if ok &#123; wordsKv[word]++ &#125; else &#123; wordsKv[word] = 1 &#125; &#125; //转换为输出格式 var rst []mapreduce.KeyValue for key, value := range wordsKv &#123; kv := mapreduce.KeyValue&#123; key, strconv.Itoa(value) &#125; rst = append(rst, kv) &#125; return rst&#125; reduceF()每个key都会调用一次reduceF()计算这个key的出现总数，并以string类型return。 Tips: strconv.Atoi()可以将string类型转换成int类型。 12345678910func reduceF(key string, values []string) string &#123; cnt := 0 //合并统计结果 for _, value := range values &#123; num, err := strconv.Atoi(value) if err != nil &#123; break &#125; cnt += num &#125; return strconv.Itoa(cnt)&#125; Part III: Distributing MapReduce tasksschedule()在一次mapReduce中会调用两次schedule()，分别在map阶段和reduce阶段。schedule()将任务分发给可用的worker并等待所有任务完成后返回。registerChan参数channel为每个worker程序生成一个字符串，其中包含工作程序的RPC地址。schedule()通过call(worker, &quot;Worker.DoTask&quot;, args, nil)（worker := &lt;-registerChan）将Worker.DoTask RPC 发送给worker程序来通知其执行任务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func schedule(jobName string, mapFiles []string, nReduce int, phase jobPhase, registerChan chan string) &#123; var ntasks int var n_other int // number of inputs (for reduce) or outputs (for map) switch phase &#123; case mapPhase: ntasks = len(mapFiles) n_other = nReduce case reducePhase: ntasks = nReduce n_other = len(mapFiles) &#125; fmt.Printf("Schedule: %v %v tasks (%d I/Os)\n", ntasks, phase, n_other) // All ntasks tasks have to be scheduled on workers. // Once all tasks have completed successfully, // schedule() should return. var wg sync.WaitGroup wg.Add(ntasks) for i := 0; i &lt; ntasks; i++ &#123; go func(index int) &#123; //rpc call args var args DoTaskArgs args.Phase = phase args.JobName = jobName args.NumOtherPhase = n_other args.TaskNumber = index if phase == mapPhase &#123; args.File = mapFiles[index] &#125; done := false for !done &#123; // registerChan yields a stream of registered workers worker := &lt;-registerChan done = call(worker, "Worker.DoTask", args, nil) go func()&#123; registerChan &lt;- worker &#125;() &#125; wg.Done() &#125;(i) &#125; wg.Wait() fmt.Printf("Schedule: %v done\n", phase)&#125; RaftRaft是一种共识算法，它在容错和性能上与Paxos等效，但更容易理解学习。可以通过 http://thesecretlivesofdata.com/raft/ 简单认知下Raft共识算法。 共识是容错分布式系统中的一个基本问题，共识涉及多个服务器就价值达成一致。共识通常出现在复制状态机的背景下，每个服务器都有一个状态机和一个日志，每个状态机都从其日志中获取输入命令。以哈希表为例，日志将包含诸如 set x=3 之类的命令，而共识算法要确保所有状态机都 x=3 作为第n个命令应用。即每个状态机处理相同系列的命令，从而产生相同系列的结果并到达相同系列的状态。 Raft PaperRaft 选举出一个leader来管理复制日志：leader从客户端接收日志条目，将其复制到其他服务器上，并在保证安全的时候应用日志条目到他们的状态机中。由此，Raft 将一致性问题分解成三个子问题：Leader Election, Log Replication, Safety（所有状态机都应执行相同的命令序列） Raft PropertiesRaft 在任何时候都保证以下的各个特性。 Election Safety：一个给定term最多能选出一位leader Leader Append-Only：leader不能删除或修改自身log中entries，只能追加新entries Log Matching：如果两个log在某一index处的entries具有相同term，则这两个log在该index前的所有entries相同。 Leader Completeness：一个committed的entry会始终存在于之后terms的leaders的log中。 State Machine Safety：若一个给定index的entry已经被某台服务器apply，则不会有服务器会在其log的index位置apply别的entry的。 Raft Basics在任何时刻，每一个服务器节点都处于这三个状态之一：leader、followers、candidates。通常系统中只有一个leader且其他节点全都是followers，在一个任期内，leader一直保持，直到自己宕机。Followers都是被动的：他们不会发送任何请求，只是简单的响应来自leader / candidates的请求。Leader处理所有的客户端请求（如果一个客户端和followers联系，那followers会把请求重定向给leader） Raft 把时间分割成任意长度的任期terms。任期用连续的整数标记。每一段任期从一次选举开始。 每一个节点存储一个当前任期号current term，这一编号在整个时期内单调的增长。当服务器之间通信的时候会交换current term；如果一个服务器的current term比其他人小，那么他会更新到较大的编号值。Raft 算法中服务器节点之间通信使用远程过程调用（RPCs），并且基本的一致性算法只需要两种类型的 RPCs。请求投票（RequestVote） RPCs 由候选人在选举期间发起，然后附加条目（AppendEntries）RPCs 由领导人发起，用来复制日志和提供一种心跳机制。 Leader electionRaft 使用一种heartbeat机制来触发领导人选举。当服务器程序启动时，各自都是followers。Leader会周期性的向所有followers发送心跳包 (AppendEntries RPCs that carry no log entries) 来维持自己的权威。Followers只要收到了来自 leader / candidates 的有效RPCs就会一直保持followers。但如果一个follower在一段时间（random，一般在150ms-300ms）里没有接收到任何消息，即选举超时，那他会认为系统中无可用leader，并发起选举以选出新的leader。 要开始一次选举过程，followers先要current term++且转换到candidates，然后并行的向集群中的其他服务器节点发送RequestVote的 RPCs 来给自己投票。Candidates会继续保持当前状态直到以下三件事情之一发生： (a) it wins the election：candidate从整个集群的大多数服务器节点获得了针对同一term的选票。每个服务器按先来先服务原则在每个term最多投一张选票。一旦candidate赢得选举，则立即成为leader并向其他的服务器发送heartbeat来建立自己的权威并且阻止产生别的leader。 (b) another server establishes itself as leader：等待投票时candidate收到别的服务器leader声明的AppendEntries RPC，如果该RPC中leader的term &gt;= candidate的term，则candidate承认该leader并回到follower；否则candidate会拒绝该RPC并保持candidate 。 (c) a period of time goes by with no winner：多个followers同时成为candidates来瓜分选票，导致无人能赢得大多数人的支持。为了避免瓜分次数过多，每个candidate在发RequestVote RPCs时都会设置一个随机的等待时间，若超时则重新发送RequestVote RPCs 。这样每次瓜分后，所有candidates都会超时，current term++开启新一轮选举。 Log replication客户端只和leader交互，leader把客户端指令作为一条log entry附加到自己log中，然后并行的发起 AppendEntries RPCs 给 followers，让他们复制这条entry。当这其被安全的复制，leader会将这条entry内容应用在自身状态机上并将其commit，同时return result给客户端。接下来leader会通知followers该entry已经committed，followers将其应用到各自状态机，完成各自log上该entry的commit。leader会不断的重复尝试 AppendEntries RPCs （尽管已经回复了客户端）直到所有的followers都最终存储了所有的日志条目。 日志由有序序号标记的条目组成。每个条目都包含创建时的任期号（图中框中的数字）和一个状态机需要执行的指令。一个条目当可以安全的被应用到状态机中去的时候，就认为是可以提交了。 在leader将创建的日志条目复制到大多数的服务器上的时候，此时可以认为该 log entry 应用到状态机中是安全的，即该 log entry is committed（如图中条目 7）。同时，leader的日志中之前的所有日志条目也都会被提交，包括之前所有terms里的条目。Leader跟踪 committed entries 的 highest index（例如图中为7），并将其写进未来所有的 AppendEntries RPCs 好让 followers 清楚。Followers会将 committed entries 按 log 顺序应用到本地的状态机中。 Raft 的日志机制维护了不同服务器的日志之间的高层次的一致性，同时也组成了上文中的日志匹配特性： 不同log中的两个entry拥有相同index&amp;term -&gt; 他们存了相同的指令且之前的所有entries也全部相同。 理由：leader会把前一个entry的index和term写进当前AppendEntries RPC中，如果followers发现在自己的log中找不到符合的entry，就会拒收这个新entry以保证其log和leader的log顺序一致。 正常操作中，logs肯定会保持一致性。但存在这种情况，leader在自己log中写入了一堆entries但在这些entries还没全部committed时leader就先自己崩溃了，下一term它作为follower，就会出现和当前leader的log不一致情况。Raft的解决办法是直接把followers冲突的那部分entries覆盖成leader的对应entries。 Leader针对每一个follower维护了一个 nextIndex（即将要发给followers的entry的index）当一个leader刚获得权力的时候，初始化所有的 nextIndex = leader log highest index + 1。如果出现AppendEntries RPC被followers拒绝的情况，leader会让 nextIndex– 并重试。最终 nextIndex 会停在两者log一致的点，此时的AppendEntries RPC将不再被拒绝，即可followers冲突的entries全部删除并逐个追加leader的entries。通过这种机制，leader在获得权力的时候就不需要任何特殊的操作来恢复一致性。 Safety我们需要保证每个状态机都会按照相同的顺序执行相同的指令。但如果出现，一些entries被大部分followers接受并commit，但存在一个follower处于unavailable没有接受这些entries，但偏偏该follower在下一term成为了leader，这就会导致那些committed entries明明执行了却被强制删除，即不同状态机执行不同的指令序列。 Raft通过在leader election中增加一些限制来保证：任何leader对于给定的term，都拥有之前terms里所有的committed entries (即上文提到的 the Leader Completeness Property) 在raft的投票环节，每个candidates发出的RequestVote RPC都会包含其last log entry的term和index，followers会根据这些信息拒绝掉没有自己新（term越大越新，term相同index越大约新）的投票请求。这样选出来的leader至少和大部分followers一样新，而entries是在大部分followers都复制后才会commit，即committed entries存在于大部分followers上，如果能和大部分followers一样新，就能保证存储了所有的committed entries。 Leader不能断定之前term中保存到大部分服务器的entry是否已经commit，如图，S1作leader，开始复制entry_2，复制了小部分followers后换leader了。S5作leader，没有entry_2且刚接受到客户端的entry_3，继续换leader。S1作leader，继续复制那条entry_2并成功复制到大部分followers上，但在entry_2即将commit前，又被换leader。若S5作leader（因为entry_3的term&gt;entry_2的term，所以能收到S2,S3,S4的选票），此时entry_3会覆盖所有的entry_2；若S1继续作leader，则S1的entries都会被提交。 leader复制之前term的entries时，这些entries的term不变（图c复制entry_2时，其term依旧为2 ） 由此，对于之前term中的entries，leader不会通过计算其副本数目的方式去commit，事实上leader只会对当前term的entries用计算副本数目的方式来commit。而当前term的entries被commit，由于日志匹配特性，之前的entries都会被间接commit。 Condenced Summary AppendEntries RPC由leader负责调用来复制日志指令；也会用作heartbeat，receiver实现： 如果 term &lt; currentTerm 就返回 false 如果日志在 prevLogIndex 位置处的 entry 和 prevLogTerm 不匹配，则返回 false 如果已经存在的日志条目和新的产生冲突（index相同但term不同），删除这一条和之后所有的 entries 附加日志中尚未存在的任何新条目 如果 leaderCommit &gt; commitIndex，令 commitIndex = MIN(leaderCommit, 新日志条目索引值) RequestVote RPC由candidates负责调用来征集选票，receiver实现： 如果term &lt; currentTerm返回 false 如果 votedFor 为 null / candidateId，且candidate的日志至少和receiver一样新，那就投票给他 Rules for ServersAll Servers 如果commitIndex &gt; lastApplied，那就 lastApplied ++，并把log[lastApplied]应用到状态机中 如果接收到的RPC请求或响应中，termT &gt; currentTerm，则令currentTerm = T，并切换状态为followers Followers 响应来自leader和candidates的RPCs 如果在选举超时前一直没有收到leader或candidates的RPCs，就自己变成candidate Candidates 在转变成candidates后就立即开始选举过程 自增当前的任期号（currentTerm） 给自己投票 重置选举计时器 发送请求投票的 RPC 给其他所有服务器 如果接收到大多数服务器的选票，那么就变成领导人 如果接收到来自新的领导人的 AppendEntries RPC，转变成跟随者 如果选举过程超时，再次发起一轮选举 Leader 一旦成为领导人：发送空的AppendEntries RPC (heartbeat) 给其他所有的服务器；在一定的空余时间之后不停的重复发送，以阻止followers超时 如果接收到来自客户端的请求：附加条目到本地日志中，在条目被应用到状态机后响应客户端 如果对于一个跟随者，最后日志条目的index &gt;= nextIndex，则发送从 nextIndex 开始的所有日志条目： 如果成功：更新相应 followers 的 nextIndex 和 matchIndex 如果因为日志不一致而失败，减少 nextIndex 重试 如果存在一个N满足N &gt; commitIndex，且大多数的matchIndex[i] ≥ N成立，并且log[N].term == currentTerm成立，则令commitIndex = N Cluster membership changes实际操作中偶尔会改变集群的配置，但服务器直接从旧配置转换到新配置很容易出现分歧。为避免存在新旧配置在同一时间下出现各自leader，即同一时刻新旧配置同时生效的情况，配置更改必须使用两阶段方法。在 Raft 中，集群先切换到一个过渡的配置，称之为共同一致；一旦共同一致被commit，那系统就切换到新配置上。共同一致是老配置和新配置的结合： 日志条目会被复制给集群中新、老配置的所有服务器。 新、旧配置的服务器都可以成为leader。 达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。 当leader接收到一个改变配置从 C-old 到 C-new 的请求，他会创建configuration entry: C-old,new来存储配置，并复制给followers。各个服务器总是用latest configuration entry作为自己的配置，无论他是否committed。 一旦 C-old,new 被提交（C-old，C-new双方大部分都复制成功），那么无论是 C-old 还是 C-new，在没有经过对方批准的情况下都不可能做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。 在关于重新配置还有三个问题需要提出： 新的服务器可能初始化没有存储任何的日志条目。此时他们需要时间来更新追赶，而没法提交新的条目。Raft 在配置更新之前使用了一种额外的阶段：该阶段里，新的服务器加入，接收leader的entries但无投票权，直到追赶上了集群中的其他机器。 当前leader不在C-new中，即当前leader节点即将下线。在C-new被commit之后Leader实际已经从集群中脱离，会退位到follower状态，此时可以对Leader节点进行下线操作，而新集群则会在C-new的配置下重新选举出一个Leader。 移除不在 C-new 中的服务器可能会扰乱集群。这些服务器因为不在C-new里，所以leader根本不会给他们发心跳，之后这些节点就会选举超时，发送term++的请求投票 RPCs，这样会导致当前leader回退成follower。为了避免这种情况，除非leader超时，不然各节点拒收投票请求RPCs。同时在每次选举前等待一个选举超时时间，这样每次旧节点在发起选举前需要等待一段时间，那这段时间新Leader可以发送心跳，减少影响。 Log compactionRaft 需要一定的机制去清除日志里积累的陈旧的信息，快照Snapshotting是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃。每个服务器独立的创建快照，快照只包括committed entries。Raft 包含状态机状态和少量元数据（last index&amp;term）到快照中，保留元数据是为了支持快照后第一个条目的附加日志请求时的一致性检查，因为这个条目需要前一日志条目的索引值和任期号。为了支持集群成员更新，快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。 但当followers运行缓慢或作为C-new刚加入集群时，需要leader发送快照让他们更新到最新状态。这种情况下leader使用 InstallSnapshot RPCs 来发送快照。当followers通过这种 RPC 接收到快照时，会决定如何处理log：通常快照会含当前log没有的信息，此时会丢弃整个log，用快照代替；也可能接收到的快照是自己日志的前面部分（网络重传或者错误），那么被快照包含的entries会被全部删除，但快照后面的entries仍然有效，必须保留。 Client interactionRaft 要求客户端只和leader交互，所以当客户端请求到follower时，如果集群里存在leader，follower会将leader地址发给客户端；如果此时集群里无leader，客户端会等待、重试。 Raft 的目标是要实现线性化语义(each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response)。但如果，leader在commit该entry之后，回复客户端之前crash，那么客户端会和新leader重试这条指令，导致这条命令就被再次执行。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。 只读操作可以直接处理而不用写进日志。但在不增加任何限制的情况下，这么做可能会返回脏数据，因为leader可能在response客户端时被退回成follower，此时原先leader的信息已经变脏。线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。 Leader必须有关于被提交日志的最新信息。在leader任期开始的时候，他可能不知道哪些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条日志条目。Raft 中通过leader在任期开始时commit一个空白entry到日志中去来实现。 Leader在处理只读的请求前必须检查自己是否已经被废黜了。Raft 中通过leader在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。]]></content>
      <categories>
        <category>Sys</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>Go</tag>
        <tag>6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go与并发]]></title>
    <url>%2Fpost%2F28fa.html</url>
    <content type="text"><![CDATA[goroutinegoroutine是Go并行设计的核心。goroutine算是协程，它比线程更小，十几个goroutine可能体现在底层就是五六个线程，Go语言内部实现了这些goroutine之间的内存共享。执行goroutine只需极少的栈内存(大概是4~5KB)。goroutine比thread更易用、更高效、更轻便。 goroutine是通过Go的runtime管理的一个线程管理器。goroutine通过go关键字实现，类似个普通函数。 123456789101112131415161718package mainimport ( "fmt" "runtime")func say(s string) &#123; for i := 0; i &lt; 5; i++ &#123; runtime.Gosched() //让CPU把时间片让给别人,下次某个时候继续恢复执行该goroutine fmt.Println(s) &#125;&#125;func main() &#123; go say("world") //开一个新的Goroutines执行 say("hello") //当前Goroutines执行&#125; 概念 说明 进程 一个程序对应一个独立程序空间 线程 一个执行空间，一个进程可以有多个线程 逻辑处理器 执行创建的goroutine，绑定一个线程 调度器 Go运行时中的，分配goroutine给不同的逻辑处理器 全局运行队列 所有刚创建的goroutine都会放到这里 本地运行队列 逻辑处理器的goroutine队列 当我们创建一个goroutine的后，会先存放在全局运行队列中，等待Go运行时的调度器进行调度，把他们分配给其中的一个逻辑处理器，并放到这个逻辑处理器对应的本地运行队列中，最终等着被逻辑处理器执行即可。 这一套管理、调度、执行goroutine的方式称之为Go的并发。那怎么做到Go的并行呢？多创建一个逻辑处理器就好了，这样调度器就可以同时分配全局运行队列中的goroutine到不同的逻辑处理器上并行执行。 1234567891011121314151617func main() &#123; var wg sync.WaitGroup wg.Add(2) go func()&#123; defer wg.Done() for i:=1;i&lt;100;i++ &#123; fmt.Println("A:",i) &#125; &#125;() go func()&#123; defer wg.Done() for i:=1;i&lt;100;i++ &#123; fmt.Println("B:",i) &#125; &#125;() wg.Wait()&#125; 这里的sync.WaitGroup其实是一个计数的信号量，使用它的目的是要main函数等待两个goroutine执行完成后再结束，不然这两个goroutine还在运行的时候，程序就结束了，看不到想要的结果。 sync.WaitGroup的使用非常简单，先使用Add 方法设置计算器为2，每个goroutine的函数执行完后就调用Done方法减1（使用延迟语句defer完成逻辑）。Wait方法，在计数器大于0时阻塞，所以main 函数会一直等待2个goroutine完成后，再结束。 默认情况下，Go默认是给每个可用的物理处理器都分配一个逻辑处理器，因为我的电脑是4核的，所以上面的例子默认创建了4个逻辑处理器，所以这个例子中同时也有并行的调度。 上面多个goroutine运行在同一个进程里，共享内存数据，设计上遵循：不通过共享来通信，而通过通信来共享。 竞争有并发，就有资源竞争，如果两个或者多个goroutine在没有相互同步的情况下，访问某个共享的资源，比如同时对该资源进行读写时，就会处于相互竞争的状态，即并发中的资源竞争。我们对同一个资源的读写必须是原子化的，即同一时间只能有一个goroutine对共享资源进行读写操作。 共享资源竞争的问题，Go为我们提供了一个工具帮助我们检查，这个就是go build -race命令。我们在当前项目目录下执行这个命令，生成一个可以执行文件，然后再运行这个可执行文件，就可以看到打印出的检测信息。 mutexsync包里提供了一种互斥型的锁sync.Mutex，可以控制哪些代码同一时段下，只能有一个goroutine访问，被sync互斥锁控制的这段代码范围，被称之为临界区。同时sync包还提供一种读写锁sync.RWMutex，设计有RLock()等四种方法，下面用经典的读者写者问题来举例RWMutex的用法。 12345678910111213141516171819202122232425262728var ( mutex sync.RWMutex wg sync.WaitGroup content string)func read() &#123; defer wg.Done() mutex.RLock() fmt.Println(content) mutex.RUnlock()&#125;func write(text string) &#123; defer wg.Done() mutex.Lock() content += text mutex.Unlock()&#125;func main() &#123; wg.Add(4) go write("hhh") go read() go write("hhh") go read() wg.Wait()&#125; channels在多个goroutine并发中，我们不仅可以通过原子函数和互斥锁保证对共享资源的安全访问，消除竞争的状态，还可以通过使用通道，在多个goroutine发送和接受共享的数据，达到数据同步的目的。 goroutine间数据的通信机制为channel，可以与Unix shell中双向管道做类比：可以通过它发送或者接收值，这些值只能是channel类型。定义一个channel时，需要定义发送到channel的值的类型。必须使用make 创建channel： 123ci := make(chan int)cs := make(chan string)cf := make(chan interface&#123;&#125;) channel通过操作符&lt;-来接收和发送数据 12ch &lt;- v // 发送v到channel ch.v := &lt;-ch // 从ch中接收数据，并赋值给v 默认情况下，channel接收和发送数据都是阻塞的，除非另一端已经准备好，这样就使得Goroutines同步变的更加的简单，而不需要显式的lock。所谓阻塞，也就是如果读取（value := &lt;-ch）它将会被阻塞，直到有数据接收。其次，任何发送（ch&lt;-5）将会被阻塞，直到数据被读出。无缓冲channel是在多个goroutine之间同步很棒的工具。 Buffered Channels上面为Go默认的非缓存类型的channel，Go也允许指定channel的缓冲大小，即channel可以存储多少元素。 ch:= make(chan bool, 4)创建了可以存储4个元素的bool 型channel。这个channel 中前4个元素可以无阻塞的写入。当写入第5个元素时，代码将会阻塞，直到其他goroutine从channel 中读取一些元素，腾出空间。 1ch := make(chan type, value) 当 value = 0 时，channel 是无缓冲阻塞读写的，当value &gt; 0 时，channel 有缓冲、是非阻塞的，直到写满 value 个元素才阻塞写入。我们可以用最为经典的生产者消费者来举例buffered channels的用法。 123456789101112131415161718192021222324var ch chan int = make(chan int, 20)func produce(max int, num int) &#123; for i := 0; i &lt; max; i++ &#123; fmt.Println("Send", num, ": ", i) ch &lt;- i time.Sleep(5) &#125;&#125;func consumer(max int, num int) &#123; for i := 0; i &lt; max; i++ &#123; v := &lt;-ch fmt.Println("Receive", num, ": ", v) &#125;&#125;func main() &#123; go produce(5, 1) go consumer(10, 1) go consumer(5, 2) go produce(10, 2) time.Sleep(1 * time.Second)&#125; Rangefor i := range c能够不断的读取channel里面的数据，直到该channel被显式的关闭。 Close我们可以通过内置函数close关闭channel，一般在生产方使用。关闭channel之后就无法再发送任何数据了，在消费方可以通过语法v, ok := &lt;-ch测试channel是否被关闭。如果ok返回false，那么说明channel已经没有任何数据并且已经被关闭。 Select如果存在多个channel的时候，Go里面提供了一个关键字select，通过select可以监听channel上的数据流动。 select默认是阻塞的，只有当监听的channel中有发送或接收可以进行时才会运行，当多个channel都准备好的时候，select是随机的选择一个执行的。 123456789101112131415161718192021222324func fibonacci(c, quit chan int) &#123; x, y := 1, 1 for &#123; select &#123; case c &lt;- x: x, y = y, x + y case &lt;-quit: fmt.Println("quit") return &#125; &#125;&#125;func main() &#123; c := make(chan int) quit := make(chan int) go func() &#123; for i := 0; i &lt; 10; i++ &#123; fmt.Println(&lt;-c) &#125; quit &lt;- 0 &#125;() fibonacci(c, quit)&#125; 在select里面还有default语法，select其实就是类似switch的功能，default就是当监听的channel都没有准备好的时候，默认执行的（select不再阻塞等待channel）。 123456select &#123;case i := &lt;-c: // use idefault: // 当c阻塞的时候执行这里&#125; 超时有时候会出现goroutine阻塞的情况，为了避免整个程序进入阻塞，可以利用select来设置超时 1234567891011121314151617func main() &#123; c := make(chan int) o := make(chan bool) go func() &#123; for &#123; select &#123; case v := &lt;- c: println(v) case &lt;- time.After(5 * time.Second): println("timeout") o &lt;- true break &#125; &#125; &#125;() &lt;- o&#125; runtime goroutineruntime包中有几个处理goroutine的函数： Goexit 退出当前执行的goroutine，但是defer函数还会继续调用 Gosched 让出当前goroutine的执行权限，调度器安排其他等待的任务运行，并在下次某个时候从该位置恢复执行。 NumCPU 返回 CPU 核数量 NumGoroutine 返回正在执行和排队的任务总数 GOMAXPROCS 用来设置可以并行计算的CPU核数的最大值，并返回之前的值。 并发控制控制并发有两种经典的方式，一种是WaitGroup，另外一种就是Context。 WaitGroup如上面用到的，waitGroup尤其适用于多个goroutine协同做一件事情的时候，因为每个goroutine做的都是这件事情的一部分，只有全部的goroutine都完成，这件事情才算是完成，即wait模式。 Context对于一个网络请求Request，每个Request都需要开启一个goroutine做一些事情，这些goroutine又可能会开启其他的goroutine。所以我们需要一种可以跟踪goroutine的方案，才可以达到控制他们的目的，Go提供了Context，即goroutine的上下文。 context.Background() 返回一个空的Context，这个空的Context一般用于整个Context树的根节点。我们使用context.WithCancel(parent)函数，创建一个可取消的子Context，然后当作参数传给goroutine使用，这样就可以使用这个子Context跟踪这个goroutine。 在goroutine中，使用select调用&lt;-ctx.Done()判断是否要结束，如果接受到值的话，就可以返回结束goroutine了；如果接收不到，就会继续进行监控。 cancel函数是由调用context.WithCancel(parent)函数生成子Context的时候返回的，，它是CancelFunc类型的。调用它就可以发出取消指令，然后我们的监控goroutine就会收到信号，就会返回结束。 Done是Context接口常用的方法，如果Context取消的时候，我们就可以得到一个关闭的chan，关闭的chan是可以读取的，所以只要可以读取的时候，就意味着收到Context取消的信号了 12345678910111213141516171819202122232425func main() &#123; ctx, cancel := context.WithCancel(context.Background()) go watch(ctx,"【监控1】") go watch(ctx,"【监控2】") go watch(ctx,"【监控3】") time.Sleep(10 * time.Second) fmt.Println("可以了，通知监控停止") cancel() //为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second)&#125;func watch(ctx context.Context, name string) &#123; for &#123; select &#123; case &lt;-ctx.Done(): fmt.Println(name,"监控退出，停止了...") return default: fmt.Println(name,"goroutine监控中...") time.Sleep(2 * time.Second) &#125; &#125;&#125; 示例中启动了3个监控goroutine进行不断的监控，每一个都使用了Context进行跟踪，当我们使用cancel函数通知取消时，这3个goroutine都会被结束。这就是Context的控制能力，它就像一个控制器一样，按下开关后，所有基于这个Context或者衍生的子Context都会收到通知，这时就可以进行清理操作了，最终释放goroutine，优雅的解决了goroutine启动后不可控的问题。 Context接口Deadline方法是获取设置的截止时间的意思，第一个返回式是截止时间，到了这个时间点，Context会自动发起取消请求；第二个返回值ok==false时表示没有设置截止时间，如果需要取消的话，需要调用取消函数进行取消。 Done方法返回一个只读的chan，类型为struct{}，我们在goroutine中，如果该方法返回的chan可以读取，则意味着parent context已经发起了取消请求，我们通过Done方法收到这个信号后，就应该做清理操作，然后退出goroutine，释放资源。 Err方法返回取消的错误原因，因为什么Context被取消。Done和Err两者搭配起来就是常用经典用法： 12345678910111213func Stream(ctx context.Context, out chan&lt;- Value) error &#123; for &#123; v, err := DoSomething(ctx) if err != nil &#123; return err &#125; select &#123; case &lt;-ctx.Done(): return ctx.Err() case out &lt;- v: &#125; &#125; &#125; Value方法获取该Context上绑定的值，是一个键值对，所以要通过一个Key才可以获取对应的值，这个值一般是线程安全的。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>Tips</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2Fpost%2Fc018.html</url>
    <content type="text"><![CDATA[Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。 Docker一般来说，先写Dockerfile文件并用此来创建image文件： 1$ docker image build -t demo:0.0.1 然后生成容器： 1$ docker container run -p 8000:3000 -it demo:0.0.1 /bin/bash -p参数：容器的 3000 端口映射到本机的 8000 端口。 -it参数：容器的 Shell 映射到当前的 Shell，然后你在本机窗口输入的命令，就会传入容器。 demo:0.0.1：image 文件的名字（如果有标签，还需要提供标签，默认是 latest 标签）。 /bin/bash：容器启动以后，内部第一个执行的命令。这里是启动 Bash，保证用户可以使用 Shell。 不过Node 进程运行在 Docker 容器的虚拟环境里面，进程接触到的文件系统和网络接口都是虚拟的，与本机的文件系统和网络接口是隔离的，因此需要定义容器与物理机的端口映射（map）才能在本机localhost访问到。 1234# 在本机的另一个终端窗口，查出容器的 ID$ docker container ls# 停止指定的容器运行$ docker container kill [containerID] 容器停止运行之后，并不会消失，用下面的命令删除容器文件。 1234# 查出容器的 ID$ docker container ls --all# 删除指定的容器文件$ docker container rm [containerID] 也可以使用docker container run命令的--rm参数，在容器终止运行后自动删除容器文件。 1$ docker container run --rm -p 8000:3000 -it demo:0.0.1 /bin/bash Vue对于Vue前端，Dockerfile位于项目的docker文件夹下，同时有nginx.conf配置文件。 DockerfileFROM nginx:alpine COPY ./nginx.conf /etc/nginx/conf.d/default.confCOPY ./dist /usr/share/nginx/html Docker 需要依赖于一个基础镜像进行构建运行容器，我们指定nginx的alpine版本作为基础镜像。使用COPY 指令从Docker主机复制内容和配置文件。不过记住，需要复制的目录一定要放在 Dockerfile 文件的同级目录下。 nginx.conf server { listen 80; location / { root /usr/share/nginx/html; index index.html; } } 在80端口监听，访问/相当于访问容器的/usr/share/nginx/html shellcd ..yarn buildcd dockermv ../dist ./distsudo docker login --username=stardust567 registry.cn-shanghai.aliyuncs.comsudo docker build -t registry.cn-shanghai.aliyuncs.com/stardust567/infoweb:$1 ./sudo docker push registry.cn-shanghai.aliyuncs.com/stardust567/infoweb:$1 在根目录编译vue项目于默认的dist文件夹下并将dist文件夹移入docker文件夹下，创建镜像并push上阿里云（$1为标签名） Go对于go后端，Dockerfile文件直接位于项目根目录下。 DockerfileFROM golang as build ENV GOPROXY=https://goproxy.cn ADD . /infoweb WORKDIR /infoweb RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o api_server FROM alpine:3.7 ENV REDIS_ADDR=&quot;&quot;ENV REDIS_PW=&quot;&quot;ENV REDIS_DB=&quot;&quot;ENV MysqlDSN=&quot;&quot;ENV GIN_MODE=&quot;release&quot;ENV PORT=3000 RUN echo &quot;http://mirrors.aliyun.com/alpine/v3.7/main/&quot; &gt; /etc/apk/repositories &amp;&amp; \ apk update &amp;&amp; \ apk add ca-certificates &amp;&amp; \ echo &quot;hosts: files dns&quot; &gt; /etc/nsswitch.conf &amp;&amp; \ mkdir -p /www/conf WORKDIR /www COPY --from=build /infoweb/api_server /usr/bin/api_serverADD ./conf /www/conf RUN chmod +x /usr/bin/api_server ENTRYPOINT [&quot;api_server&quot;] 将当前目录下文件拷入docker并作为工作目录，在linux环境下编译go文件生成api_server文件。 再生成一个基础镜像并初始化一下并创建拷贝conf文件夹，将上个镜像的api_server拷贝并赋予权限，最后在启动容器时执行api_server文件。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips]]></title>
    <url>%2Fpost%2F5f09.html</url>
    <content type="text"><![CDATA[学习路上总会遇到各种坑，之前一直搞完就算了，总觉得很可惜，就有了记录踩的各种坑的想法…… 未完待续（不断填坑）中…… Git 如果你add &amp; commit过完看命令行发现自己智障了还好没push上去。 git reset –mixed HEAD^ 如果commit&amp;push后error: src refspec matser does not match any. git push –set-upstream origin master Python 打开项目所在文件目录导出python库依赖requirements.txt pipreqs ./ –encoding=utf-8 这样之后有人需要用这个项目的时候直接打开命令行输入以下即可： pip install -r requirements.txt 解决matplotlib中文图例乱码问题 12345678## Go1. 添加 Go Modules Replace的时候，replace配置项的路径只能是`~`根目录开头，`.`当前目录开头，`..`上级目录开头，这点在WSL上就还是安利后两者，毕竟在根目录下VScode估计找不到文件夹orz2. 为变量给予附属属性`json`，这样子在`c.JSON`(c *gin.Context)的时候就会自动转换格式，非常的便利。 ```go Name string `json:&quot;name&quot;` Docker Windows下用docker端口进行映射，但打开localhost:port无法访问对应的服务。 因为docker是运行在Linux上的，在Windows中运行实际上还是在Windows下Linux环境中运行的docker。即服务中使用的localhost指的是这个Linux环境的地址，而非宿主环境Windows。我们可以通过命令docker-machine ip defaul找到这个Linux环境的IP地址，一般是192.168.99.100（比如我） MySQL 如果是WSL的话，MySQL被设计为可以使用大写字母跨平台正常工作，其中数据库名称/表名和/或列为小写字母可能会给操作系统带来麻烦，并且可能会生成相同的二进制表文件。简单点，会出bug不建议WSL建议直接windows上MySQL，如果真的想WSL的话（比如我），当Cannot stat file /proc/1/fd/5: Operation not permitted时，不要慌，重启下bash然后输入： sudo service mysql start sudo dpkg –configure -a 然后你可能会像我一样发现Error: Access denied for user ‘root‘@’localhost’莫慌： sudo mysql ALTER USER ‘root‘@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘root’; 这样再进入的时候usr:root, psd:root 就ok了 mysql -u root -p 接下来就可以愉快地创建新用户，然后赋予并刷新权限了： mysql &gt;CREATE USER ‘newuser‘@’localhost’ IDENTIFIED BY ‘password’; mysql&gt; GRANT ALL PRIVILEGES ON . TO ‘newuser‘@’localhost’; mysql &gt;FLUSH PRIVILEGES; 如果发现ERROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/var/run/mysqld/mysqld.sock’ (2)那是因为没开服务，打开服务sudo service mysql start即可。 用pymysql时候(pymysql.err.InternalError) (1366, “Incorrect string value: ‘\xE6\xAD\xA3\xE5\xB8\xB8’中文编码问题，修改即可。 mysql&gt; alter table recruit convert to character set utf8mb4; 用pymysql时候，想借MySQL函数STR_TO_DATE传DATE类型数据，记得年月日用%%而不是%： sql = ‘insert into recruit_info(title,time) VALUES (%s,STR_TO_DATE(%s, “%%Y-%%m-%%d”))’ PostgreSQL 打开pgAdmin如果出现无法连接并要求输入postgres密码的情况，请打开任务管理器&gt;服务，找到并启动postgresql-x64-10即可。 Redis WSL下安装Redis的话，要记得修改/etc/redis/redis.conf这个文件，将supervised no修改为supervised systemd即可，ubuntu用systemd初始系统。 如果连不上，第一件事就是启动服务试试，sudo service redis-server start Vue.js 刚开始我在修改代码中途发现自己没下vue-router强停后npm install结果errno -4048 npm ERR! syscall rename，解决方法npm cache clean --force后npm install即可。 Terminal 用tree直接导出当前目录下的目录结构： tree -L 2 &gt; test.txt 不想每次都输服务器密码（懒）怎么办： ssh-copy-id USER_NAME@IP_ADDRESS]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go与Web]]></title>
    <url>%2Fpost%2F6e5d.html</url>
    <content type="text"><![CDATA[Go日记的Web篇外传正式开张了，可喜可贺、可喜可贺！ 这篇简单介绍下Go的Web基础，有计网知识或者写过前后端应该会很好上手。 Web基础web工作方式的几个概念Request：用户请求的信息，用来解析用户的请求信息，包括post、get、cookie、url等信息Response：服务器需要反馈给客户端的信息Conn：用户的每次请求链接Handler：处理请求和生成返回信息的处理逻辑 一个简单的例子使用go语言可以采用net/http包进行web搭建，like this： 1234567891011121314151617181920212223242526272829package mainimport ( "fmt" "net/http" "strings" "log")func sayhelloName(w http.ResponseWriter, r *http.Request) &#123; r.ParseForm() //解析参数，默认是不会解析的 fmt.Println(r.Form) //这些信息是输出到服务器端的打印信息 fmt.Println("path", r.URL.Path) fmt.Println("scheme", r.URL.Scheme) fmt.Println(r.Form["url_long"]) for k, v := range r.Form &#123; fmt.Println("key:", k) fmt.Println("val:", strings.Join(v, "")) &#125; fmt.Fprintf(w, "Hello astaxie!") //这个写入到w的是输出到客户端的&#125;func main() &#123; http.HandleFunc("/", sayhelloName) //设置访问的路由 err := http.ListenAndServe(":9090", nil) //设置监听的端口 if err != nil &#123; log.Fatal("ListenAndServe: ", err) &#125;&#125; 执行web.exe，此时在9090端口开始监听http链接请求打开http://localhost:9090即可看到浏览器页面输出Hello astaxie! 我们可以对比localhost:9090/?url_long=111&amp;url_long=222下terminal的输出： map[]path /scheme[]map[]path /favicon.icoscheme[] map[url_long:[111 222]]path /scheme[111 222]key: url_longval: 111222map[]path /favicon.icoscheme[] http://localhost:9090 http://localhost:9090/?url_long=111&amp;url_long=222 http包运行机制http包执行流程： 创建Listen Socket, 监听指定端口, 等待客户端请求。 Listen Socket接受客户端请求, 得到Client Socket, 接下来通过Client Socket与客户端通信。 处理客户端请求, 首先从Client Socket读取HTTP请求的协议头, 如果是POST方法, 还可能要读取客户端提交的数据, 然后交给相应的handler处理请求, handler处理完毕准备好客户端需要的数据, 通过Client Socket写给客户端。 Go是如何让Web运行起来：如何监听端口？如何接收客户端请求？如何分配handler？ Go通过函数ListenAndServe来处理，即例子中err := http.ListenAndServe(&quot;:9090&quot;, nil)具体如下：先初始化一个server对象，然后调用函数net.Listen(&quot;tcp&quot;, addr)，即底层用TCP协议搭建了一个服务，然后监控我们设置的端口。 下面代码来自Go的http包的源码，可以看到整个的http处理过程：1234567891011121314151617181920212223242526272829func (srv *Server) Serve(l net.Listener) error &#123; defer l.Close() var tempDelay time.Duration // how long to sleep on accept failure for &#123; rw, e := l.Accept() if e != nil &#123; if ne, ok := e.(net.Error); ok &amp;&amp; ne.Temporary() &#123; if tempDelay == 0 &#123; tempDelay = 5 * time.Millisecond &#125; else &#123; tempDelay *= 2 &#125; if max := 1 * time.Second; tempDelay &gt; max &#123; tempDelay = max &#125; log.Printf("http: Accept error: %v; retrying in %v", e, tempDelay) time.Sleep(tempDelay) continue &#125; return e &#125; tempDelay = 0 c, err := srv.newConn(rw) if err != nil &#123; continue &#125; go c.serve() &#125;&#125; 监控之后如何接收客户端的请求？ 上面代码执行监控端口之后，调用函数srv.Serve(net.Listener)处理接收客户端的请求信息。 这个函数里面起了一个for{}，首先通过Listener接收请求，其次创建一个Conn，最后单独开了一个goroutine，把这个请求的数据当做参数扔给这个conn去服务：go c.serve()体现高并发，用户的每一次请求都是在一个新的goroutine去服务，互不影响。 如何具体分配到相应的函数来处理请求呢？ conn首先会解析request:c.readRequest(),然后获取相应的handler:handler := c.server.Handler，也就是我们刚才在调用函数err := http.ListenAndServe(&quot;:9090&quot;, nil)时候的第二个参数，传递nil即为空，那么默认获取handler = DefaultServeMux,这个变量就是一个路由器，用来匹配url跳转到其相应的handle函数，这个变量的设置，在main的第一句http.HandleFunc(&quot;/&quot;, sayhelloName)里，相当于注册了请求/的路由规则，当请求uri为”/“，路由就会转到函数sayhelloName。DefaultServeMux会调用ServeHTTP方法，这个方法内部感觉就是sayhelloName方法的callback，最后将信息写入response反馈到client。 http连接处理的具体流程可以如下显示： Go的http包详解上面是过个系统性的流程（类似复习计网的感觉）接下来详谈http包： 先谈下Go的http包的两个核心功能：Conn、ServeMux Conn的goroutineGo为了实现高并发和高性能, 使用goroutines处理Conn的读写事件,这样每个请求都能保持独立，相互不会阻塞，可以高效的响应网络事件。 Go在等待客户端请求里面是这样写的： 12345c, err := srv.newConn(rw)if err != nil &#123; continue&#125;go c.serve() client的每次请求都会创建一个Conn，这个Conn里面保存了该次请求的信息，然后再传递到对应的handler，该handler中便可以读取到相应的header信息，保证了每个请求的独立性。 ServeMux的自定义路由器的结构之前描述conn.server时，其实内部是调用了http包默认的路由器，通过路由器把本次请求的信息传递到了后端的处理函数。该路由器的结构如下： 12345type ServeMux struct &#123; mu sync.RWMutex //锁，由于请求涉及到并发处理，因此这里需要一个锁机制 m map[string]muxEntry // 路由规则，一个string对应一个mux实体，这里的string就是注册的路由表达式 hosts bool // 是否在任意的规则中带有host信息&#125; 下面看一下muxEntry的结构： 12345type muxEntry struct &#123; explicit bool // 是否精确匹配 h Handler // 这个路由表达式对应哪个handler pattern string //匹配字符串&#125; 接着看一下Handler的定义，标准的interface，但实际使用时并不需要显示继承该接口。 123type Handler interface &#123; ServeHTTP(ResponseWriter, *Request) // 路由实现器&#125; 在http包里定义了一个类型HandlerFunc,我们定义的函数sayhelloName就是这个HandlerFunc调用之后的结果http.HandleFunc(&quot;/&quot;, sayhelloName)，这个类型默认实现了ServeHTTP接口，即我们调用了HandlerFunc(f),强制类型转换f成为HandlerFunc类型，这样f就拥有了ServeHTTP方法。不过这个使用时可以不管，但学习时候还是清楚一下比较好（小声） 123456type HandlerFunc func(ResponseWriter, *Request)// ServeHTTP calls f(w, r).func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) &#123; f(w, r)&#125; 具体请求的分发路由器里面存储好了相应的路由规则之后，如何实现具体请求的分发？默认的路由器实现了ServeHTTP： 123456789func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) &#123; if r.RequestURI == "*" &#123; w.Header().Set("Connection", "close") w.WriteHeader(StatusBadRequest) return &#125; h, _ := mux.Handler(r) h.ServeHTTP(w, r)&#125; 如上所示路由器接收到请求之后，如果是*那么关闭链接，不然调用mux.Handler(r)返回对应设置路由的处理Handler，然后执行h.ServeHTTP(w, r)也就是调用对应路由的handler的ServerHTTP接口。 那么mux.Handler(r)怎么处理的呢？ 1234567891011121314151617181920212223242526func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) &#123; if r.Method != "CONNECT" &#123; if p := cleanPath(r.URL.Path); p != r.URL.Path &#123; _, pattern = mux.handler(r.Host, p) return RedirectHandler(p, StatusMovedPermanently), pattern &#125; &#125; return mux.handler(r.Host, r.URL.Path)&#125;func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) &#123; mux.mu.RLock() defer mux.mu.RUnlock() // Host-specific pattern takes precedence over generic ones if mux.hosts &#123; h, pattern = mux.match(host + path) &#125; if h == nil &#123; h, pattern = mux.match(path) &#125; if h == nil &#123; h, pattern = NotFoundHandler(), "" &#125; return&#125; 根据用户请求的URL和路由器里面存储的map去匹配的，当匹配到之后返回存储的handler，调用这个handler的ServeHTTP接口就可以执行到相应的函数了。 以上是整个路由过程，Go其实支持外部实现的路由器 ListenAndServe的第二个参数就是用以配置外部路由器的，它是一个Handler接口，即外部路由器只要实现了Handler接口就可以,我们可以在自己实现的路由器的ServeHTTP里面实现自定义路由功能。 如下代码所示，我们可以自己实现一个简易的路由器： 1234567891011121314151617181920212223242526package mainimport ( "fmt" "net/http")type MyMux struct &#123;&#125;func (p *MyMux) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; if r.URL.Path == "/" &#123; sayhelloName(w, r) return &#125; http.NotFound(w, r) return&#125;func sayhelloName(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, "Hello myroute!")&#125;func main() &#123; mux := &amp;MyMux&#123;&#125; http.ListenAndServe(":9090", mux)&#125; Go代码的执行流程通过对http包的分析之后，梳理下整个的代码执行过程： 首先调用Http.HandleFunc： 1 调用了DefaultServeMux的HandleFunc2 调用了DefaultServeMux的Handle3 往DefaultServeMux的map[string]muxEntry中增加对应的handler和路由规则 其次调用http.ListenAndServe(“:9090”, nil)： 1 实例化Server2 调用Server的ListenAndServe()3 调用net.Listen(“tcp”, addr)监听端口4 启动一个for循环，在循环体中Accept请求5 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve()6 读取每个请求的内容w, err := c.readRequest()7 判断handler是否为空，如果没有设置handler（这个例子就没有设置handler），handler就设置为DefaultServeMux8 调用handler的ServeHttp9 在这个例子中，下面就进入到DefaultServeMux.ServeHttp10 根据request选择handler，并且进入到这个handler的ServeHTTP mux.handler(r).ServeHTTP(w, r)11 选择handler： A 判断是否有路由能满足这个request（循环遍历ServeMux的muxEntry） B 如果有路由满足，调用这个路由handler的ServeHTTP C 如果没有路由满足，调用NotFoundHandler的ServeHTTP 表单对于表单，写过前后端的朋友应该比较熟悉了，方便前后端数据交互。 表单是一个包含表单元素的区域。表单元素是允许用户在表单中（eg：文本域、下拉列表、单选框、复选框etc.）输入信息的元素。表单使用表单标签（\）定义。 12345&lt;form&gt;...input 元素...&lt;/form&gt; 处理表单的输入先来看一个表单递交的例子，我们有如下的表单内容，命名成文件login.gtpl(放入当前新建项目的目录里面) 123456789101112&lt;html&gt;&lt;head&gt;&lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="/login" method="post"&gt; 用户名:&lt;input type="text" name="username"&gt; 密码:&lt;input type="password" name="password"&gt; &lt;input type="submit" value="登录"&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 上面递交表单到服务器的/login，当用户输入信息点击登录之后，会跳转到服务器的路由login里面，我们首先要判断这个是什么方式传递过来，POST还是GET呢？ http包里面有一个很简单的方式就可以获取，在前面web例子的基础上来看看怎么处理login页面的form数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport ( "fmt" "html/template" "log" "net/http" "strings")func sayhelloName(w http.ResponseWriter, r *http.Request) &#123; r.ParseForm() //解析url传递的参数，对于POST则解析响应包的主体（request body） //注意:如果没有调用ParseForm方法，下面无法获取表单的数据 fmt.Println(r.Form) //这些信息是输出到服务器端的打印信息 fmt.Println("path", r.URL.Path) fmt.Println("scheme", r.URL.Scheme) fmt.Println(r.Form["url_long"]) for k, v := range r.Form &#123; fmt.Println("key:", k) fmt.Println("val:", strings.Join(v, "")) &#125; fmt.Fprintf(w, "Hello astaxie!") //这个写入到w的是输出到客户端的&#125;func login(w http.ResponseWriter, r *http.Request) &#123; fmt.Println("method:", r.Method) //获取请求的方法 if r.Method == "GET" &#123; t, _ := template.ParseFiles("login.gtpl") log.Println(t.Execute(w, nil)) &#125; else &#123; //请求的是登录数据，那么执行登录的逻辑判断 fmt.Println("username:", r.Form["username"]) fmt.Println("password:", r.Form["password"]) &#125;&#125;func main() &#123; http.HandleFunc("/", sayhelloName) //设置访问的路由 http.HandleFunc("/login", login) //设置访问的路由 err := http.ListenAndServe(":9090", nil) //设置监听的端口 if err != nil &#123; log.Fatal("ListenAndServe: ", err) &#125;&#125; 获取请求方法是通过r.Method来完成的，这是个字符串类型的变量，返回GET, POST, PUT等method信息。 login函数中我们根据r.Method来判断是显示登录界面还是处理登录逻辑。当GET方式请求时显示登录界面，其他方式请求时则处理登录逻辑，如查询数据库、验证登录信息等。 当我们在浏览器里面打开http://127.0.0.1:9090/login的时候，出现如下界面 如果你看到一个空页面，可能是你写的 login.gtpl 文件中有错误，请根据控制台中的日志进行修复。 我们输入用户名和密码之后发现在服务器端是不会打印出来任何输出的，为什么呢？默认情况下，Handler里面是不会自动解析form的，必须显式的调用r.ParseForm()后，你才能对这个表单数据进行操作。我们修改一下代码，在fmt.Println(&quot;username:&quot;, r.Form[&quot;username&quot;])之前加一行r.ParseForm(),重新编译，再次测试输入递交，现在是不是在服务器端有输出你的输入的用户名和密码了。 r.Form里面包含了所有请求的参数，比如URL中query-string、POST的数据、PUT的数据，所以当你在URL中的query-string字段和POST冲突时，会保存成一个slice，里面存储了多个值，Go官方文档中说在接下来的版本里面将会把POST、GET这些数据分离开来。 现在我们修改一下login.gtpl里面form的action值http://127.0.0.1:9090/login修改为http://127.0.0.1:9090/login?username=astaxie，再次测试，服务器的输出username是不是一个slice。服务器端的输出如下： request.Form是一个url.Values类型，里面存储的是对应的类似key=value的信息，下面展示了可以对form数据进行的一些操作: 123456789v := url.Values&#123;&#125;v.Set("name", "Ava")v.Add("friend", "Jess")v.Add("friend", "Sarah")v.Add("friend", "Zoe")// v.Encode() == "name=Ava&amp;friend=Jess&amp;friend=Sarah&amp;friend=Zoe"fmt.Println(v.Get("name"))fmt.Println(v.Get("friend"))fmt.Println(v["friend"]) Tips: Request本身也提供了FormValue()函数来获取用户提交的参数。如r.Form[“username”]也可写成r.FormValue(“username”)。调用r.FormValue时会自动调用r.ParseForm，所以不必提前调用。r.FormValue只会返回同名参数中的第一个，若参数不存在则返回空字符串。 验证表单的输入开发Web的一个原则就是，不能信任用户输入的任何信息，所以验证和过滤用户的输入信息就变得非常重要。~（就如某软件安全设计老师所说）~ 我们平常编写Web应用主要有两方面的数据验证，一个是在页面端的js验证(目前在这方面有很多的插件库，比如ValidationJS插件)，一个是在服务器端的验证，下面是如何在服务器端验证。 必填字段通过len来获取数据的长度限定非空： 123if len(r.Form["username"][0])==0&#123; //为空的处理&#125; r.Form对不同类型的表单元素的留空有不同的处理， 对于空文本框、空文本区域以及文件上传，元素的值为空值，而如果是未选中的复选框和单选按钮，则根本不会在r.Form中产生相应条目。所以建议通过r.Form.Get()来获取值，如果字段不存在，通过该方式获取的是空值。但r.Form.Get()只能获取单个的值，如果是map的值，必须通过上面的方式来获取。 数字如果我们是判断正整数，那么我们先转化成int类型，然后进行处理 123456789getint,err:=strconv.Atoi(r.Form.Get("age"))if err!=nil&#123; //数字转化出错了，那么可能就不是数字&#125;//接下来就可以判断这个数字的大小范围了if getint &gt;100 &#123; //太大了&#125; 还有一种方式就是正则匹配的方式 123if m, _ := regexp.MatchString("^[0-9]+$", r.Form.Get("age")); !m &#123; return false&#125; Go实现的正则是RE2，所有的字符都是UTF-8编码的。 中文对于中文我们目前有两种方式来验证，可以使用 unicode 包提供的 func Is(rangeTab *RangeTable, r rune) bool 来验证，也可以使用正则方式来验证，这里使用最简单的正则方式，如下代码所示 123if m, _ := regexp.MatchString("^\\p&#123;Han&#125;+$", r.Form.Get("realname")); !m &#123; return false&#125; 英文通过正则验证数据： 123if m, _ := regexp.MatchString("^[a-zA-Z]+$", r.Form.Get("engname")); !m &#123; return false&#125; 电子邮件地址12345if m, _ := regexp.MatchString(`^([\w\.\_]&#123;2,10&#125;)@(\w&#123;1,&#125;).([a-z]&#123;2,4&#125;)$`, r.Form.Get("email")); !m &#123; fmt.Println("no")&#125;else&#123; fmt.Println("yes")&#125; 手机号码123if m, _ := regexp.MatchString(`^(1[3|4|5|8][0-9]\d&#123;4,8&#125;)$`, r.Form.Get("mobile")); !m &#123; return false&#125; 下拉菜单如果我们想要判断表单里面&lt;select&gt;元素生成的下拉菜单中是否有被选中的项目。有些时候黑客可能会伪造这个下拉菜单不存在的值发送给你，那么如何判断这个值是否是我们预设的值呢？ 我们的select可能是这样的一些元素 12345&lt;select name="fruit"&gt;&lt;option value="apple"&gt;apple&lt;/option&gt;&lt;option value="pear"&gt;pear&lt;/option&gt;&lt;option value="banane"&gt;banane&lt;/option&gt;&lt;/select&gt; 那么我们可以这样来验证 12345678910slice:=[]string&#123;"apple","pear","banane"&#125;v := r.Form.Get("fruit")for _, item := range slice &#123; if item == v &#123; return true &#125;&#125;return false 单选按钮如果我们想要判断radio按钮是否有一个被选中了，我们页面的输出可能就是一个男、女性别的选择，但是也可能一个15岁大的无聊小孩，一手拿着http协议的书，另一只手通过telnet客户端向你的程序在发送请求呢，你设定的性别男值是1，女是2，他给你发送一个3，你的程序会出现异常吗？因此我们也需要像下拉菜单的判断方式类似，判断我们获取的值是我们预设的值，而不是额外的值。 12&lt;input type="radio" name="gender" value="1"&gt;男&lt;input type="radio" name="gender" value="2"&gt;女 那我们也可以类似下拉菜单的做法一样 12345678slice:=[]int&#123;1,2&#125;for _, v := range slice &#123; if v == r.Form.Get("gender") &#123; return true &#125;&#125;return false 复选框有一项选择兴趣的复选框，你想确定用户选中的和你提供给用户选择的是同一个类型的数据。 123&lt;input type="checkbox" name="interest" value="football"&gt;足球&lt;input type="checkbox" name="interest" value="basketball"&gt;篮球&lt;input type="checkbox" name="interest" value="tennis"&gt;网球 对于复选框我们的验证和单选有点不一样，因为接收到的数据是一个slice 1234567slice:=[]string&#123;"football","basketball","tennis"&#125;a:=Slice_diff(r.Form["interest"],slice)if a == nil&#123; return true&#125;return false 上面这个函数Slice_diff包含在大佬astaxie开源的一个库里面(操作slice和map的库) 日期和时间Go里面提供了一个time的处理包，我们可以把用户的输入年月日转化成相应的时间，然后进行逻辑判断 12t := time.Date(2009, time.November, 10, 23, 0, 0, 0, time.UTC)fmt.Printf("Go launched at %s\n", t.Local()) 获取time之后我们就可以进行很多时间函数的操作。具体的判断就根据自己的需求调整。 身份证号码123456789//验证15位身份证，15位的是全部数字if m, _ := regexp.MatchString(`^(\d&#123;15&#125;)$`, r.Form.Get("usercard")); !m &#123; return false&#125;//验证18位身份证，18位前17位为数字，最后一位是校验位，可能为数字或字符X。if m, _ := regexp.MatchString(`^(\d&#123;17&#125;)([0-9]|X)$`, r.Form.Get("usercard")); !m &#123; return false&#125; 预防XSS攻击动态站点会受到一种名为“跨站脚本攻击”（Cross Site Scripting, 安全专家们通常将其缩写成 XSS）的威胁，而静态站点则完全不受其影响。~（还记得期末把XSS写成CSS的绝望，缩写成XSS是为了和前端CSS区分）~ 攻击者通常会在有漏洞的程序中插入JavaScript、VBScript、 ActiveX或Flash以欺骗用户。一旦得手，他们可以盗取用户帐户信息，修改用户设置，盗取/污染cookie和植入恶意广告等。 对XSS最佳的防护应该结合以下两种方法：一是验证所有输入数据，有效检测攻击(这个我们前面小节已经有过介绍)；另一个是对所有输出数据进行适当的处理，以防止任何已成功注入的脚本在浏览器端运行。 Go的html/template里面带有下面几个函数可以帮忙转义： func HTMLEscape(w io.Writer, b []byte) //把b进行转义之后写到w func HTMLEscapeString(s string) string //转义s之后返回结果字符串 func HTMLEscaper(args …interface{}) string //支持多个参数一起转义，返回结果字符串 123fmt.Println("username:", template.HTMLEscapeString(r.Form.Get("username"))) //输出到服务器端fmt.Println("password:", template.HTMLEscapeString(r.Form.Get("password")))template.HTMLEscape(w, []byte(r.Form.Get("username"))) //输出到客户端 如果输入的username是&lt;script&gt;alert()&lt;/script&gt;，浏览器上输出如下所示： 防止多次提交表单不知道你是否曾经看到过一个论坛或者博客，在一个帖子或者文章后面出现多条重复的记录，这些大多数是因为用户重复递交了留言的表单引起的。由于种种原因，用户经常会重复递交表单。通常这只是鼠标的误操作，如双击了递交按钮，也可能是为了编辑或者再次核对填写过的信息，点击了浏览器的后退按钮，然后又再次点击了递交按钮而不是浏览器的前进按钮。当然，也可能是故意的——比如，在某项在线调查或者博彩活动中重复投票。那我们如何有效的防止用户多次递交相同的表单呢？ 解决方案是在表单中添加一个带有唯一值的隐藏字段。在验证表单时，先检查带有该唯一值的表单是否已经递交过了。如果是，拒绝再次递交；如果不是，则处理表单进行逻辑处理。另外，如果是采用了Ajax模式递交表单的话，当表单递交后，通过javascript来禁用表单的递交按钮。 1234567&lt;input type="checkbox" name="interest" value="football"&gt;足球&lt;input type="checkbox" name="interest" value="basketball"&gt;篮球&lt;input type="checkbox" name="interest" value="tennis"&gt;网球 用户名:&lt;input type="text" name="username"&gt;密码:&lt;input type="password" name="password"&gt;&lt;input type="hidden" name="token" value="&#123;&#123;.&#125;&#125;"&gt;&lt;input type="submit" value="登陆"&gt; 我们在模版里面增加了一个隐藏字段token，这个值我们通过MD5(时间戳)来获取唯一值，然后我们把这个值存储到服务器端(session控制，之后再说)，以方便表单提交时比对判定。 12345678910111213141516171819202122232425func login(w http.ResponseWriter, r *http.Request) &#123; fmt.Println("method:", r.Method) //获取请求的方法 if r.Method == "GET" &#123; crutime := time.Now().Unix() h := md5.New() io.WriteString(h, strconv.FormatInt(crutime, 10)) token := fmt.Sprintf("%x", h.Sum(nil)) t, _ := template.ParseFiles("login.gtpl") t.Execute(w, token) &#125; else &#123; //请求的是登陆数据，那么执行登陆的逻辑判断 r.ParseForm() token := r.Form.Get("token") if token != "" &#123; //验证token的合法性 &#125; else &#123; //不存在token报错 &#125; fmt.Println("username length:", len(r.Form["username"][0])) fmt.Println("username:", template.HTMLEscapeString(r.Form.Get("username"))) //输出到服务器端 fmt.Println("password:", template.HTMLEscapeString(r.Form.Get("password"))) template.HTMLEscape(w, []byte(r.Form.Get("username"))) //输出到客户端 &#125;&#125; 上面的代码输出到页面的源码如下： 我们看到token已经有输出值，你可以不断的刷新，可以看到这个值在不断的变化。这样就保证了每次显示form表单的时候都是唯一的，用户递交的表单保持了唯一性。 我们的解决方案可以防止非恶意的攻击，并能使恶意用户暂时不知所措，然后，它却不能排除所有的欺骗性的动机，对此类情况还需要更复杂的工作。 处理文件上传要使表单能够上传文件，首先第一步就是要添加form的enctype属性，enctype属性有如下三种情况: 123application/x-www-form-urlencoded 表示在发送前编码所有字符（默认）multipart/form-data 不对字符编码。在使用包含文件上传控件的表单时，必须使用该值。text/plain 空格转换为 &quot;+&quot; 加号，但不对特殊字符编码。 所以，创建新的表单html文件, 命名为upload.gtpl, html代码应该类似于: 123456789101112&lt;html&gt;&lt;head&gt; &lt;title&gt;上传文件&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form enctype="multipart/form-data" action="/upload" method="post"&gt; &lt;input type="file" name="uploadfile" /&gt; &lt;input type="hidden" name="token" value="&#123;&#123;.&#125;&#125;"/&gt; &lt;input type="submit" value="upload" /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 在服务器端，我们增加一个handlerFunc: 12345678910111213141516171819202122232425262728293031http.HandleFunc("/upload", upload)// 处理/upload 逻辑func upload(w http.ResponseWriter, r *http.Request) &#123; fmt.Println("method:", r.Method) //获取请求的方法 if r.Method == "GET" &#123; crutime := time.Now().Unix() h := md5.New() io.WriteString(h, strconv.FormatInt(crutime, 10)) token := fmt.Sprintf("%x", h.Sum(nil)) t, _ := template.ParseFiles("upload.gtpl") t.Execute(w, token) &#125; else &#123; r.ParseMultipartForm(32 &lt;&lt; 20) file, handler, err := r.FormFile("uploadfile") if err != nil &#123; fmt.Println(err) return &#125; defer file.Close() fmt.Fprintf(w, "%v", handler.Header) f, err := os.OpenFile("./test/"+handler.Filename, os.O_WRONLY|os.O_CREATE, 0666) // 此处假设当前目录下已存在test目录 if err != nil &#123; fmt.Println(err) return &#125; defer f.Close() io.Copy(f, file) &#125;&#125; 通过上面的代码可以看到，处理文件上传我们需要调用r.ParseMultipartForm，里面的参数表示maxMemory，调用ParseMultipartForm之后，上传的文件存储在maxMemory大小的内存里面，如果文件大小超过了maxMemory，那么剩下的部分将存储在系统的临时文件中。我们可以通过r.FormFile获取上面的文件句柄，然后实例中使用了io.Copy来存储文件。 获取其他非文件字段信息的时候就不需要调用r.ParseForm，因为在需要的时候Go自动会去调用。而且ParseMultipartForm调用一次之后，后面再次调用不会再有效果。 通过上面的实例我们可以看到我们上传文件主要三步处理： 表单中增加enctype=”multipart/form-data” 服务端调用r.ParseMultipartForm,把上传的文件存储在内存和临时文件中 使用r.FormFile获取文件句柄，然后对文件进行存储等处理。 文件handler是multipart.FileHeader,里面存储了如下结构信息 12345type FileHeader struct &#123; Filename string Header textproto.MIMEHeader // contains filtered or unexported fields&#125; 我们通过上面的实例代码打印出来上传文件的信息如下 客户端上传文件除了通过表单上传文件，然后在服务器端处理文件外，Go还支持模拟客户端表单功能支持文件上传： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package mainimport ( "bytes" "fmt" "io" "io/ioutil" "mime/multipart" "net/http" "os")func postFile(filename string, targetUrl string) error &#123; bodyBuf := &amp;bytes.Buffer&#123;&#125; bodyWriter := multipart.NewWriter(bodyBuf) //关键的一步操作 fileWriter, err := bodyWriter.CreateFormFile("uploadfile", filename) if err != nil &#123; fmt.Println("error writing to buffer") return err &#125; //打开文件句柄操作 fh, err := os.Open(filename) if err != nil &#123; fmt.Println("error opening file") return err &#125; defer fh.Close() //iocopy _, err = io.Copy(fileWriter, fh) if err != nil &#123; return err &#125; contentType := bodyWriter.FormDataContentType() bodyWriter.Close() resp, err := http.Post(targetUrl, contentType, bodyBuf) if err != nil &#123; return err &#125; defer resp.Body.Close() resp_body, err := ioutil.ReadAll(resp.Body) if err != nil &#123; return err &#125; fmt.Println(resp.Status) fmt.Println(string(resp_body)) return nil&#125;// sample usagefunc main() &#123; target_url := "http://localhost:9090/upload" filename := "./astaxie.pdf" postFile(filename, target_url)&#125; 客户端通过multipart.Write把文件的文本流写入一个缓存中，然后调用http的Post方法把缓存传到服务器。 如果还有其他普通字段例如username之类的需要同时写入，那么可以调用multipart的WriteField方法写很多其他类似的字段。 访问数据库database/sql接口Go与PHP不同的地方是Go官方没有提供数据库驱动，而是为开发数据库驱动定义了一些标准接口，开发者可以根据定义的接口来开发相应的数据库驱动，这样做有一个好处，只要是按照标准接口开发的代码， 以后需要迁移数据库时，不需要任何修改。 以下为Go定义的标准接口： sql.Register这个存在于database/sql的函数是用来注册数据库驱动的，当第三方开发者开发数据库驱动时，都会实现init函数，在init里面会调用这个Register(name string, driver driver.Driver)完成本驱动的注册。 我们来看一下mymysql、sqlite3的驱动里面都是怎么调用的： 123456789101112//https://github.com/mattn/go-sqlite3驱动func init() &#123; sql.Register("sqlite3", &amp;SQLiteDriver&#123;&#125;)&#125;//https://github.com/mikespook/mymysql驱动// Driver automatically registered in database/sqlvar d = Driver&#123;proto: "tcp", raddr: "127.0.0.1:3306"&#125;func init() &#123; Register("SET NAMES utf8") sql.Register("mymysql", &amp;d)&#125; 我们看到第三方数据库驱动都是通过调用这个函数来注册自己的数据库驱动名称以及相应的driver实现。在database/sql内部通过一个map来存储用户定义的相应驱动。 123var drivers = make(map[string]driver.Driver)drivers[name] = driver 因此通过database/sql的注册函数可以同时注册多个数据库驱动，只要不重复。 在我们使用database/sql接口和第三方库的时候经常看到如下: 12345&gt; import (&gt; &quot;database/sql&quot;&gt; _ &quot;github.com/mattn/go-sqlite3&quot;&gt; )&gt; 新手都会被这个_所迷惑，其实这个就是Go设计的巧妙之处，我们在变量赋值的时候经常看到这个符号，它是用来忽略变量赋值的占位符，那么包引入用到这个符号也是相似的作用，这儿使用_的意思是引入后面的包名而不直接使用这个包中定义的函数，变量等资源。 我们在流程和函数一节中介绍过init函数的初始化过程，包在引入的时候会自动调用包的init函数以完成对包的初始化。因此，我们引入上面的数据库驱动包之后会自动去调用init函数，然后在init函数里面注册这个数据库驱动，这样我们就可以在接下来的代码中直接使用这个数据库驱动了。 driver.DriverDriver是一个数据库驱动的接口，他定义了一个method： Open(name string)，这个方法返回一个数据库的Conn接口。 123type Driver interface &#123; Open(name string) (Conn, error)&#125; 返回的Conn只能用来进行一次goroutine的操作，即不能把这个Conn应用于多个goroutine里面。 如下代码会出现错误 1234...go goroutineA (Conn) //执行查询操作go goroutineB (Conn) //执行插入操作... 上面这样的代码可能会使Go不知道某个操作究竟是由哪个goroutine发起的,从而导致数据混乱，比如可能会把goroutineA里面执行的查询操作的结果返回给goroutineB从而使B错误地把此结果当成自己执行的插入数据。 第三方驱动都会定义这个函数，它会解析name参数来获取相关数据库的连接信息，解析完成后，它将使用此信息来初始化一个Conn并返回它。 driver.ConnConn是一个数据库连接的接口定义，这个Conn只能应用在一个goroutine里面。 12345type Conn interface &#123; Prepare(query string) (Stmt, error) Close() error Begin() (Tx, error)&#125; Prepare函数返回与当前连接相关的执行Sql语句的准备状态，可以进行查询、删除等操作。 Close函数关闭当前的连接，执行释放连接拥有的资源等清理工作。因为驱动实现了database/sql里面建议的conn pool，所以不用再去实现缓存conn之类的，这样会容易引起问题。 Begin函数返回一个代表事务处理的Tx，通过它你可以进行查询,更新等操作，或者对事务进行回滚、递交。 driver.StmtStmt是一种准备好的状态，和Conn相关联，只能应用于一个goroutine中，不能应用于多个goroutine。 123456type Stmt interface &#123; Close() error NumInput() int Exec(args []Value) (Result, error) Query(args []Value) (Rows, error)&#125; Close函数关闭当前的链接状态，但是如果当前正在执行query，query还是有效返回rows数据。 NumInput函数返回当前预留参数的个数，当返回&gt;=0时数据库驱动就会智能检查调用者的参数。当数据库驱动包不知道预留参数的时候，返回-1。 Exec函数执行Prepare准备好的sql，传入参数执行update/insert等操作，返回Result数据 Query函数执行Prepare准备好的sql，传入需要的参数执行select操作，返回Rows结果集 driver.Tx事务处理一般就两个过程，递交或者回滚。数据库驱动里面也只需要实现这两个函数就可以 1234type Tx interface &#123; Commit() error Rollback() error&#125; 这两个函数一个用来递交一个事务，一个用来回滚事务。 driver.Execer这是一个Conn可选择实现的接口 123type Execer interface &#123; Exec(query string, args []Value) (Result, error)&#125; 如果这个接口没有定义，那么在调用DB.Exec,就会首先调用Prepare返回Stmt，然后执行Stmt的Exec，然后关闭Stmt。 driver.Result这个是执行Update/Insert等操作返回的结果接口定义 1234type Result interface &#123; LastInsertId() (int64, error) RowsAffected() (int64, error)&#125; LastInsertId函数返回由数据库执行插入操作得到的自增ID号。 RowsAffected函数返回query操作影响的数据条目数。 driver.RowsRows是执行查询返回的结果集接口定义 12345type Rows interface &#123; Columns() []string Close() error Next(dest []Value) error&#125; Columns函数返回查询数据库表的字段信息，这个返回的slice和sql查询的字段一一对应，而不是返回整个表的所有字段。 Close函数用来关闭Rows迭代器。 Next函数用来返回下一条数据，把数据赋值给dest。dest里面的元素必须是driver.Value的值除了string，返回的数据里面所有的string都必须要转换成[]byte。如果最后没数据了，Next函数最后返回io.EOF。 driver.RowsAffectedRowsAffected其实就是一个int64的别名，但是他实现了Result接口，用来底层实现Result的表示方式 12345type RowsAffected int64func (RowsAffected) LastInsertId() (int64, error)func (v RowsAffected) RowsAffected() (int64, error) driver.ValueValue其实就是一个空接口，他可以容纳任何的数据 1type Value interface&#123;&#125; drive的Value是驱动必须能够操作的Value，Value要么是nil，要么是下面的任意一种 123456int64float64bool[]bytestring [*]除了Rows.Next返回的不能是string.time.Time driver.ValueConverterValueConverter接口定义了如何把一个普通的值转化成driver.Value的接口 123type ValueConverter interface &#123; ConvertValue(v interface&#123;&#125;) (Value, error)&#125; 在开发的数据库驱动包里面实现这个接口的函数在很多地方会使用到，这个ValueConverter有很多好处： 转化driver.value到数据库表相应的字段，例如int64的数据如何转化成数据库表uint16字段 把数据库查询结果转化成driver.Value值 在scan函数里面如何把driver.Value值转化成用户定义的值 driver.ValuerValuer接口定义了返回一个driver.Value的方式 123type Valuer interface &#123; Value() (Value, error)&#125; 很多类型都实现了这个Value方法，用来自身与driver.Value的转化。 通过上面的讲解，你应该对于驱动的开发有了一个基本的了解，一个驱动只要实现了这些接口就能完成增删查改等基本操作了，剩下的就是与相应的数据库进行数据交互等细节问题了，在此不再赘述。 database/sqldatabase/sql在database/sql/driver提供的接口基础上定义了一些更高阶的方法，用以简化数据库操作,同时内部还建议性地实现一个conn pool。 1234567type DB struct &#123; driver driver.Driver dsn string mu sync.Mutex // protects freeConn and closed freeConn []driver.Conn closed bool&#125; 我们可以看到Open函数返回的是DB对象，里面有一个freeConn，它就是那个简易的连接池。它的实现相当简单或者说简陋，就是当执行Db.prepare的时候会defer db.putConn(ci, err),也就是把这个连接放入连接池，每次调用conn的时候会先判断freeConn的长度是否大于0，大于0说明有可以复用的conn，直接拿出来用就是了，如果不大于0，则创建一个conn,然后再返回之。 PostgreSQLPostgreSQL 是一个自由的对象-关系数据库服务器(数据库管理系统)，也是笔者在数据库课上学所学的数据库，所以就直接用它了。TIPS:打开pgAdmin如果出现无法连接并要求输入postgres密码的情况，请打开任务管理器&gt;服务，找到并启动postgresql-x64-10即可 驱动Go实现的支持PostgreSQL的驱动也很多，因为国外很多人在开发中使用了这个数据库。以下采用github.com/lib/pq驱动，它目前使用的人最多，在github上也比较活跃。 实例代码数据库建表语句： 1234567891011121314151617CREATE TABLE userinfo( uid serial NOT NULL, username character varying(100) NOT NULL, departname character varying(500) NOT NULL, Created date, CONSTRAINT userinfo_pkey PRIMARY KEY (uid))WITH (OIDS=FALSE);CREATE TABLE userdeatail( uid integer, intro character varying(100), profile character varying(100))WITH(OIDS=FALSE); 看下面这个Go如何操作数据库表数据:增删改查 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package mainimport ( "database/sql" "fmt" _ "github.com/lib/pq")func main() &#123; db, err := sql.Open("postgres", "user=astaxie password=astaxie dbname=test sslmode=disable") checkErr(err) //插入数据 stmt, err := db.Prepare("INSERT INTO userinfo(username,departname,created) VALUES($1,$2,$3) RETURNING uid") checkErr(err) res, err := stmt.Exec("astaxie", "研发部门", "2012-12-09") checkErr(err) //pg不支持这个函数，因为他没有类似MySQL的自增ID // id, err := res.LastInsertId() // checkErr(err) // fmt.Println(id) var lastInsertId int err = db.QueryRow("INSERT INTO userinfo(username,departname,created) VALUES($1,$2,$3) returning uid;", "astaxie", "研发部门", "2012-12-09").Scan(&amp;lastInsertId) checkErr(err) fmt.Println("最后插入id =", lastInsertId) //更新数据 stmt, err = db.Prepare("update userinfo set username=$1 where uid=$2") checkErr(err) res, err = stmt.Exec("astaxieupdate", 1) checkErr(err) affect, err := res.RowsAffected() checkErr(err) fmt.Println(affect) //查询数据 rows, err := db.Query("SELECT * FROM userinfo") checkErr(err) for rows.Next() &#123; var uid int var username string var department string var created string err = rows.Scan(&amp;uid, &amp;username, &amp;department, &amp;created) checkErr(err) fmt.Println(uid) fmt.Println(username) fmt.Println(department) fmt.Println(created) &#125; //删除数据 stmt, err = db.Prepare("delete from userinfo where uid=$1") checkErr(err) res, err = stmt.Exec(1) checkErr(err) affect, err = res.RowsAffected() checkErr(err) fmt.Println(affect) db.Close()&#125;func checkErr(err error) &#123; if err != nil &#123; panic(err) &#125;&#125; PostgreSQL是通过$1,$2这种方式来指定要传递的参数，而不是MySQL中的?，另外在sql.Open中的dsn信息的格式也与MySQL的驱动中的dsn格式不一样，所以在使用时请注意它们的差异。还有pg不支持LastInsertId函数，因为PostgreSQL内部没有实现类似MySQL的自增ID返回，其他的代码几乎是一模一样。 mongoDBMongoDB是一个高性能，开源，无模式的文档型数据库，是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。他支持的数据结构非常松散，采用的是类似json的bjson格式来存储数据，因此可以存储比较复杂的数据类型。Mongo最大的特点是他支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。 目前Go支持mongoDB比较好的驱动有mgo，这个驱动非常的简明好用，但是作者弃坑了orz官方推出了新的驱动mongo-driver，虽然繁琐一点，但支持了事务的处理，并且是官方的。 安装方面个人还是建议从github上clone下来然后本地go install go.mongodb.org/mongo-driver这里吐槽一下，由于网络原因，不能够直接访问 golang.org，但我们还有镜像啊 Golang - Github 安装之后会有个warning，这不要紧，我们自己test一下以下的实例代码（当然请先打开mongo）： 123456789101112131415161718192021222324252627package mainimport ( "context" "fmt" "log" "go.mongodb.org/mongo-driver/mongo" "go.mongodb.org/mongo-driver/mongo/options")func main() &#123; // Set client options clientOptions := options.Client().ApplyURI("mongodb://localhost:27017") // Connect to MongoDB client, err := mongo.Connect(context.TODO(), clientOptions) if err != nil &#123; log.Fatal(err) &#125; // Check the connection err = client.Ping(context.TODO(), nil) if err != nil &#123; log.Fatal(err) &#125; fmt.Println("Connected to MongoDB!")&#125; 连接后，获取test数据库中集合的句柄： 1collection := client.Database("test").Collection("trainers") 一般需要保持连接到MongoDB的客户端，以便应用程序可以使用连接池，而不是为每个查询打开和关闭连接。 当然如果应用程序不再需要连接，也可以关闭连接client.Disconnect()： 123456err = client.Disconnect(context.TODO())if err != nil &#123; log.Fatal(err)&#125;fmt.Println("Connection to MongoDB closed.") 在Go中使用BSON对象MongoDB中的JSON文档以称为BSON（二进制编码的JSON）的二进制表示形式存储。与其他将JSON数据存储为简单字符串和数字的数据库不同，BSON编码扩展了JSON表示形式，以包括其他类型，例如int，long，date，float point和decimal128。这使应用程序更容易可靠地处理，排序和比较数据。go驱动用两种类型来表示BSON数据，D和Raw，接下来我们主要使用D类型。 The D family consists of four types: D: A BSON document, which is used in situations where order matters, such as MongoDB commands. M: An unordered map. It is the same as D, except it does not preserve order. A: A BSON array. E: A single element inside a D. 这是一个使用D类型构建的过滤器文档的示例，该类型可用于查找name字段与Alice或Bob匹配的文档： 1234567bson.D&#123;&#123; "name", bson.D&#123;&#123; "$in", bson.A&#123;"Alice", "Bob"&#125; &#125;&#125;&#125;&#125; CRUD操作连接到数据库后，就该开始增删查改了： 插入文件首先，创建一些新Trainer结构（就是个自定义的结构体）以插入数据库： 123ash := Trainer&#123;"Ash", 10, "Pallet Town"&#125;misty := Trainer&#123;"Misty", 10, "Cerulean City"&#125;brock := Trainer&#123;"Brock", 15, "Pewter City"&#125; 插入单个文档，用collection.InsertOne()方法： 12345insertResult, err := collection.InsertOne(context.TODO(), ash)if err != nil &#123; log.Fatal(err)&#125;fmt.Println("Inserted a single document: ", insertResult.InsertedID) 需要一次插入多个文档，collection.InsertMany()方法： 1234567trainers := []interface&#123;&#125;&#123;misty, brock&#125;insertManyResult, err := collection.InsertMany(context.TODO(), trainers)if err != nil &#123; log.Fatal(err)&#125;fmt.Println("Inserted multiple documents: ", insertManyResult.InsertedIDs) 更新文件collection.UpdateOne()更新单个文档。它需要先匹配再更新，可以用以下bson.D类型构建： 1234567filter := bson.D&#123;&#123;"name", "Ash"&#125;&#125;update := bson.D&#123; &#123;"$inc", bson.D&#123; &#123;"age", 1&#125;, &#125;&#125;,&#125; 然后，此代码将匹配名称为Ash的文档，并将Ash的age加1。 123456updateResult, err := collection.UpdateOne(context.TODO(), filter, update)if err != nil &#123; log.Fatal(err)&#125;fmt.Printf("Matched %v documents and updated %v documents.\n", updateResult.MatchedCount, updateResult.ModifiedCount) 查找文件查找单个文档，用collection.FindOne()。此方法返回单个结果，该结果可以解码为一个值。 用filter在更新查询中使用的相同变量来匹配名称为Ash的文档。 123456789// create a value into which the result can be decodedvar result Trainererr = collection.FindOne(context.TODO(), filter).Decode(&amp;result)if err != nil &#123; log.Fatal(err)&#125;fmt.Printf("Found a single document: %+v\n", result) 要查找多个文档，用collection.Find()，此方法返回一个游标Cursor。 Cursor提供了一系列文档，可以通过它们一次迭代和解码一个文档。一旦Cursor用尽，应该关闭Cursor。options程序包设置一些操作选项，比如设置一个限制，以便仅返回2个文档。 1234567891011121314151617181920212223242526272829303132333435// Pass these options to the Find methodfindOptions := options.Find()findOptions.SetLimit(2)// Here's an array in which you can store the decoded documentsvar results []*Trainer// Passing bson.D&#123;&#123;&#125;&#125; as the filter matches all documents in the collectioncur, err := collection.Find(context.TODO(), bson.D&#123;&#123;&#125;&#125;, findOptions)if err != nil &#123; log.Fatal(err)&#125;// Finding multiple documents returns a cursor// Iterating through the cursor allows us to decode documents one at a timefor cur.Next(context.TODO()) &#123; // create a value into which the single document can be decoded var elem Trainer err := cur.Decode(&amp;elem) if err != nil &#123; log.Fatal(err) &#125; results = append(results, &amp;elem)&#125;if err := cur.Err(); err != nil &#123; log.Fatal(err)&#125;// Close the cursor once finishedcur.Close(context.TODO())fmt.Printf("Found multiple documents (array of pointers): %+v\n", results) 删除文件用collection.DeleteOne()或collection.DeleteMany()删除文档。传参bson.D作为个过滤器，该参数将匹配集合中的所有文档。 12345deleteResult, err := collection.DeleteMany(context.TODO(), bson.D&#123;&#123;&#125;&#125;)if err != nil &#123; log.Fatal(err)&#125;fmt.Printf("Deleted %v documents in the trainers collection\n", deleteResult.DeletedCount) session和数据存储session &amp; cookie当用户来到微博登陆页面，输入用户名和密码之后点击“登录”后浏览器将认证信息POST给远端的服务器，服务器执行验证逻辑，如果验证通过，则浏览器会跳转到登录用户的微博首页，在登录成功后，服务器如何验证我们对其他受限制页面的访问呢？因为HTTP协议是无状态的，所以很显然服务器不可能知道我们已经在上一次的HTTP请求中通过了验证。当然，最简单的解决方案就是所有的请求里面都带上用户名和密码，这样虽然可行，但大大加重了服务器的负担（对于每个request都需要到数据库验证），也大大降低了用户体验(每个页面都需要重新输入用户名密码，每个页面都带有登录表单)。既然直接在请求中带上用户名与密码不可行，那么就只有在服务器或客户端保存一些类似的可以代表身份的信息了，所以就有了cookie与session。 cookie，简而言之就是在本地计算机保存一些用户操作的历史信息（当然包括登录信息），并在用户再次访问该站点时浏览器通过HTTP协议将本地cookie内容发送给服务器，从而完成验证，或继续上一步操作。 session，简而言之就是在服务器上保存用户操作的历史信息。服务器使用session id来标识session，session id由服务器负责产生，保证随机性与唯一性，相当于一个随机密钥，避免在握手或传输中暴露用户真实密码。但该方式下，仍然需要将发送请求的客户端与session进行对应，所以可以借助cookie机制来获取客户端的标识（即session id），也可以通过GET方式将id提交给服务器。 cookieCookie是由浏览器维持的，存储在客户端的一小段文本信息，伴随着用户请求和页面在Web服务器和浏览器之间传递。用户每次访问站点时，Web应用程序都可以读取cookie包含的信息。浏览器设置里面有cookie隐私数据选项，打开它，可以看到很多已访问网站的cookies，如下图所示： cookie是有时间限制的，根据生命期不同分成两种：会话cookie和持久cookie； 如果不设置过期时间，则表示这个cookie生命周期为从创建到浏览器关闭止，只要关闭浏览器窗口，cookie就消失了。这种生命期为浏览会话期的cookie被称为会话cookie。会话cookie一般不保存在硬盘上而是保存在内存里。 如果设置了过期时间(setMaxAge(606024))，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie依然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存的cookie，不同的浏览器有不同的处理方式。 Go设置cookieGo语言中通过net/http包中的SetCookie来设置： 1http.SetCookie(w ResponseWriter, cookie *Cookie) w表示需要写入的response，cookie是一个struct，让我们来看一下cookie对象是怎么样的 1234567891011121314151617type Cookie struct &#123; Name string Value string Path string Domain string Expires time.Time RawExpires string// MaxAge=0 means no 'Max-Age' attribute specified.// MaxAge&lt;0 means delete cookie now, equivalently 'Max-Age: 0'// MaxAge&gt;0 means Max-Age attribute present and given in seconds MaxAge int Secure bool HttpOnly bool Raw string Unparsed []string // Raw text of unparsed attribute-value pairs&#125; 我们来看一个例子，如何设置cookie 1234expiration := time.Now()expiration = expiration.AddDate(1, 0, 0)cookie := http.Cookie&#123;Name: "username", Value: "astaxie", Expires: expiration&#125;http.SetCookie(w, &amp;cookie) Go读取cookie上面的例子演示了如何设置cookie数据，我们这里来演示一下如何读取cookie 12cookie, _ := r.Cookie("username")fmt.Fprint(w, cookie) 还有另外一种读取方式 123for _, cookie := range r.Cookies() &#123; fmt.Fprint(w, cookie.Name)&#125; 可以看到通过request获取cookie非常方便。 sessionsession，会话，其本来含义是指有始有终的一系列动作/消息，比如打电话是从拿起电话拨号到挂断电话这中间的一系列过程可以称之为一个session。然而当session一词与网络协议相关联时，它又往往隐含了“面向连接”和/或“保持状态”这样两个含义。 session在Web开发环境下的语义又有了新的扩展，它的含义是指一类用来在客户端与服务器端之间保持状态的解决方案。有时候Session也用来指这种解决方案的存储结构。 session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构(也可能就是使用散列表)来保存信息。 但程序需要为某个客户端的请求创建一个session的时候，服务器首先检查这个客户端的请求里是否包含了一个session标识－称为session id，如果已经包含一个session id则说明以前已经为此客户创建过session，服务器就按照session id把这个session检索出来使用(如果检索不到，可能会新建一个，这种情况可能出现在服务端已经删除了该用户对应的session对象，但用户人为地在请求的URL后面附加上一个JSESSION的参数)。如果客户请求不包含session id，则为此客户创建一个session并且同时生成一个与此session相关联的session id，这个session id将在本次响应中返回给客户端保存。 session机制本身并不复杂，然而其实现和配置上的灵活性却使得具体情况复杂多变。这也要求我们不能把仅仅某一次的经验或者某一个浏览器，服务器的经验当作普遍适用的。 对比session和cookie目的相同，都是为了克服http协议无状态的缺陷，但完成的方法不同。session通过cookie，在客户端保存session id，而将用户的其他会话消息保存在服务端的session对象中，与此相对的，cookie需要将所有信息都保存在客户端。因此cookie存在着一定的安全隐患，例如本地cookie中保存的用户名密码被破译，或cookie被其他网站收集（例如：1. appA主动设置域B cookie，让域B cookie获取；2. XSS，在appA上通过javascript获取document.cookie，并传递给自己的appB）。 通过上面的一些简单介绍我们了解了cookie和session的一些基础知识，知道他们之间的联系和区别，做web开发之前，有必要将一些必要知识了解清楚，才不会在用到时捉襟见肘，或是在调bug时候如无头苍蝇乱转。接下来的几小节我们将详细介绍session相关的知识。 使用sessionsession创建过程session的基本原理是由服务器为每个会话维护一份信息数据，客户端和服务端依靠一个全局唯一的标识来访问这份数据，以达到交互的目的。当用户访问Web应用时，服务端程序会随需要创建session，这个过程可以概括为三个步骤： 生成全局唯一标识符（sessionid）； 开辟数据存储空间。一般会在内存中创建相应的数据结构，但这种情况下，系统一旦掉电，所有的会话数据就会丢失，如果是电子商务类网站，这将造成严重的后果。所以为了解决这类问题，你可以将会话数据写到文件里或存储在数据库中，当然这样会增加I/O开销，但是它可以实现某种程度的session持久化，也更有利于session的共享； 将session的全局唯一标示符发送给客户端。 以上三个步骤中，最关键的是如何发送这个session的唯一标识这一步上。考虑到HTTP协议的定义，数据无非可以放到请求行、头域或Body里，所以一般来说会有两种常用的方式：cookie和URL重写。 Cookie 服务端通过设置Set-cookie头就可以将session的标识符传送到客户端，而客户端此后的每一次请求都会带上这个标识符，另外一般包含session信息的cookie会将失效时间设置为0(会话cookie)，即浏览器进程有效时间。至于浏览器怎么处理这个0，每个浏览器都有自己的方案，但差别都不会太大(一般体现在新建浏览器窗口的时候)； URL重写 所谓URL重写，就是在返回给用户的页面里的所有的URL后面追加session标识符，这样用户在收到响应之后，无论点击响应页面里的哪个链接或提交表单，都会自动带上session标识符，从而就实现了会话的保持。虽然这种做法比较麻烦，但是，如果客户端禁用了cookie的话，此种方案将会是首选。 Go实现session管理通过上面session创建过程的讲解，读者应该对session有了一个大体的认识，但是具体到动态页面技术里面，又是怎么实现session的呢？下面我们将结合session的生命周期（lifecycle），来实现go语言版本的session管理。 session管理设计我们知道session管理涉及到如下几个因素 全局session管理器 保证sessionid 的全局唯一性 为每个客户关联一个session session 的存储(可以存储到内存、文件、数据库等) session 过期处理 接下来我将讲解一下我关于session管理的整个设计思路以及相应的go代码示例： Session管理器定义一个全局的session管理器 1234567891011121314type Manager struct &#123; cookieName string //private cookiename lock sync.Mutex // protects session provider Provider maxlifetime int64&#125;func NewManager(provideName, cookieName string, maxlifetime int64) (*Manager, error) &#123; provider, ok := provides[provideName] if !ok &#123; return nil, fmt.Errorf("session: unknown provide %q (forgotten import?)", provideName) &#125; return &amp;Manager&#123;provider: provider, cookieName: cookieName, maxlifetime: maxlifetime&#125;, nil&#125; Go实现整个的流程应该也是这样的，在main包中创建一个全局的session管理器 12345var globalSessions *session.Manager//然后在init函数中初始化func init() &#123; globalSessions, _ = NewManager("memory","gosessionid",3600)&#125; 我们知道session是保存在服务器端的数据，它可以以任何的方式存储，比如存储在内存、数据库或者文件中。因此我们抽象出一个Provider接口，用以表征session管理器底层存储结构。 123456type Provider interface &#123; SessionInit(sid string) (Session, error) SessionRead(sid string) (Session, error) SessionDestroy(sid string) error SessionGC(maxLifeTime int64)&#125; SessionInit函数实现Session的初始化，操作成功则返回此新的Session变量 SessionRead函数返回sid所代表的Session变量，如果不存在，那么将以sid为参数调用SessionInit函数创建并返回一个新的Session变量 SessionDestroy函数用来销毁sid对应的Session变量 SessionGC根据maxLifeTime来删除过期的数据 那么Session接口需要实现什么样的功能呢？有过Web开发经验的读者知道，对Session的处理基本就 设置值、读取值、删除值以及获取当前sessionID这四个操作，所以我们的Session接口也就实现这四个操作。 123456type Session interface &#123; Set(key, value interface&#123;&#125;) error //set session value Get(key interface&#123;&#125;) interface&#123;&#125; //get session value Delete(key interface&#123;&#125;) error //delete session value SessionID() string //back current sessionID&#125; 以上设计思路来源于database/sql/driver，先定义好接口，然后具体的存储session的结构实现相应的接口并注册后，相应功能这样就可以使用了，以下是用来随需注册存储session的结构的Register函数的实现。 1234567891011121314var provides = make(map[string]Provider)// Register makes a session provide available by the provided name.// If Register is called twice with the same name or if driver is nil,// it panics.func Register(name string, provider Provider) &#123; if provider == nil &#123; panic("session: Register provide is nil") &#125; if _, dup := provides[name]; dup &#123; panic("session: Register called twice for provide " + name) &#125; provides[name] = provider&#125; 全局唯一的Session IDSession ID是用来识别访问Web应用的每一个用户，因此必须保证它是全局唯一的（GUID），下面代码展示了如何满足这一需求： 1234567func (manager *Manager) sessionId() string &#123; b := make([]byte, 32) if _, err := io.ReadFull(rand.Reader, b); err != nil &#123; return "" &#125; return base64.URLEncoding.EncodeToString(b)&#125; session创建我们需要为每个来访用户分配或获取与他相关连的Session，以便后面根据Session信息来验证操作。SessionStart这个函数就是用来检测是否已经有某个Session与当前来访用户发生了关联，如果没有则创建之。 123456789101112131415func (manager *Manager) SessionStart(w http.ResponseWriter, r *http.Request) (session Session) &#123; manager.lock.Lock() defer manager.lock.Unlock() cookie, err := r.Cookie(manager.cookieName) if err != nil || cookie.Value == "" &#123; sid := manager.sessionId() session, _ = manager.provider.SessionInit(sid) cookie := http.Cookie&#123;Name: manager.cookieName, Value: url.QueryEscape(sid), Path: "/", HttpOnly: true, MaxAge: int(manager.maxlifetime)&#125; http.SetCookie(w, &amp;cookie) &#125; else &#123; sid, _ := url.QueryUnescape(cookie.Value) session, _ = manager.provider.SessionRead(sid) &#125; return&#125; 我们用前面login操作来演示session的运用： 123456789101112func login(w http.ResponseWriter, r *http.Request) &#123; sess := globalSessions.SessionStart(w, r) r.ParseForm() if r.Method == "GET" &#123; t, _ := template.ParseFiles("login.gtpl") w.Header().Set("Content-Type", "text/html") t.Execute(w, sess.Get("username")) &#125; else &#123; sess.Set("username", r.Form["username"]) http.Redirect(w, r, "/", 302) &#125;&#125; 操作值：设置、读取和删除SessionStart函数返回的是一个满足Session接口的变量，那么我们该如何用他来对session数据进行操作呢？ 上面的例子中的代码session.Get(&quot;uid&quot;)已经展示了基本的读取数据的操作，现在我们再来看一下详细的操作: 12345678910111213141516171819func count(w http.ResponseWriter, r *http.Request) &#123; sess := globalSessions.SessionStart(w, r) createtime := sess.Get("createtime") if createtime == nil &#123; sess.Set("createtime", time.Now().Unix()) &#125; else if (createtime.(int64) + 360) &lt; (time.Now().Unix()) &#123; globalSessions.SessionDestroy(w, r) sess = globalSessions.SessionStart(w, r) &#125; ct := sess.Get("countnum") if ct == nil &#123; sess.Set("countnum", 1) &#125; else &#123; sess.Set("countnum", (ct.(int) + 1)) &#125; t, _ := template.ParseFiles("count.gtpl") w.Header().Set("Content-Type", "text/html") t.Execute(w, sess.Get("countnum"))&#125; 通过上面的例子可以看到，Session的操作和操作key/value数据库类似:Set、Get、Delete等操作 因为Session有过期的概念，所以我们定义了GC操作，当访问过期时间满足GC的触发条件后将会引起GC，但是当我们进行了任意一个session操作，都会对Session实体进行更新，都会触发对最后访问时间的修改，这样当GC的时候就不会误删除还在使用的Session实体。 session重置我们知道，Web应用中有用户退出这个操作，那么当用户退出应用的时候，我们需要对该用户的session数据进行销毁操作，上面的代码已经演示了如何使用session重置操作，下面这个函数就是实现了这个功能： 1234567891011121314//Destroy sessionidfunc (manager *Manager) SessionDestroy(w http.ResponseWriter, r *http.Request)&#123; cookie, err := r.Cookie(manager.cookieName) if err != nil || cookie.Value == "" &#123; return &#125; else &#123; manager.lock.Lock() defer manager.lock.Unlock() manager.provider.SessionDestroy(cookie.Value) expiration := time.Now() cookie := http.Cookie&#123;Name: manager.cookieName, Path: "/", HttpOnly: true, Expires: expiration, MaxAge: -1&#125; http.SetCookie(w, &amp;cookie) &#125;&#125; session销毁我们来看一下Session管理器如何来管理销毁,只要我们在Main启动的时候启动： 12345678910func init() &#123; go globalSessions.GC()&#125;func (manager *Manager) GC() &#123; manager.lock.Lock() defer manager.lock.Unlock() manager.provider.SessionGC(manager.maxlifetime) time.AfterFunc(time.Duration(manager.maxlifetime), func() &#123; manager.GC() &#125;)&#125; 我们可以看到GC充分利用了time包中的定时器功能，当超时maxLifeTime之后调用GC函数，这样就可以保证maxLifeTime时间内的session都是可用的，类似的方案也可以用于统计在线用户数之类的。 存储session上一节我们介绍了Session管理器的实现原理，定义了存储session的接口，这小节我们将示例一个基于内存的session存储接口的实现，其他的存储方式，读者可以自行参考示例来实现，内存的实现请看下面的例子代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package memoryimport ( "container/list" "github.com/astaxie/session" "sync" "time")var pder = &amp;Provider&#123;list: list.New()&#125;type SessionStore struct &#123; sid string //session id唯一标示 timeAccessed time.Time //最后访问时间 value map[interface&#123;&#125;]interface&#123;&#125; //session里面存储的值&#125;func (st *SessionStore) Set(key, value interface&#123;&#125;) error &#123; st.value[key] = value pder.SessionUpdate(st.sid) return nil&#125;func (st *SessionStore) Get(key interface&#123;&#125;) interface&#123;&#125; &#123; pder.SessionUpdate(st.sid) if v, ok := st.value[key]; ok &#123; return v &#125; else &#123; return nil &#125; return nil&#125;func (st *SessionStore) Delete(key interface&#123;&#125;) error &#123; delete(st.value, key) pder.SessionUpdate(st.sid) return nil&#125;func (st *SessionStore) SessionID() string &#123; return st.sid&#125;type Provider struct &#123; lock sync.Mutex //用来锁 sessions map[string]*list.Element //用来存储在内存 list *list.List //用来做gc&#125;func (pder *Provider) SessionInit(sid string) (session.Session, error) &#123; pder.lock.Lock() defer pder.lock.Unlock() v := make(map[interface&#123;&#125;]interface&#123;&#125;, 0) newsess := &amp;SessionStore&#123;sid: sid, timeAccessed: time.Now(), value: v&#125; element := pder.list.PushBack(newsess) pder.sessions[sid] = element return newsess, nil&#125;func (pder *Provider) SessionRead(sid string) (session.Session, error) &#123; if element, ok := pder.sessions[sid]; ok &#123; return element.Value.(*SessionStore), nil &#125; else &#123; sess, err := pder.SessionInit(sid) return sess, err &#125; return nil, nil&#125;func (pder *Provider) SessionDestroy(sid string) error &#123; if element, ok := pder.sessions[sid]; ok &#123; delete(pder.sessions, sid) pder.list.Remove(element) return nil &#125; return nil&#125;func (pder *Provider) SessionGC(maxlifetime int64) &#123; pder.lock.Lock() defer pder.lock.Unlock() for &#123; element := pder.list.Back() if element == nil &#123; break &#125; if (element.Value.(*SessionStore).timeAccessed.Unix() + maxlifetime) &lt; time.Now().Unix() &#123; pder.list.Remove(element) delete(pder.sessions, element.Value.(*SessionStore).sid) &#125; else &#123; break &#125; &#125;&#125;func (pder *Provider) SessionUpdate(sid string) error &#123; pder.lock.Lock() defer pder.lock.Unlock() if element, ok := pder.sessions[sid]; ok &#123; element.Value.(*SessionStore).timeAccessed = time.Now() pder.list.MoveToFront(element) return nil &#125; return nil&#125;func init() &#123; pder.sessions = make(map[string]*list.Element, 0) session.Register("memory", pder)&#125; 上面这个代码实现了一个内存存储的session机制。通过init函数注册到session管理器中。这样就可以方便的调用了。我们如何来调用该引擎呢？请看下面的代码 1234import ( "github.com/astaxie/session" _ "github.com/astaxie/session/providers/memory") 当import的时候已经执行了memory函数里面的init函数，这样就已经注册到session管理器中，我们就可以使用了，通过如下方式就可以初始化一个session管理器： 1234567var globalSessions *session.Manager//然后在init函数中初始化func init() &#123; globalSessions, _ = session.NewManager("memory", "gosessionid", 3600) go globalSessions.GC()&#125; 预防session劫持session劫持是一种广泛存在的比较严重的安全威胁，在session技术中，客户端和服务端通过session的标识符来维护会话， 但这个标识符很容易就能被嗅探到，从而被其他人利用。它是中间人攻击的一种类型。 本节将通过一个实例来演示会话劫持，希望通过这个实例，能让读者更好地理解session的本质。 session劫持过程我们写了如下的代码来展示一个count计数器： 123456789101112func count(w http.ResponseWriter, r *http.Request) &#123; sess := globalSessions.SessionStart(w, r) ct := sess.Get("countnum") if ct == nil &#123; sess.Set("countnum", 1) &#125; else &#123; sess.Set("countnum", (ct.(int) + 1)) &#125; t, _ := template.ParseFiles("count.gtpl") w.Header().Set("Content-Type", "text/html") t.Execute(w, sess.Get("countnum"))&#125; count.gtpl的代码如下所示： 1Hi. Now count:&#123;&#123;.&#125;&#125; 然后我们在浏览器里面刷新可以看到如下内容： 随着刷新，数字将不断增长，当数字显示为6的时候，打开浏览器(以chrome为例）的cookie管理器，可以看到类似如下的信息： 下面这个步骤最为关键: 打开另一个浏览器(这里我打开了firefox浏览器),复制chrome地址栏里的地址到新打开的浏览器的地址栏中。然后打开firefox的cookie模拟插件，新建一个cookie，把按上图中cookie内容原样在firefox中重建一份: 回车后，你将看到如下内容： 可以看到虽然换了浏览器，但是我们却获得了sessionID，然后模拟了cookie存储的过程。这个例子是在同一台计算机上做的，不过即使换用两台来做，其结果仍然一样。此时如果交替点击两个浏览器里的链接你会发现它们其实操纵的是同一个计数器。不必惊讶，此处firefox盗用了chrome和goserver之间的维持会话的钥匙，即gosessionid，这是一种类型的“会话劫持”。在goserver看来，它从http请求中得到了一个gosessionid，由于HTTP协议的无状态性，它无法得知这个gosessionid是从chrome那里“劫持”来的，它依然会去查找对应的session，并执行相关计算。与此同时 chrome也无法得知自己保持的会话已经被“劫持”。 session劫持防范cookieonly和token通过上面session劫持的简单演示可以了解到session一旦被其他人劫持，就非常危险，劫持者可以假装成被劫持者进行很多非法操作。那么如何有效的防止session劫持呢？ 其中一个解决方案就是sessionID的值只允许cookie设置，而不是通过URL重置方式设置，同时设置cookie的httponly为true,这个属性是设置是否可通过客户端脚本访问这个设置的cookie，第一这个可以防止这个cookie被XSS读取从而引起session劫持，第二cookie设置不会像URL重置方式那么容易获取sessionID。 第二步就是在每个请求里面加上token，实现类似前面章节里面讲的防止form重复递交类似的功能，我们在每个请求里面加上一个隐藏的token，然后每次验证这个token，从而保证用户的请求都是唯一性。 12345678h := md5.New()salt:="astaxie%^7&amp;8888"io.WriteString(h,salt+time.Now().String())token:=fmt.Sprintf("%x",h.Sum(nil))if r.Form["token"]!=token&#123; //提示登录&#125;sess.Set("token",token) 间隔生成新的SID还有一个解决方案就是，我们给session额外设置一个创建时间的值，一旦过了一定的时间，我们销毁这个sessionID，重新生成新的session，这样可以一定程度上防止session劫持的问题。 1234567createtime := sess.Get("createtime")if createtime == nil &#123; sess.Set("createtime", time.Now().Unix())&#125; else if (createtime.(int64) + 60) &lt; (time.Now().Unix()) &#123; globalSessions.SessionDestroy(w, r) sess = globalSessions.SessionStart(w, r)&#125; session启动后，我们设置了一个值，用于记录生成sessionID的时间。通过判断每次请求是否过期(这里设置了60秒)定期生成新的ID，这样使得攻击者获取有效sessionID的机会大大降低。 上面两个手段的组合可以在实践中消除session劫持的风险，一方面， 由于sessionID频繁改变，使攻击者难有机会获取有效的sessionID；另一方面，因为sessionID只能在cookie中传递，然后设置了httponly，所以基于URL攻击的可能性为零，同时被XSS获取sessionID也不可能。最后，由于我们还设置了MaxAge=0，这样就相当于session cookie不会留在浏览器的历史记录里面。 RPCRPC是远程过程调用Remote procedure call的简称，可以使运行远程代码就像本机代码一样而不用考虑通信编程以及开销。是分布式系统中不同节点间流行的通信方式，Go语言的标准库也提供了一个简单的RPC实现。 net/rpcPackage rpc provides access to the exported methods of an object across a network or other I/O connection. A server registers an object, making it visible as a service with the name of the type of the object. After registration, exported methods of the object will be accessible remotely. A server may register multiple objects (services) of different types but it is an error to register multiple objects of the same type. Hello World我们先构造一个HelloService类型，其中的Hello方法用于实现打印功能： 123456type HelloService struct &#123;&#125;func (p *HelloService) Hello(request string, reply *string) error &#123; *reply = "hello:" + request return nil&#125; 其中Hello方法必须满足Go语言的RPC规则：方法只能有两个可序列化的参数，其中第二个参数是指针类型，并且返回一个error类型，同时必须是公开的方法。 然后就可以将HelloService类型的对象注册为一个RPC服务，其中rpc.Register函数调用会将对象类型中所有满足RPC规则的对象方法注册为RPC函数，所有注册的方法会放在“HelloService”服务空间之下。然后我们建立一个唯一的TCP链接，并且通过rpc.ServeConn函数在该TCP链接上为对方提供RPC服务。 123456789101112131415func main() &#123; rpc.RegisterName("HelloService", new(HelloService)) listener, err := net.Listen("tcp", ":1234") if err != nil &#123; log.Fatal("ListenTCP error:", err) &#125; conn, err := listener.Accept() if err != nil &#123; log.Fatal("Accept error:", err) &#125; rpc.ServeConn(conn)&#125; 下面是客户端请求HelloService服务的代码，首先是通过rpc.Dial拨号RPC服务，然后通过client.Call调用具体的RPC方法。在调用client.Call时，第一个参数是用点号链接的RPC服务名字和方法名字，第二和第三个参数分别我们定义RPC方法的两个参数。 1234567891011121314func main() &#123; client, err := rpc.Dial("tcp", "localhost:1234") if err != nil &#123; log.Fatal("dialing:", err) &#125; var reply string err = client.Call("HelloService.Hello", "hello", &amp;reply) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(reply)&#125; 由这个例子可以看出RPC的使用其实非常简单。 Arith这是go官网所给出的一个例子，A server wishes to export an object of type Arith: 123456789101112131415161718192021222324252627package serverimport "errors"type Args struct &#123; A, B int&#125;type Quotient struct &#123; Quo, Rem int&#125;type Arith intfunc (t *Arith) Multiply(args *Args, reply *int) error &#123; *reply = args.A * args.B return nil&#125;func (t *Arith) Divide(args *Args, quo *Quotient) error &#123; if args.B == 0 &#123; return errors.New("divide by zero") &#125; quo.Quo = args.A / args.B quo.Rem = args.A % args.B return nil&#125; The server calls (for HTTP service): 12345678arith := new(Arith)rpc.Register(arith)rpc.HandleHTTP()l, e := net.Listen("tcp", ":1234")if e != nil &#123; log.Fatal("listen error:", e)&#125;go http.Serve(l, nil) At this point, clients can see a service “Arith” with methods “Arith.Multiply” and “Arith.Divide”. To invoke one, a client first dials the server: 1234client, err := rpc.DialHTTP("tcp", serverAddress + ":1234")if err != nil &#123; log.Fatal("dialing:", err)&#125; Then it can make a remote call: 12345678// Synchronous callargs := &amp;server.Args&#123;7,8&#125;var reply interr = client.Call("Arith.Multiply", args, &amp;reply)if err != nil &#123; log.Fatal("arith error:", err)&#125;fmt.Printf("Arith: %d*%d=%d", args.A, args.B, reply) or 12345// Asynchronous callquotient := new(Quotient)divCall := client.Go("Arith.Divide", args, quotient, nil)replyCall := &lt;-divCall.Done // will be equal to divCall// check errors, print, etc. 这里暂时只做简单介绍（毕竟只是为了写MIT6.824的Labs）更多详情可以见官方文档 https://golang.org/pkg/net/rpc/]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>Go</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go日记]]></title>
    <url>%2Fpost%2F2766.html</url>
    <content type="text"><![CDATA[记录下休学期间学习Go语言入门的一些想法、笔记和踩过的一些坑。 希望之后这个Go系列还会继续完善下去不被弃坑（小声） 安装下载因为一些奇怪的原因我分别在Windows和Linux子系统上安装了Go。 Windows下载地址 笔者现安装的版本是go version go1.13.4 windows/amd64 安装完成后默认会在环境变量 Path 后添加 Go 安装目录下的 bin 目录 C:\Go\bin\，并添加环境变量 GOROOT，值为 Go 安装根目录 C:\Go\因为笔者电脑内存不够就放在了D盘，于是修改一通环境变量，最主要就是GOROOT目录下存在go.exe以及你的代码放置区域要存在GOPATH里（GOPATH在go提出GO MOD之后就没那么重要了)。 Linux强烈建议不要直接apt install而是去官网下载最新的版本手动安装 因为之前C++课程使用的VSCode接WSL确实用得舒服一点，就直接在WSL环境下ubuntu使用Go第一次apt直接安装sudo apt install golang-go版本为go version go1.10.4 linux/amd64然后笔者觉得还是统一下会比较好一点（强迫症）于是去官网下了Linux的1.13版本解压在本地安装，然后因为卑微的C盘，于是把go文件夹放在了D盘，由于之Windows版本的Go放在D盘就给Linux版本文件夹重命名了一下，修改PATH：export GOPATH=/mnt/e/Program/Goexport GOROOT=/mnt/d/Go_linuxexport PATH=$PATH:$GOROOT/bin:$GOPATH/binsource /etc/profile 检查至此理论上就能跑了，不放心可以用go version检查版本go env检查环境变量。 基础结构用hello world信仰开头：12345package mainimport "fmt"func main() &#123; fmt.Printf("Hello, world 你好，世界 καλημ ́ρα κóσμ こんにちはせかい\n")&#125; package &lt;pkgName&gt;表明当前文件属于哪个包，包名main表明它是一个可独立运行的包，编译后会产生可执行文件。除了main包之外，其它的包最后都会生成*.a文件（包文件）并放置在$GOPATH/pkg/$GOOS_$GOARCH中。每个可独立运行的Go程序，必定包含一个package main其中必含一个无参无return的入口函数main。 Go使用UTF-8字符串和标识符。 变量使用var关键字是Go最基本的定义变量方式，Go把变量类型放在变量名后面： 123456789//初始化“variableName”的变量为“value”值，类型是“type”var variableName type = value//定义三个变量，分别初始化相应值，编译器会根据初始化值自动推导相应类型var vname1, vname2, vname3 = v1, v2, v3/* :=这个符号直接取代了var和type，但只能用在函数内部 定义全局变量一般还是猜用var方式来*/vname1, vname2, vname3 := v1, v2, v3 _是个特殊的变量名，任何赋予它的值都会被丢弃。eg.我们将值35赋予b，并同时丢弃34： 1_, b := 34, 35 Go对于已声明但未使用的变量会在编译阶段报错，eg.声明了i但未使用。 12345package mainfunc main() &#123; var i int&#125; 常量在Go程序中，常量可定义为数值、布尔值或字符串等类型。 123const constantName = value//如果需要，也可以明确指定常量的类型：const Pi float32 = 3.1415926 Go 常量和一般程序语言不同的是，可以指定相当多的小数位数(例如200位)， 若指定給float32自动缩短为32bit，指定给float64自动缩短为64bit，详情参考链接 基础类型Boolean12345678//在Go中，布尔值的类型为bool，值是true或false，默认为false。var isActive bool // 全局变量声明var enabled, disabled = true, false // 忽略类型的声明func test() &#123; var available bool // 一般声明 valid := false // 简短声明 available = true // 赋值操作&#125; 数值类型Go同时支持int和uint，两种类型长度相同，但具体长度取决于编译器的实现。Go里面也有直接定义好位数的类型：int8, int16, int32(rune), int64和uint8(byte), uint16, uint32, uint64。不同类型的变量之间不允许互相赋值或操作！ 浮点数的类型有float32和float64两种（没有float类型），默认是float64。 复数默认类型是complex128（64位实数+64位虚数）也有complex64(32位实数+32位虚数)复数的形式为RE + IMi，其中RE是实数部分，IM是虚数部分，而最后的i是虚数单位。 字符串1234567//Go中的字符串都是采用UTF-8字符集编码var frenchHello string // 声明变量为字符串的一般方法var emptyString string = "" // 声明了一个字符串变量，初始化为空字符串func test() &#123; no, yes, maybe := "no", "yes", "maybe" // 简短声明，同时声明多个变量 frenchHello = "Bonjour" // 常规赋值&#125; 在Go中字符串不能当初char数组修改，例如下面的代码编译时会报错：cannot assign to s[0] 12var s string = "hello"s[0] = 'c' 真的需要修改，要将字符串 s 转换为 []byte 类型，修改后再转回 string 类型： 12345s := "hello"c := []byte(s) // 将字符串 s 转换为 []byte 类型c[0] = 'c's2 := string(c) // 再转换回 string 类型fmt.Printf("%s\n", s2) Go中可以使用+操作符来连接两个字符串所以字符串虽不能更改，但可进行切片操作，故修改字符串也可写为： 123s := "hello"s = "c" + s[1:] // 字符串虽不能更改，但可进行切片操作fmt.Printf("%s\n", s) 如果要声明一个多行的字符串怎么办？可以通过 `来声明： 12m := `hello world` ` 括起的字符串为Raw字符串，即字符串在代码中的形式就是打印时的形式，它没有字符转义，换行也将原样输出。例如本例中会输出： 12hello world ERRORGo内置有一个error类型，专门用来处理错误信息，Go的package里面还专门有一个包errors来处理错误： 1234err := errors.New("emit macho dwarf: elf header corrupted")if err != nil &#123; fmt.Print(err)&#125; 分组声明12345678910111213141516import( "fmt" "os")const( i = 100 pi = 3.1415 prefix = "Go_")var( i int pi float32 prefix string) 代码规范 大写字母开头的变量是可导出的，也就是其它包可以读取的，是公有变量； 小写字母开头的就是不可导出的，是私有变量。 大写字母开头的函数相当于class中的带public关键词的公有函数； 小写字母开头的函数相当于private关键词的私有函数。 内建类型array1234var arr [10]int // 声明了一个int类型的数组arr[0] = 42 // 数组下标是从0开始的fmt.Printf("The first one is %d\n", arr[0]) // 返回42fmt.Printf("The last one is %d\n", arr[9]) // 未赋值默认返回0 数组间的赋值是值的赋值，即当把一个数组作为参数传入函数的时候，传入的其实是该数组的副本，而不是它的指针。如果要使用指针，那么就需要用到后面介绍的slice类型了。 123a := [3]int&#123;1, 2, 3&#125; // 简短声明了一个长度为3的int数组b := [10]int&#123;1, 2, 3&#125; // 简短声明，前三个元素初始化为1、2、3，其它默认为0c := [...]int&#123;4, 5, 6&#125; // 采用`...`的方式Go会自动根据元素个数来计算长度 1234// 声明了一个二维数组，该数组以两个数组作为元素，其中每个数组中又有4个int类型的元素doubleArray := [2][4]int&#123;[4]int&#123;1, 2, 3, 4&#125;, [4]int&#123;5, 6, 7, 8&#125;&#125;// 上面的声明可以简化，直接忽略内部的类型easyArray := [2][4]int&#123;&#123;1, 2, 3, 4&#125;, &#123;5, 6, 7, 8&#125;&#125; slice初始定义数组时并不知道数组长度，在Go里面这种数据结构叫slice。slice并不是真正意义上的动态数组，而是一个引用类型。slice总是指向一个底层array。 12var fslice []intslice := []byte &#123;'a', 'b', 'c', 'd'&#125; 123456// 声明一个含有10个元素元素类型为byte的数组var ar = [10]byte &#123;'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'&#125;// 声明两个含有byte的slicevar a, b []bytea = ar[2:5] // a含有的元素: ar[2]、ar[3]和ar[4]b = ar[3:5] // b的元素是：ar[3]和ar[4] array方括号内写明数组长度或使用...自动计算长度，声明slice时，方括号内没有任何字符。 slice的默认开始位置是0，ar[:n]等价于ar[0:n] slice的默认结束位置是数组长度，ar[n:]等价于ar[n:len(ar)] 如果从一个数组里面直接获取slice，可以这样ar[:]等价于ar[0:len(ar)] slice是引用类型，所以当引用改变其中元素的值时，其它的所有引用都会改变该值。 slice内置函数： len 获取slice的长度 cap 获取slice的最大容量 append 向slice里追加一或多个元素，然后返回一个和修改后slice一样类型的slice copy 函数copy从源slice的src中复制元素到目标dst，并且返回复制的元素的个数 但当slice中没有剩余空间（即(cap-len) == 0）时，此时将动态分配新的数组空间。返回的slice数组指针将指向这个空间，而原数组的内容将保持不变；其它引用此数组的slice则不受影响。 1234var array [10]intslice := array[2:4] // slice的容量10-2，即8slice = array[2:4:7] // 第三个参数可以指定容量// 容量为7-2，即5。这样新的slice就没办法访问array最后三个元素 mapmap也就是Python中字典的概念 123456789// 声明一个key是字符串，值为int的字典,这种方式的声明需要在使用之前使用make初始化var numbers map[string]int// 另一种map的声明方式numbers := make(map[string]int)numbers["one"] = 1 //赋值numbers["ten"] = 10 //赋值numbers["three"] = 3fmt.Println("第三个数字是: ", numbers["three"]) // 读取数据// 打印出来如:第三个数字是: 3 使用map过程中需要注意的几点： map无序，每次打印出的map会不一样，它不能通过index获取，而必须通过key获取 map长度不固定，和slice一样是引用类型，如果两个map同时指向一个底层，一个改变，另一个也相应改变 内置的len函数同样适用于map，返回map拥有的key的数量 map的值很方便修改，通过numbers[&quot;one&quot;]=11可以把key为one的字典值改为11 map和其他基本型别不同，它不是thread-safe，在多个go-routine存取时，必须使用mutex lock机制 map的初始化可以通过key:val的方式初始化值，同时map内置有判断是否存在key的方式 123456789rating := map[string]float32&#123;"C":5, "Go":4.5, "Python":4.5, "C++":2 &#125;// map有两个返回值，第二个返回值，如果不存在key，那么ok为false，如果存在ok为truecsharpRating, ok := rating["C#"]if ok &#123; fmt.Println("C# is in the map and its rating is ", csharpRating)&#125; else &#123; fmt.Println("We have no rating associated with C# in the map")&#125;delete(rating, "C") // 删除key为C的元素 make、new操作（TODO）内建函数new和make是两个用于内存分配的原语，简单说new只分配内存，make用于slice，map，和channel的初始化。在Go语言中，如果一个局部变量在函数返回后仍然被使用，这个变量会从heap，而不是stack中分配内存。内建函数make(T, args)与new(T)的用途不一样。它只用来创建slice，map和channel，并且返回一个初始化的(而不是置零)，类型为T的值（而不是*T）。之所以有所不同，是因为这三个类型的背后引用了使用前必须初始化的数据结构。例如，slice是一个三元描述符，包含一个指向数据（在数组中）的指针，长度，以及容量，在这些项被初始化之前，slice都是nil的。对于slice，map和channel，make初始化这些内部数据结构，并准备好可用的值。记住make只用于map，slice和channel，并且不返回指针。要获得一个显式的指针，使用new进行分配，或者显式地使用一个变量的地址。 流程控制ifGo里面if条件判断语句中不需要括号，如下代码所示 12345if x &gt; 10 &#123; fmt.Println("x is greater than 10")&#125; else &#123; fmt.Println("x is less than 10")&#125; if语句里允许声明一个变量，变量作用域只能在该条件逻辑块内，有点类似python的for i in range(100): 12345678// 计算获取值x,然后根据x返回的大小，判断是否大于10。if x := computedValue(); x == 10 &#123; fmt.Println("x is equal to 10")&#125; else if x &gt; 10 &#123; fmt.Println("x is greater than 10")&#125; else &#123; fmt.Println("x is less than 10")&#125; for12345678910package mainimport "fmt"func main()&#123; sum := 0; for index:=0; index &lt; 10 ; index++ &#123; sum += index &#125; fmt.Println("sum is equal to ", sum)&#125;// 输出：sum is equal to 45 有时我们可以省略一点 1234sum := 1for ; sum &lt; 1000; &#123; sum += sum&#125; 甚至更省略一点，看起来就像个while，配合continue和break用风味更佳 1234sum := 1for sum &lt; 1000 &#123; sum += sum&#125; for配合range可以用于读取slice和map的数据（真的很像python啊） 1234for k,v:=range map &#123; fmt.Println("map's key:",k) fmt.Println("map's val:",v)&#125; Go 对于“声明而未被调用”的变量, 编译器会报错, 于是用_来丢弃不需要的返回值 123for _, v := range map&#123; fmt.Println("map's val:", v)&#125; switch1234567891011i := 10switch i &#123;case 1: fmt.Println("i is equal to 1")case 2, 3, 4: fmt.Println("i is equal to 2, 3 or 4")case 10: fmt.Println("i is equal to 10")default: fmt.Println("All I know is that i is an integer")&#125; Go里面switch默认相当于每个case最后带有break，匹配成功后不会自动向下执行其他case，而是跳出整个switch, 但是可以在case最后加上fallthrough强制执行后面的case代码。 函数声明1234func funcName(input1 type1, input2 type2) (output1 type1, output2 type2) &#123; //这里是处理逻辑代码 return value1, value2&#125; 来个实例： 12345678910111213141516package mainimport "fmt"func SumAndProduct(A, B int) (int, int) &#123; return A+B, A*B&#125;func main() &#123; x := 3 y := 4 xPLUSy, xTIMESy := SumAndProduct(x, y) fmt.Printf("%d + %d = %d\n", x, y, xPLUSy) fmt.Printf("%d * %d = %d\n", x, y, xTIMESy)&#125; 当然，函数的声明还可以更人性化，可读性更强一点： 12345func SumAndProduct(A, B int) (add int, Multiplied int) &#123; add = A+B Multiplied = A*B return&#125; 变参接受变参的函数是有着不定数量的参数的。为了做到这点，首先需要定义函数使其接受变参： 1func myfunc(arg ...int) &#123;&#125; arg ...int告诉Go这个函数接受不定数量的参数。注意，这些参数的类型全部是int。在函数体中，变量arg是一个int的slice： 123for _, n := range arg &#123; fmt.Printf("And the number is: %d\n", n)&#125; 传值与传指针当传参到函数里时，实际是传了这个值的一份copy，当在被调用函数中修改参数值的时候，调用函数中相应实参不会发生任何变化，因为数值变化只作用在copy上，而想直接传这个值本身就需要用到指针。 变量在内存中是存放于一定地址上的，修改变量实际是修改变量地址处的内存。只有add1函数知道x变量所在的地址，才能修改x变量的值。所以我们需要将x所在地址&amp;x传入函数，并将函数的参数的类型由int改为*int，即改为指针类型，才能在函数中修改x变量的值。此时参数仍然是按copy传递的，只是copy的是一个指针。 1234567891011121314151617package mainimport "fmt"//简单的一个函数，实现了参数+1的操作func add1(a *int) int &#123; // 请注意， *a = *a+1 // 修改了a的值 return *a // 返回新值&#125;func main() &#123; x := 3 fmt.Println("x = ", x) // 应该输出 "x = 3" x1 := add1(&amp;x) // 调用 add1(&amp;x) 传x的地址 fmt.Println("x+1 = ", x1) // 应该输出 "x+1 = 4" fmt.Println("x = ", x) // 应该输出 "x = 4"&#125; 这样，我们就达到了修改x的目的。那么到底传指针有什么好处呢？ 传指针使得多个函数能操作同一个对象。 传指针比较轻量级 (8bytes)只传内存地址，我们可以用指针传递体积大的结构体。如果用参数值传递的话, 在每次copy上就会花费相对较多的系统开销（内存和时间）。所以当传递大结构体的时候，用指针是一个明智的选择。 Go语言中channel，slice，map这三种类型的实现机制类似指针，所以可以直接传递，而不用取地址后传递指针。（注：若函数需改变slice的长度，则仍需要取地址传递指针） deferGo支持延迟（defer）语句，可以在函数中添加多个defer语句。当函数执行到最后时，这些defer语句会按照逆序执行，最后该函数返回。 在进行一些打开资源的操作时，遇到错误需要提前返回，在返回前需要关闭相应的资源，不然很容易造成资源泄露等问题。如下代码所示，我们一般写打开一个资源是这样操作的： 1234567891011121314func ReadWrite() bool &#123; file.Open("file") // 做一些工作 if failureX &#123; file.Close() return false &#125; if failureY &#123; file.Close() return false &#125; file.Close() return true&#125; 而使用defer则会显得优雅很多，在defer后指定的函数会在函数退出前调用。 1234567891011func ReadWrite() bool &#123; file.Open("file") defer file.Close() if failureX &#123; return false &#125; if failureY &#123; return false &#125; return true&#125; 如果有很多调用defer，那么defer是采用后进先出模式，所以如下代码会输出4 3 2 1 0 123for i := 0; i &lt; 5; i++ &#123; defer fmt.Printf("%d ", i) // 4 3 2 1 0&#125; 函数作为值、类型在Go中函数也是一种变量，我们可以通过type来定义它 1type typeName func(input1 type1, input2 type2 [, ...]) (result1 type1 [, ...]) 123456789101112131415161718192021222324252627282930313233343536373839package mainimport "fmt"type testInt func(int) bool // 声明了一个函数类型func isOdd(integer int) bool &#123; if integer%2 == 0 &#123; return false &#125; return true&#125;func isEven(integer int) bool &#123; if integer%2 == 0 &#123; return true &#125; return false&#125;// 声明的函数类型在这个地方当做了一个参数func filter(slice []int, f testInt) []int &#123; var result []int for _, value := range slice &#123; if f(value) &#123; result = append(result, value) &#125; &#125; return result&#125;func main()&#123; slice := []int &#123;1, 2, 3, 4, 5, 7&#125; fmt.Println("slice = ", slice) odd := filter(slice, isOdd) // 函数当做值来传递了 fmt.Println("Odd elements of slice are: ", odd) even := filter(slice, isEven) // 函数当做值来传递了 fmt.Println("Even elements of slice are: ", even)&#125; 函数当做值和类型在写一些通用接口的时候非常有用，程序灵活性也会大大增加。 Panic和RecoverGo没有像Java那样的异常机制，而是使用了panic和recover机制。BUT代码中应当没有，或很少有panic。 Panic 是一个内建函数，可以中断原有的控制流程，进入一个令人恐慌的流程中。当函数F调用panic，函数F的执行被中断，但是F中的延迟函数会正常执行，然后F返回到调用它的地方。在调用的地方，F的行为就像调用了panic。这一过程继续向上，直到发生panic的goroutine中所有调用的函数返回，此时程序退出。恐慌可以直接调用panic产生。也可以由运行时错误产生，例如访问越界的数组。 Recover 是一个内建的函数，可以让进入令人恐慌的流程中的goroutine恢复过来。recover仅在延迟函数中有效。在正常的执行过程中，调用recover会返回nil，并且没有其它任何效果。如果当前的goroutine陷入恐慌，调用recover可以捕获到panic的输入值，并且恢复正常的执行。 下面这个函数演示了如何在过程中使用panic 1234567var user = os.Getenv("USER")func init() &#123; if user == "" &#123; panic("no value for $USER") &#125;&#125; 下面这个函数检查作为其参数的函数在执行时是否会产生panic： 123456789func throwsPanic(f func()) (b bool) &#123; defer func() &#123; if x := recover(); x != nil &#123; b = true &#125; &#125;() f() //执行函数f，如果f中出现了panic，那么就可以恢复回来 return&#125; main函数和init函数Go有两个保留的函数：init函数（能用于所有package）和main函数（只用于package main）。这两个函数在定义时不能有任何的参数和返回值。虽然一个package里面可以写任意多个init函数，但这无论是对于可读性还是以后的可维护性来说，都强烈建议在一个package中每个文件只写一个init函数。 Go程序会自动调用init()和main()，所以你不需要在任何地方调用这两个函数。每个package中的init函数都是可选的，但package main就必须包含一个main函数。 程序的初始化和执行都起始于main包。如果main包还导入了其它的包，那会在编译时将它们依次导入。若一个包被多个包同时导入，那它只会被导入一次（例如很多包可能都会用到fmt包，但它只会被导入一次）。当一个包被导入时，如果该包还导入了其它的包，那会先将其它包导入进来，然后再对这些包中的包级常量和变量进行初始化，接着执行init函数（如果有的话）依次类推。等所有被导入的包都加载完毕了，就会开始对main包中的包级常量和变量进行初始化，然后执行main包中的init函数（如果存在的话）最后执行main函数。 import用import命令来导入包文件，而我们经常看到的方式参考如下： 123import( "fmt") 然后我们代码里面可以通过如下的方式调用 1fmt.Println("hello world") 上面这个fmt是Go语言的标准库，其实是去GOROOT环境变量指定目录下去加载该模块，当然Go的import还支持用相对路径或者绝对路径来加载自己写的模块： 12import “./model” //当前文件同一目录的model目录，但是不建议这种方式来importimport “shorturl/model” //加载gopath/src/shorturl/model模块 上面展示了一些import常用的几种方式，但是还有一些特殊的import 点操作 123import( . &quot;fmt&quot; ) 表示这个包导入之后，在调用这个包的函数时，可以省略前缀的包名，即调用fmt.Println(&quot;hello world&quot;)可以直接写成Println(&quot;hello world&quot;) 别名操作 123import( f &quot;fmt&quot; ) 顾名思义，调用包函数时前缀变成了我们的前缀，即f.Println(&quot;hello world&quot;) _操作 1234import ( "database/sql" _ "github.com/ziutek/mymysql/godrv" ) _操作其实是引入该包，而不直接使用包里面的函数，而是调用了该包里面的init函数。 struct 12345678910type person struct &#123; name string age int&#125;var P person // P现在就是person类型的变量了P.name = "Astaxie" // 赋值"Astaxie"给P的name属性.P.age = 25 // 赋值"25"给变量P的age属性fmt.Printf("The person's name is %s", P.name) // 访问P的name属性. 除了上面这种P的声明使用之外，还有另外几种声明使用方式： 1.按照顺序提供初始化值 P := person{“Tom”, 25} 2.通过field:value的方式初始化，这样可以任意顺序 P := person{age:24, name:”Tom”} 3.当然也可以通过new函数分配一个指针，此处P的类型为*person P := new(person) struct的匿名字段 Go支持只提供类型，而不写字段名的方式，也就是匿名字段，也称为嵌入字段。 当匿名字段是一个struct的时候，那么这个struct所拥有的全部字段都被隐式地引入了当前定义的这个struct。 1234567891011121314151617181920212223242526package mainimport "fmt"type Human struct &#123; name string age int weight int&#125;type Student struct &#123; Human // 匿名字段，那么默认Student就包含了Human的所有字段 speciality string&#125;func main() &#123; // 初始化一个学生 mark := Student&#123;Human&#123;"Mark", 25, 120&#125;, "Computer Science"&#125; // 访问相应的字段 fmt.Println("His name is ", mark.name) fmt.Println("His speciality is ", mark.speciality) // 修改对应的备注信息 mark.speciality = "AI" // 修改他的体重信息 mark.weight += 60 fmt.Println("His weight is", mark.weight)&#125; 匿名字段就是这样，能够实现字段的继承。 同时student还能访问Human这个字段作为字段名。 12mark.Human = Human&#123;"Marcus", 55, 220&#125;mark.Human.age -= 1 所有的内置类型和自定义类型都是可以作为匿名字段。 123456789101112131415161718192021222324252627282930package mainimport "fmt"type Skills []stringtype Human struct &#123; name string age int weight int&#125;type Student struct &#123; Human // 匿名字段，struct Skills // 匿名字段，自定义的类型string slice int // 内置类型作为匿名字段 speciality string&#125;func main() &#123; // 初始化学生Jane jane := Student&#123;Human:Human&#123;"Jane", 35, 100&#125;, speciality:"Biology"&#125; // 修改自定义类型skill技能字段 jane.Skills = []string&#123;"anatomy"&#125; fmt.Println("Her skills are ", jane.Skills) jane.Skills = append(jane.Skills, "physics", "golang") fmt.Println("Her skills now are ", jane.Skills) // 修改匿名内置类型字段 jane.int = 3 fmt.Println("Her preferred number is", jane.int)&#125; 可以看到这种类似于继承的方式，真的非常人性化了。 面向对象函数的另一种形态，带有接收者的函数，我们称为method method用Rob Pike的话来说就是： “A method is a function with an implicit first argument, called a receiver.” method的语法如下，注意不要和function弄混哦： 1func (r ReceiverType) funcName(parameters) (results) 下面我们用最开始的例子用method来实现： 12345678910111213141516171819202122232425262728package mainimport ( "fmt" "math")type Rectangle struct &#123; width, height float64&#125;type Circle struct &#123; radius float64&#125;func (r Rectangle) area() float64 &#123; return r.width*r.height&#125;func (c Circle) area() float64 &#123; return c.radius * c.radius * math.Pi&#125;func main() &#123; r := Rectangle&#123;12, 2&#125; c := Circle&#123;10&#125; fmt.Println("Area of r is: ", r.area()) fmt.Println("Area of c is: ", c.area())&#125; 在使用method的时候重要注意几点 虽然method的名字一模一样，但是如果接收者不一样，那么method就不一样 method里面可以访问接收者的字段 调用method通过.访问，就像struct里面访问字段一样 除了结构体这一比较特殊的自定义类型外，还可以在任意自定义类型中定义任意多的method 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package mainimport "fmt"const( WHITE = iota BLACK BLUE RED YELLOW)type Color bytetype Box struct &#123; width, height, depth float64 color Color&#125;type BoxList []Box //a slice of boxesfunc (b Box) Volume() float64 &#123; return b.width * b.height * b.depth&#125;func (b *Box) SetColor(c Color) &#123; b.color = c&#125;func (bl BoxList) BiggestColor() Color &#123; v := 0.00 k := Color(WHITE) for _, b := range bl &#123; if bv := b.Volume(); bv &gt; v &#123; v = bv k = b.color &#125; &#125; return k&#125;func (bl BoxList) PaintItBlack() &#123; for i, _ := range bl &#123; bl[i].SetColor(BLACK) &#125;&#125;func (c Color) String() string &#123; strings := []string &#123;"WHITE", "BLACK", "BLUE", "RED", "YELLOW"&#125; return strings[c]&#125;func main() &#123; boxes := BoxList &#123; Box&#123;4, 4, 4, RED&#125;, Box&#123;10, 10, 1, YELLOW&#125;, Box&#123;1, 1, 20, BLACK&#125;, Box&#123;10, 10, 1, BLUE&#125;, Box&#123;10, 30, 1, WHITE&#125;, Box&#123;20, 20, 20, YELLOW&#125;, &#125; fmt.Printf("We have %d boxes in our set\n", len(boxes)) fmt.Println("The volume of the first one is", boxes[0].Volume(), "cm³") fmt.Println("The color of the last one is",boxes[len(boxes)-1].color.String()) fmt.Println("The biggest one is", boxes.BiggestColor().String()) fmt.Println("Let's paint them all black") boxes.PaintItBlack() fmt.Println("The color of the second one is", boxes[1].color.String()) fmt.Println("Obviously, now, the biggest one is", boxes.BiggestColor().String())&#125; 指针作为receiverSetColor这个method，它的receiver是一个指向Box的指针，这不难理解。 Q: 那SetColor函数里应该是*b.Color=c,而不是b.Color=c才对啊,因为需要读取到指针相应的值。 A: 其实Go里面这两种方式都ok，当你用指针去访问相应的字段时(虽然指针没有任何的字段)，Go知道要通过指针去获取这个值，多人性化。 Q: 那PaintItBlack里面调用SetColor不应该写成(&amp;bl[i]).SetColor(BLACK)吗，因为SetColor的receiver是*Box，而不是Box。 A: Yep，但这两种方式都可以，因为Go知道receiver是指针，就自动帮你转了。 也就是说： 如果一个method的receiver是*T,你可以在一个T类型的实例变量V上面调用这个method，而不需要&amp;V去调用这个method 类似的 如果一个method的receiver是T，你可以在一个T类型的变量P上面调用这个method，而不需要 P去调用这个method method继承&amp;重写如果匿名字段实现了一个method，那么包含这个匿名字段的struct也能调用该method包括重写这个method。 1234567891011121314151617181920212223242526272829303132333435363738package mainimport "fmt"type Human struct &#123; name string age int phone string&#125;type Student struct &#123; Human //匿名字段 school string&#125;type Employee struct &#123; Human //匿名字段 company string&#125;//Human定义methodfunc (h *Human) SayHi() &#123; fmt.Printf("Hi, I am %s you can call me on %s\n", h.name, h.phone)&#125;//Employee的method重写Human的methodfunc (e *Employee) SayHi() &#123; fmt.Printf("Hi, I am %s, I work at %s. Call me on %s\n", e.name, e.company, e.phone) //Yes you can split into 2 lines here.&#125;func main() &#123; mark := Student&#123;Human&#123;"Mark", 25, "222-222-YYYY"&#125;, "MIT"&#125; sam := Employee&#123;Human&#123;"Sam", 45, "111-888-XXXX"&#125;, "Golang Inc"&#125; mark.SayHi() sam.SayHi()&#125; 通过这些内容，我们可以设计出基本的面向对象的程序了，但是Go里面的面向对象是如此的简单，没有任何的私有、公有关键字，通过大小写来实现(大写开头的为公有，小写开头的为私有)，方法也同样适用这个原则。 Interface什么是interface简单的说，interface是一组method签名的组合，我们通过interface来定义对象的一组行为。interface定义了一组方法，如果某个对象实现了某个接口的所有方法，则此对象实现了此接口。interface可以被任意的对象实现，一个对象可以实现任意多个interface。 interface值一个interface变量可以存实现这个interface的任意类型的对象。 例如定义了一个Men interface类型的变量m，那么m可以存Human、Student或者Employee值。因为m能够持有这三种类型的对象，那我们可以定义一个Men类型的slicex := make([]Men, 3)，这个slice可以被赋予实现了Men接口的任意结构的对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package mainimport "fmt"type Human struct &#123; name string age int phone string&#125;type Student struct &#123; Human //匿名字段 school string loan float32&#125;type Employee struct &#123; Human //匿名字段 company string money float32&#125;func (h Human) SayHi() &#123; fmt.Printf("Hi, I am %s you can call me on %s\n", h.name, h.phone)&#125;func (h Human) Sing(lyrics string) &#123; fmt.Println("La la la la...", lyrics)&#125;//Employee重载Human的SayHi方法func (e Employee) SayHi() &#123; fmt.Printf("Hi, I am %s, I work at %s. Call me on %s\n", e.name, e.company, e.phone) &#125;// Interface Men被Human,Student和Employee实现// 因为这三个类型都实现了这两个方法type Men interface &#123; SayHi() Sing(lyrics string)&#125;func main() &#123; mike := Student&#123;Human&#123;"Mike", 25, "222-222-XXX"&#125;, "MIT", 0.00&#125; paul := Student&#123;Human&#123;"Paul", 26, "111-222-XXX"&#125;, "Harvard", 100&#125; sam := Employee&#123;Human&#123;"Sam", 36, "444-222-XXX"&#125;, "Golang Inc.", 1000&#125; tom := Employee&#123;Human&#123;"Tom", 37, "222-444-XXX"&#125;, "Things Ltd.", 5000&#125; //定义Men类型的变量i var i Men //i能存储Student i = mike fmt.Println("This is Mike, a Student:") i.SayHi() i.Sing("November rain") //i也能存储Employee i = tom fmt.Println("This is tom, an Employee:") i.SayHi() i.Sing("Born to be wild") //定义了slice Men fmt.Println("Let's use a slice of Men and see what happens") x := make([]Men, 3) //这三个都是不同类型的元素，但是他们实现了interface同一个接口 x[0], x[1], x[2] = paul, sam, mike for _, value := range x&#123; value.SayHi() &#125;&#125; interface就是一组抽象方法的集合，必须由其他非interface类型实现，而不能自我实现。 空interface空interface(interface{})不包含任何的method，正因为如此，所有的类型都实现了空interface。 空interface可以存储任意类型的数值，有点类似于C语言的void*类型。 1234567// 定义a为空接口var a interface&#123;&#125;var i int = 5s := "Hello world"// a可以存储任意类型的数值a = ia = s 一个函数把interface{}作为参数，那么他可以接受任意类型的值作为参数；如果一个函数返回interface{}，那么也就可以返回任意类型的值。 interface函数参数interface的变量可以持有任意实现该interface类型的对象，那是不是可以通过定义interface参数，让函数接受各种类型的参数。比如fmt.Println可以接受任意类型的数据，即任何实现了String方法的类型都能作为参数被fmt.Println调用。 123type Stringer interface &#123; String() string&#125; 123456789101112131415161718192021package mainimport ( "fmt" "strconv")type Human struct &#123; name string age int phone string&#125;// 通过这个方法 Human 实现了 fmt.Stringerfunc (h Human) String() string &#123; return "❰"+h.name+" - "+strconv.Itoa(h.age)+" years - ✆ " +h.phone+"❱"&#125;func main() &#123; Bob := Human&#123;"Bob", 39, "000-7777-XXX"&#125; fmt.Println("This Human is : ", Bob)&#125; method：String实现了fmt.Stringer这个interface，即如果需要某个类型能被fmt包以特殊的格式输出，就必须实现Stringer接口。如果没有实现这个接口，fmt将以默认的方式输出。 123//实现同样的功能fmt.Println("The biggest one is", boxes.BiggestsColor().String())fmt.Println("The biggest one is", boxes.BiggestsColor()) 注：实现了error接口的对象（即实现了Error() string的对象），使用fmt输出时，会调用Error()方法，因此不必再定义String()方法了。 interface变量存储的类型我们知道interface的变量里面可以存储任意类型的数值(该类型实现了interface)。那怎么反向知道这个变量里面实际保存了的是哪个类型的对象呢？目前常用的有两种方法： Comma-ok断言 直接判断是否是该类型的变量： value, ok = element.(T)，这里value就是变量的值，ok是一个bool类型，element是interface变量，T是断言的类型。 如果element里面确实存储了T类型的数值，ok返回true，否则返回false（但这样一般会引入大量if-else） switch测试 123456789101112131415161718192021222324252627282930313233343536373839package main import ( "fmt" "strconv" ) type Element interface&#123;&#125; type List [] Element type Person struct &#123; name string age int &#125; //打印 func (p Person) String() string &#123; return "(name: " + p.name + " - age: "+strconv.Itoa(p.age)+ " years)" &#125; func main() &#123; list := make(List, 3) list[0] = 1 //an int list[1] = "Hello" //a string list[2] = Person&#123;"Dennis", 70&#125; for index, element := range list&#123; switch value := element.(type) &#123; case int: fmt.Printf("list[%d] is an int and its value is %d\n", index, value) case string: fmt.Printf("list[%d] is a string and its value is %s\n", index, value) case Person: fmt.Printf("list[%d] is a Person and its value is %s\n", index, value) default: fmt.Println("list[%d] is of a different type", index) &#125; &#125; &#125; element.(type)语法不能在switch外的任何逻辑里面使用，如果要在switch外面判断一个类型就使用comma-ok。 嵌入interfaceGo里面真正吸引人的是它内置的逻辑语法，就像我们在学习Struct时学习的匿名字段。如果一个interface1作为interface2的一个嵌入字段，那么interface2隐式的包含了interface1里面的method。 源码包container/heap里面有这样的一个定义： 12345type Interface interface &#123; sort.Interface //嵌入字段sort.Interface Push(x interface&#123;&#125;) //a Push method to push elements into the heap Pop() interface&#123;&#125; //a Pop elements that pops elements from the heap&#125; 另一个例子就是io包下面的 io.ReadWriter ，它包含了io包下面的Reader和Writer两个interface： 12345// io.ReadWritertype ReadWriter interface &#123; Reader Writer&#125; 反射所谓反射就是能检查程序在运行时的状态，一般用到的包是reflect包reflect包的实现原理 使用reflect一般分成三步：要去反射是一个类型的值(这些值都实现了空interface)，首先需要把它转化成reflect对象(reflect.Type或者reflect.Value，根据不同的情况调用不同的函数)。 12t := reflect.TypeOf(i) //得到类型的元数据,通过t我们能获取类型定义里面的所有元素v := reflect.ValueOf(i) //得到实际的值，通过v我们获取存储在里面的值，还可以去改变值 转化为reflect对象之后我们就可以进行一些操作了，也就是将reflect对象转化成相应的值，例如 12tag := t.Elem().Field(0).Tag //获取定义在struct里面的标签name := v.Elem().Field(0).String() //获取存储在第一个字段里面的值 获取反射值能返回相应的类型和数值 12345var x float64 = 3.4v := reflect.ValueOf(x)fmt.Println("type:", v.Type())fmt.Println("kind is float64:", v.Kind() == reflect.Float64)fmt.Println("value:", v.Float()) 最后，反射的字段必须是可修改的。如果下面这样写，会error 123var x float64 = 3.4v := reflect.ValueOf(x)v.SetFloat(7.1) 如果要修改相应的值，必须这样写 1234var x float64 = 3.4p := reflect.ValueOf(&amp;x)v := p.Elem()v.SetFloat(7.1)]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>GAP</tag>
        <tag>Tips</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[21Projects-01MNIST]]></title>
    <url>%2Fpost%2Fadbc.html</url>
    <content type="text"><![CDATA[有关MNIST数据集上进行softmax-交叉熵和简易CNN两种方法的知识点小整理。 占位符1x = tf.placeholder(tf.float32, [None, 784]) tf的占位符，用于传入外部数据 placeholder(dtype, shape=None, name=None): :param dtype: 数据类型:param shape: 数据维度，None表无限制:param name: 名称:return: Tensor类型 例子：TensorFlow中加载图片的维度为[batch, height, width, channels] 故placeholder的shape可写为[None, None, None, 3] 变量1W = tf.Variable(tf.zeros([784, 10])) tf变量需要初始值，一旦初始值确定，那么该变量的类型和形状就基本确定了 Variable(initial_value=None, trainable=True, validate_shape=True, name=None): :param initial_value:初始值，可以搭配tensorflow随机生成函数:param trainable:默认该变量可被算法优化，不想该变量被优化，改为False:param validate_shape:默认形状不接受更改，如需更改，改为False:param name:给变量确定名称:return: Tensor类型 会话1sess = tf.InteractiveSession() 对上述结点进行计算的上下文，变量的值会保存在会话中。 softmax即归一化指数函数，将多分类中各个类别的评价分数转换为和为1的分布概率 for j = 1, …, K. 交叉熵信息量一件事发生的可能性越小，其发生所带来的信息量越大；反之，一件发生概率为1的事发生，对我们来说毫无价值，获取到的信息量为0。 假设X是一个离散型随机变量，定义事件X=x0的信息量为：I(x0)=−log(p(x0))，其中p(x0)取值为[0, 1] 熵对于某个事件，熵反映了其所有可能性所带来的信息量，是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。 熵用来表示所有信息量的期望H(X)=−∑p(xi)*log(p(xi)) 相对熵即KL散度，如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量这两个分布的差异，即如果用P来描述目标问题，而不是用Q来描述目标问题，得到的信息增量。 在机器学习中，P表示样本真实分布，比如[1,0,0]（样本属于第一类）Q表示模型预测分布，比如[0.7,0.2,0.1] 对于离散随机变量，其概率分布P 和 Q的KL散度可按下式定义为D~KL~(P|Q) = ∑~i~P(x~i~)log(p(x~i~)/q(x~i~))其中D~KL~的值越小，表示q分布和p分布越接近。 交叉熵相对熵中D~KL~(P|Q) = ∑~i~P(x~i~)log[p(x~i~)/q(x~i~)] = ∑~i~P(x~i~)log(p(x~i~)) - ∑~i~P(x~i~)log(q(x~i~)) = -H(p(x)) + [-∑~i~P(x~i~)*log(q(x~i~))] 其中真实样本熵固定，所以直接用-∑~i~P(x~i~)*log(q(x~i~))作为优化指标（交叉熵）即可，即交叉熵越小越好。 交叉熵VS均方根误差做分类问题时，为什么我们不用RSME而是交叉熵作损失函数呢？ 我们希望损失函数能做到，当预测值跟目标值越远时，修改参数后能减去一个更大的值，做到更加快速的下降。 函数更不容易陷入局部最优解。 在做后向传播时，会出现（基于一个样本的情况，对于softmax下w，b偏导进行计算）： RSME在更新w，b时候，w,b的梯度跟激活函数的梯度成正比，激活函数梯度越大，w,b调整就越快，训练收敛就越快，但是Simoid函数在值非常高时候，梯度是很小的，比较平缓。 交叉熵在更新w,b时候，w,b的梯度跟激活函数的梯度没有关系了，bz已经表抵消掉了，其中bz-y表示的是预测值跟实际值差距，如果差距越大，那么w,b调整就越快，收敛就越快。 均方根误差VS平均绝对误差而在做回归问题时，我们的选择又该是什么样呢？ RSME：若出现误差较大的点，RSME将被调整以最小化这个离群数据点，但却是以牺牲其他正常数据点的预测效果为代价，这最终会降低模型的整体性能。 MAE：若出现误差较大的点，最小化MAE的预测为所有目标值的中位数。我们知道中位数对于离群点比平均值更鲁棒，这使得MAE比MSE更加鲁棒。但使用MAE损失（特别是对于神经网络）的一个大问题是它的梯度始终是相同的，这意味着即使对于小的损失值，其梯度也是大的。 Log-Cosh LossLog-cosh是用于回归任务的另一种损失函数，它比之前两种更加平滑。顾名思义，它采用 ∑~i~log(cosh(y~i~^p^-y~i~))作预测误差。Log-cosh Loss对于小的x来说，其大约等于 (x 2) / 2，而对于大的x来说，其大约等于 abs(x) - log(2)。这意味着logcosh的作用大部分与均方误差一样，但不会受到偶尔出现的极端不正确预测的强烈影响，且二阶可导**。 CNN1x_image = tf.reshape(x, [-1, 28, 28, 1]) 因为cnn需要在图片的像素矩阵上进行池化等操作，所以需要将原来的784*1向量转成28*28的矩阵（[-1, 28, 28, 1]中的-1形状的第一维大小是根据x自动确定的） tf.nn.conv2d()12def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None) input：需要做卷积的输入图像(Tensor=[batch, in_height, in_width, in_channels])即[训练时一个batch的图片数量, 图片高, 图片宽, 图像通道数]，该Tensor要求类型为float32或float64 filter：CNN卷积核(Tensor=[filter_height, filter_width, in_channels, out_channels])即[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维 strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4 padding：只能是”SAME”,”VALID”其中之一，这个值决定了不同的卷积方式 use_cudnn_on_gpu：bool类型，是否使用cudnn加速，默认为true return：Tensor，就是我们常说的feature map tf.nn.max_pool()12def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') tf.nn.max_pool(value, ksize, strides, padding, name=None) value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape ksize：池化窗口的大小，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1 strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1] padding：和卷积类似，可以取’VALID’ 或者’SAME’ return：Tensor=[batch, height, width, channels]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>大三</tag>
        <tag>机器学习</tag>
        <tag>读书笔记</tag>
        <tag>21Projects</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy爬虫框架]]></title>
    <url>%2Fpost%2Fb2a.html</url>
    <content type="text"><![CDATA[Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 以下整理于自己业余写的简单爬虫 theGuardianNews路过朋友有兴趣可以看看。 创建项目创建新Scrapy项目，在存储代码的目录下git bashscrapy startproject news 这将创建一个名为news的目录，文件tree如下： news│ scrapy.cfg&emsp; &emsp; &emsp; # deploy configuration file└─news&emsp; &emsp; &emsp; &emsp; &emsp; # project Python module, import your code from here&emsp; │ items.py&emsp; &emsp; &emsp; # project items definition file&emsp; │ middlewares.py&emsp; # project middlewares file&emsp; │ pipelines.py&emsp; &emsp; # project pipelines file&emsp; │ settings.py&emsp; &emsp; # project settings file&emsp; │ __init__.py&emsp; └─spiders&emsp; &emsp; &emsp; # a directory where you will later put your spiders&emsp; &emsp; │ __init__.py 编写Spiders定义Spider类用来从网站中提取信息。 必须子类化 scrapy.Spider并定义要生成的初始请求 可选地如何跟踪页面中的链接 解析下载的页面内容以提取数据 在news目录下cmd输入scrapy genspider news theguardian.com 12345678910111213import scrapyclass news(scrapy.Spider): name = "news" start_urls = [ 'https://www.theguardian.com/', ] '''start_urls将默认执行yield scrapy.Request故可省略以下： for url in urls: yield scrapy.Request(url=url, callback=self.parse) ''' def parse(self, response): pass parse()方法通常解析响应，将抽取的数据提取为dicts，并查找要遵循的新URL并Request从中创建新的request() 提取数据强烈安利用scrapy shell url的方式先在shell里试试，这样不会运行几次就被反爬。 ::text表示抽取标签内字符串，::attr(title)表示抽取title属性内容 extract()为一个包含数据串的list，extract_first()为list的第一个值 123456urls = response.css('a[class="fc-item__link"]').css('a[data-link-name="article"]').xpath('@href').extract()title = response.css('h1[class ="content__headline "]').css('h1[itemprop="headline"]::text').extract()time = response.css('time[itemprop = "datePublished"]::text').extract_first()category = response.css('a[class ="subnav-link subnav-link--current-section"]::text').extract_first()tags = response.css('a[class = "submeta__link"]::text').extract()content = response.css('div[itemprop = "articleBody"]').css('p::text').extract() 创建Item打开news文件夹下的items.py创建类NewsItem 123456class NewsItem(scrapy.Item): title = scrapy.Field() time = scrapy.Field() category = scrapy.Field() tags = scrapy.Field() content = scrapy.Field() 设置PipelineItem pipeline组件有两个典型作用：1. 查重丢弃 ；2. 保存数据到文件或数据库中。 假设我们采用本地json存取： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546from scrapy.exceptions import DropItemimport jsonimport codecsclass NewsPipeline(object): def process_item(self, item, spider): title = item['title'] if title: title_str = ''.join(title) item['title'] = title_str.replace('\n', '') else: // 如果title为空则跳过这个item raise DropItem('Information was missing') time = item['time'] if time: item['time'] = time.replace('\n', '') else: raise DropItem('Information was missing') category = item['category'] if category: item['category'] = category.replace('\n', '') else: raise DropItem('Information was missing') tags = item['tags'] if tags: tags_str = ','.join(tags) item['tags'] = tags_str.replace('\n', '') else: raise DropItem('Information was missing') content = item['content'] if content: content_str = ''.join(content) item['content'] = content_str.replace('\n', '') else: raise DropItem('Information was missing') class JsonWriterPipeline(object): def __init__(self): self.file = codecs.open('data_utf8.json', 'w', encoding='utf-8') // 如果是中文需要utf8的话，import codecs def process_item(self, item, spider): line = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(line) return item def spider_closed(self, spider): self.file.close() 之后别忘了在setting中改一下ITEM_PIPELINES的设置： 1234ITEM_PIPELINES = &#123; 'news.pipelines.NewsPipeline': 300, 'news.pipelines.JsonWriterPipeline': 800,&#125; 假设我们采用MongoDB，打开news文件夹下的pipelines.py创建类NewsPipline 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from scrapy.exceptions import DropItemimport pymongoimport codecsimport loggingclass NewsPipeline(object): def process_item(self, item, spider): title = item['title'] if title: title_str = ''.join(title) item['title'] = title_str.replace('\n', '') else: // 如果title为空则跳过这个item raise DropItem('Information was missing') time = item['time'] if time: item['time'] = time.replace('\n', '') else: raise DropItem('Information was missing') category = item['category'] if category: item['category'] = category.replace('\n', '') else: raise DropItem('Information was missing') tags = item['tags'] if tags: tags_str = ','.join(tags) item['tags'] = tags_str.replace('\n', '') else: raise DropItem('Information was missing') content = item['content'] if content: content_str = ''.join(content) item['content'] = content_str.replace('\n', '') else: raise DropItem('Information was missing') class MongoPipeline(object): def __init__(self): # 链接数据库 client = pymongo.MongoClient(host=settings['MONGO_HOST'], port=settings['MONGO_PORT']) self.db = client[settings['MONGO_DB']] # 获得数据库的句柄 self.coll = self.db[settings['MONGO_COLL']] # 获得collection的句柄 # 数据库登录需要帐号密码的话 # self.db.authenticate(settings['MONGO_USER'], settings['MONGO_PSW']) def process_item(self, item, spider): postItem = dict(item) # 把item转化成字典形式 self.coll.insert(postItem) # 向数据库插入一条记录 return item # 会在控制台输出原item数据，可以选择不写 同样我们需要在setting中改一下ITEM_PIPELINES的设置： 12345678910ITEM_PIPELINES = &#123; 'news.pipelines.NewsPipeline': 300, 'news.pipelines.MongoPipeline': 800,&#125;MONGO_HOST = "127.0.0.1" # 主机IPMONGO_PORT = 27017 # 端口号MONGO_DB = news_tutorial" # 库名MONGO_COLL = "news_items" # collection名# MONGO_USER = "simple" #用户名# MONGO_PSW = "test" #用户密码 修改Spider12345678910from news.items import NewsItemdef get_news(self, response): item = NewsItem() ......# 提取数据的过程 item['title'] = title item['time'] = time item['category'] = category item['tags'] = tags item['content'] = content yield item 修改完的spider如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import scrapyfrom scrapy.http import Requestfrom news.items import NewsItemclass HeadSpider(scrapy.Spider): name = 'news' allowed_domains = ['theguardian.com'] base_url = 'https://www.theguardian.com/' start_urls = [] topics = ['world', 'science', 'cities', 'global-development', 'uk/sport', 'uk/technology', 'uk/business', 'uk/environment', 'uk/culture'] years = [2019] months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'] dates = range(1, 31) for topic in topics: for y in years: for m in months: for d in dates: url = base_url + topic + '/' + str(y) + '/' + m + '/' + '%02d' % d + '/' + 'all' start_urls.append(url) # 类似于 https://www.theguardian.com/uk/sport/2019/apr/01/all def parse(self, response): urls = response.css('a[class="fc-item__link"]').css('a[data-link-name="article"]').xpath('@href').extract() for url in urls: # 每个https://www.theguardian.com/uk/sport/2019/apr/01/all页面上的news连接 yield Request(url, self.get_news) def get_news(self, response): item = NewsItem() title = response.css('h1[class ="content__headline "]').css('h1[itemprop="headline"]::text').extract() if(len(title)==0): # 卫报的news页面有两种形式，如果第一种没抓到，就用模式二 title = response.css('meta[itemprop="description"]').xpath('@content').extract() time = response.css('time[itemprop = "datePublished"]::text').extract_first() category = response.css('a[class ="subnav-link subnav-link--current-section"]::text').extract_first() tags = response.css('a[class = "submeta__link"]::text').extract() content = response.css('div[itemprop = "articleBody"]').css('p::text').extract() if (len(content) == 0): content = title # 有些news内容可能时视频或者其它非文本模式，这时我们用title作为content item['title'] = title item['time'] = time item['category'] = category item['tags'] = tags item['content'] = content yield item 修改设置打开news文件夹下的settings.py可以修改一些小细节便于我们爬取数据。 TIPS：加上FEED_EXPORT_ENCODING = &#39;utf-8&#39;适用于爬取中文内容哦。 12345678910111213FEED_EXPORT_ENCODING = 'utf-8' # 修改编码为utf-8DOWNLOAD_TIMEOUT = 10 # 下载超时设定，超过10秒没响应则放弃当前URLITEM_PIPELINES = &#123; 'news.pipelines.NewsPipeline': 300, // 这个没改，参照pipe看个人选择&#125;CONCURRENT_REQUESTS = 32 # 最大并发请求数（默认16DOWNLOAD_DELAY = 0.01 # 增加爬取延迟，降低被爬网站服务器压力DEFAULT_REQUEST_HEADERS = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'zh-CN,zh;q=0.8', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36'&#125; 针对反爬去奥斯汀访学（旅游）时候写的yelp爬虫 yelpReview路过朋友有兴趣可以看看。 使用user agent池，轮流选择之一来作为user agent；池中包含常见的浏览器的user agent。 禁止cookies(参考 COOKIES_ENABLED)，有些站点会使用cookies来发现爬虫的轨迹。 设置下载延迟(2或更高)。参考 DOWNLOAD_DELAY 设置。 如果可行，使用 Google cache 来爬取数据，而不是直接访问站点。 使用IP池。例如免费的 Tor项目 或付费服务(ProxyMesh)。 使用分布式下载器(downloader)来绕过禁止(ban)，就只需要专注分析处理页面。eg: Crawlera Mongodb创建userAdminAnyDatabase角色，用来管理用户，可以通过这个角色来创建、删除用户。 123456789101112131415161718&gt; use adminswitched to db admin&gt; db.createUser(... &#123;... user: "userAdmin",//用户名... pwd: "123",//密码... roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ]... &#125;... )Successfully added user: &#123; "user" : "dba", "roles" : [ &#123; "role" : "userAdminAnyDatabase", "db" : "admin" &#125; ]&#125; 以上是我怕忘了自己设置过的user和pwd各位游客可以不用在意（小声） 打开安装mongoDB环境的命令行，比如我之前是在Windows下安装的，故打开cmd输入mongo会有如下输出： 123MongoDB shell version v3.4.10-58-g9847179connecting to: mongodb://127.0.0.1:27017MongoDB server version: 3.4.10-58-g9847179 然后我个人安利下可视化工具adminMongo下载轻松简单（主要是界面好看清爽） 打开adminMongo目录，npm start会有如下输入，根据提示打开http://localhost:1234即可 1234&gt; admin-mongo@0.0.23 start D:\adminMongo&gt; node app.jsadminMongo listening on host: http://localhost:1234 打开页面后，connection名字仅供参考（随便设）MongoDB连接字符串的格式可以是：mongodb://&lt;user&gt;:&lt;password&gt;@127.0.0.1:&lt;port&gt;/&lt;db&gt;指定级别是可选的，一般mongodb://:@127.0.0.1:27017即可。有关MongoDB连接字符串的更多信息，请参阅正式的MongoDB文档。 写入依赖打开项目所在文件目录，然后打开命令行输入即可得到requirements.txt 1pipreqs ./ --encoding=utf-8 这样之后有人需要用这个项目的时候直接打开命令行输入以下即可： 1pip install -r requirements.txt]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>大二</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件安全设计]]></title>
    <url>%2Fpost%2F811e.html</url>
    <content type="text"><![CDATA[这是一门针对安全过程模型、基于风险测试、威胁分析报告等谈漏洞讲威胁的学科。 软件与软件安全信息安全与软件安全信息安全基本属性 安全性、可用性、保密性、可控性、可靠性 恰当的信息安全定义：信息具有保密性，完整性，可用性 软件安全 软件安全 ：软件在恶意攻击下能够正确地完成其功能。 软件安全性 ：软件不被恶意使用或者攻击进而造成用户信息资产损失的属性。包括：（1） 可信性：保护敏感信息不被未授权用户访问（2） 完整性：保护数据不被更改或破坏（3） 可用性：确保资源被授权用户的使用 软件安全保护 ：软件的完整性、可用性、可信性。（1） 自身安全：防止软件丢失、被破坏、被篡改、被伪造（2） 存储安全：可靠存储，保密存储，压缩存储，备份存储（3） 通信安全：安全传输、加密传输、网络安全下载、完整下载（4） 使用安全：防止非法用户授权访问，软件滥用，软件窃取，非法复制（5） 运行安全：确保软件正常运行，功能正常 软件安全研究 ：如何设计、构造、验证和维护软件以保证其是安全的。——改进和实现软件安全的 架构、工具、方法 开放系统互连安全的体系架构ISO7498-25种 安全服务 ：鉴别服务、访问控制、数据完整性、数据保密性、抗抵赖8种 安全机制 ：加密、数字签名、访问控制、数据完整性、数据交换、业务流填充、路由控制、公证5种 安全管理机制 ：可信功能度、安全标记、事件检测、安全审计跟踪、安全恢复 软件安全重要概念漏洞 安全漏洞 ：可能被入侵者恶意利用的属性，也称脆弱性。漏洞是软件的属性。 漏洞本质 ：通过已授权的手段获取对资源的未经授权访问，或对系统造成损害。 安全事件 ：当系统的某个漏洞被入侵者渗透而造成泄密时，其结果称一次安全事件。 脆弱状态 ：从已授权的状态变换到未授权状态。 攻击 ：以授权状态或脆弱状态开始，以受损状态为目标的状态变换。 ISO9126质量特性1.功能性 准确性：功能精确度 互操作性：与其它系统交互 保密安全性：主要是权限和密码2.可靠性 成熟性：对内错误的隔离 容错性：对外错误的隔离 易恢复性：系统失效后的重新恢复3.易用性 易理解性；易学性；易操作性4.效率 时间特性：响应时间；资源利用性：所耗系统资源5.维护性 易分析性 易改变性：降低修复问题的成本 稳定性：避免由于软件修改而造成意外结果 易测试性6.可移植性 适应性：无需变动就能适应不同环境 易安装性 共存性：公共环境中与其它软件分享公共资源 易替换性：同样环境下，替代相同用途的产品 缺陷：安全性是软件功能性的子属性-&gt;对安全的忽视 安全的代码 ：能够抵抗恶意攻击的代码；安全的代码同时也是健壮的代码 安全性代码 ：实现安全功能的代码 安全的程序 ：安全隐含一定程度信任，实现了期望的机密性、完整性、可用性及其功能 典型软件安全问题安全问题来源 攻击者 软件存在的攻击路径－攻击面问题 漏洞 漏洞是软件的属性，也是软件安全威胁的根源。产生原因：（1） 软件或协议设计时的瑕疵（2） 软件或协议实现中的弱点（3） 软件本身的瑕疵——良好的、编写安全程序的的编程习惯。（4） 系统和网络的错误配置可以分为：（1） 设计漏洞：设计错误，往往发现于软件的安全功能特性中。（2） 实现漏洞：实际编码中的安全缺陷。意外行为：即程序安全缺陷，由于程序脆弱性引起的不适当程序行为。分有意的和无意的两种（废话）常见缺陷：缓冲区溢出、未校验输入、 资源竞争、访问控制问题、认证、授权、加密缺陷。 安全设计问题 密码技术使用的败笔（1）创建自己的密码技术or选用不当的密码技术（2）依赖隐蔽式安全（3）编写到程序中的密钥（4）错误地处理私密信息 对用户及其许可权限进行跟踪的薄弱或缺失会话管理、身份鉴别、授权的薄弱或缺失 有缺陷的输入验证（1）未在安全的上下文环境中执行验证：如在服务器验证而未在客户端验证（2）验证例程不集中：验证应尽可能靠近用户输入and验证应集中以便核实（3）不安全的组件边界 薄弱的结构性安全（1）过大的攻击面（2）在过高权限级别上运行进程（3）没有纵深防御（4）失效时的处理不安全 其他（1）代码和数据混在一起（2）错将信任寄予外部系统（3）不安全的默认值（4）未做审计日志 C/C++安全问题问题1：没有安全的本地字符串类型and没有安全易用的字符串处理函数以NULL终止符表示一个字符串的末尾 -&gt; 容易导致缓冲区溢出问题2：缓冲区超限覆盖栈中的函数返回地址栈溢出：函数返回地址在栈中位置紧接本地变量后，该变量缓冲区溢出会覆盖栈中返回地址。预防：精确控制输入变量长度；避免使用无界字符串；使用字符串缓冲区模块。问题3：printf类型的格式化函数－格式化字符串攻击sprintf(target, “Name: %s%s%s%s”);从栈中读取四个字符串，但事实上栈中不存在这四个字符串，程序读取栈中原本用于其他目的的值。问题4：整数溢出在C语言中，整数的正负标识是默认的；C语言没有任何措施预防整数溢出。 平台实现安全问题平台：指程序在其中所运行的环境，包括OS及其交互组件。威胁多来自交互和OS平台问题1：符号链接符号链接等效于其指向的文件，即攻击者可以借此启动系统中任何程序。权限——必须检查该文件的符号链接，不可以基于文件名做任何安全方面的判定平台问题2：目录遍历CIFS文件共享协议，允许计算机通过网络访问彼此文件系统。攻击者可通过使用“..”向上追溯平台问题3：字符转换平台进行升级的时候可能会引入新的字符编码，可能会产生意料外的结果 应用程序安全问题引起原因：某个组件的恶意数据在其另一个组件中被当作了合法代码；对涉密信息的不当处理问题1：SQL注入在连接到数据库的应用程序上执行自己所构造的查询。输入用户名’or’1=1，数据库里执行Select * from sys_user where username=’’or’1=1’成功入侵问题2：跨站点执行脚本来自非受信环境的攻击者可在受信环境注入数据，使其在受信环境作为脚本予以执行。类似于我写了一个脚本在你电教务系统上跑，你以为是你电新功能就输了你的学号密码收到了学号密码的我就能为所欲为为所欲为跨站点执行脚本还可以用来访问数据，如用户cookies。&lt;input type=&quot;text&quot; name=&quot;address1&quot; value=&quot;value1from&quot;&gt;value后面的值是来自用户的输入，如果用户输入&quot;/&gt;&lt;script&gt;alert(document.cookie)&lt;/script&gt;&lt;!-那么就会变成&lt;input type=&quot;text&quot; name=&quot;address1&quot; value=&quot;&quot;/&gt;&lt;script&gt;alert(document.cookie)&lt;/script&gt;&lt;!- &quot;&gt;嵌入的JavaScript代码将会被执行。 开发过程问题安全需求和前提条件的文档记录缺乏交流和文档匮乏缺少安全过程部署上的薄弱性：执行部署的一般不属于开发团队，很可能会扩大权限 OWASPThe Open Web Application Security ProjectA1-注入：发送的恶意数据可以欺骗解释器，执行计划外的命令或者在未被恰当授权时访问数据。A2-失效的身份认证A3-跨站脚本A4-不安全的直接对象引用A5-安全配置错误A6-敏感信息泄漏A7-功能级访问控制缺失A8-跨站请求伪造：迫使用户浏览器向存在漏洞的web应用程序发送请求A9-使用含有已知漏洞的组件A10-未验证的重定向和转发 安全软件工程SSE-CMM在CMM模型的基础上，通过对安全工程进行管理的途径将系统安全工程转变为一个具有良好定义的、成熟的、可测量的先进工程学科。发起者：国防部&amp;国家安全局 开发目的降低开发和维护系统的花费；提高工程进度和预算的一致性；选择合适的承包者。 主要内容能力方面： 通用设施（增强执行过程能力的实现和制度化实施）-&gt; 公共特征（一组实施，管理和制度化过程的相同点 ）-&gt; 能力级别（共同工作的一组公共特征，主要增强执行一个过程的能力 ）能力级别１――非正式执行执行基本实施能力级别２――计划与跟踪计划、规范化、验证、跟踪执行能力级别３――充分定义定义标准过程、执行已定义的过程、协调安全实施能力级别４――定量控制建立可测的质量目标、客观地管理过程的执行能力级别５――连续改进改进组织能力、改进过程的有效性 域方面：基础设施（工程和安全实施是安全工程过程中必须存在的性质，指出特殊过程区的目的并属于该过程区 ）-&gt; 过程区（每个过程区（PA）是一组相关安全工程过程的性质，当这些性质全部实施后则能达到过程区定义的目的）-&gt; 过程类（一组过程区指出活动的同一通用区） 关于安全工程与评估安全工程分三个基本过程：风险、工程、保证风险过程：确定产品或者系统的危险性，并对这些危险性进行优先级排序-风险信息工程过程：针对危险性，安全工程过程与相关过程一起确定并实施解决方案-产品/服务保证过程：建立起对解决方案的信任，并把这种信任传达给顾客-保证论据 SSAM（SSE-CMM评定方法）——决定实施安全工程过程的能力；——定义了安全工程环境便于评定；——评定时巧妙使用了SSE-CMM体系结构中的两个方面。 SDL安全开发生命周期模型微软可信计算努力的一个组成部分基于并行理念的标准软件开发过程基于威胁建模和测试 SDL概览SDL从三个方面考虑软件安全的保障：设计安全: 保护软件自身及其处理的信息并抵御攻击，应从架构、设计和实现上考虑安全缺省安全: 设计者应假定安全缺陷将会出现。为了使被攻击时损害降到最小，软件缺省状态应保证安全。比如，最小特权原则。提交安全: 工具和指南应随软件一起提供以帮助用户安全使用。软件更新应易提交。 SDL过程 教育和意识 安全设计基础：受攻击面分析、深度防御、最小特权、安全默认配置 威胁建模：设计威胁建模、编码威胁建模、测试威胁建模 项目启动：SDL覆盖应用，安全顾问及领导团队，BUG标准、BUG追踪中含安全、隐私类BUG 最佳设计阶段： 常见安全设计原则，如最小特权，权限分离、最少公共机制等 受攻击面分析：枚举所有接口、协议以及可执行代码的过程。 受攻击面降低：代码中一定存在漏洞，部分严重漏洞用户不得不去妥协。唯一解决的方法是将代码的利用率降至为零。 受攻击面降低的方法：综合考虑完美的安全与无法规避的风险，尽可能减少未经信任的用户可能接触到的代码比例： （1）降低默认执行的代码量 （2）限制可访问到代码的人员范围 （3）限定可访问到代码的人员身份 （4）降低代码所需权限。 产品风险评估： 安全风险评估：安装、受攻击、移动代码（ActiveX） 隐私影响分级：分级1：存储或转发个人信息，儿童相关，不间断监控，安装新软件or改变文件类型的关联（改变IPEG解码）。分级2：传输匿名数据 。分级3：其余。 统一各种因素：确定安全与隐私风险后必须在日程中排出响应时间，减少客户风险。 风险分析/威胁建模： 优点： （1）有助于整个风险管理过程 （2）进入编码阶段前发现系统威胁 （3）通过威胁建模重新验证其架构与设计 （4）进一步明确针对应用及环境采取相应的解决对策 （5）指导整个代码审核过程和渗透测试过程 威胁建模过程： （1）定义应用场景、收集外部依赖列表 （2）定义安全假设、创建外部安全备注 （3）绘制待建模应用的一个或多个数据流图 （4）确定威胁类型 （5）识别系统威胁、判断风险并规划消减措施 创建安全文档、工具：安装、使用、帮助、开发文档；禁止端口和不必要服务etc. 安全编码策略：使用最新版本编译器及其内置防御特性，使用源代码分析工具 安全测试策略：模糊测试、渗透测试、运行时测试、 重审威胁模型、重估受攻击模型 最后五个阶段 安全推进活动：代码评审、安全测试、更新威胁模型 最终安全评审：威胁模型评审、未修复安全BUG评审、工具使用有效性验证 安全响应规划：建立安全的响应过程 产品发布阶段：用户验收，确认SDL过程被正确执行 安全响应执行：遵从计划尽可能补救 SDL缺陷SDL过程不能保证生产出绝对安全的软件：（1）开发团队一定会出错（2）新漏洞一定会变化（3）规则一定会变化 软件安全测试安全测试安全性测试：验证应用程序安全等级and识别潜在安全性缺陷。安全指标不同测试策略不同。目的：查找软件程序设计中的安全隐患and检查应用程序对非法侵入的防范能力。法律问题：安全测试及工具应用必须得到授权；未明确授权下，不允许针对第三方系统进行穿透测试实验 安全测试方法 静态测试：对源码进行安全扫描，与特有软件安全规则库匹对，找出潜在安全漏洞。 编码阶段使用，适用早期代码开发阶段，而不是测试阶段。 动态测试：即渗透测试使用自动化工具或人工方法模拟黑客输入进行攻击性测试，找出运行时存在的安全漏洞。 真实有效，问题一般正确且严重。缺点：测试数据只能达有限的测试点，覆盖率低。 程序数据扫描：进行内存测试，发现许多诸如缓冲区溢出的漏洞，这类漏洞使用其他测试手段都难以发现。 内容（测试点）：程序安全性测试、数据安全性测试程序安全测试：用户权限、用户冲突、密码保护网络安全测试：防护配置（系统补丁）、模拟非授权共计、检查漏洞、木马、外挂数据安全测试：系统数据的机密性、完整性、管理性、独立性、可备份和恢复能力 安全的常规测试方法基于风险的安全测试安全测试目标：给定的时间和资源不变的情况下，尽可能多地找出最严重的安全缺陷。威胁建模＝风险建模 信息搜集目的：熟悉程序的设计、了解程序访问入口点位置和需要保护的信息（1）评审程序设计文档（2）与设计人员和架构师会谈了解组件框架、组件间主要数据流、程序外数据流（非受信数据，攻击性输入）（3）在运行时用调试和诊断程序分析运行时分析应用程序痕迹：网络端口、文件、注册表键值 威胁建模目的：排定测试优先级，找出测试区域，发现系统弱点 识别威胁路径目的：识别应用程序级别最高的风险领域，确定相应的保护措施（1）了解应用程序平台和编程语言的整体强度（2）确定用户的访问类别（3）建立并分析数据流图 识别威胁目的：深入识别威胁路径的处理，理清与处理相关的每一种威胁。该组件执行什么样的处理、如何确定身份、是否信任数据或其他组件、修改了什么数据、有何外部连接在一个威胁路径上的9个高风险活动：（1）数据解析、处理私密数据（2）文件访问、数据库访问、网络访问（3）生成子进程、同步或会话管理（4）身份鉴别、授权 识别漏洞目的：找出可能存在于组件中的实际漏洞。对漏洞的可能缓解措施：（1）数据验证测试（2）资源监视（3）关键功能的访问控制搜寻漏洞的方法及途径：（1）安全设计审查（2）安全代码审查（3）安全测试 风险分级——DREAD模型 判定可利用性目的：判断漏洞是否可被攻击者利用。基本原则：开发中直接修补一个漏洞比浪费时间判定其是否会被利用容易。 白盒、黑盒和灰盒测试 白盒测试可以看作是内部的攻击。测试人员可以访问源代码和设计文档，可以进行威胁建模或逐行的代码检查。白盒测试是找出漏洞最为有效的方法。 黑盒测试是白盒测试的补充。以局外人身份攻击系统，使用工具检查系统的攻击面，并探查系统的内部信息。方向工程团队利用黑盒测试验证隐蔽式安全方法的强度。 组合使用白盒测和黑盒测试。白盒测试用于发现在设计和开发中详细说明的功能的缺陷；黑盒测试在无法了解程序内部信息的时候找出缺陷。程序开发中的调试运行是典型的灰盒测试方法。 安全漏洞分级DREAD模型：进行威胁程度级别分析的有效技术（1）潜在的破坏：如果该漏洞被利用，所产生的破坏程度（2）再现性：探测并利用该漏洞所需要的努力要多久（3）可利用性：是否需要身份鉴别或者特殊的知识（4）受影响的用户：漏洞利用的影响面有多大（5）可发现性：漏洞研究人员或黑客找出该漏洞的可能性基于可利用性提出TRAP模型：（1）时间：某些漏洞可能需要长时间的探测才能利用。比如解个密要算一千年，那风险肯定很低。（2）可靠性/再现性：漏洞的严重程度依赖于可被攻击者利用的程度。通常高级别漏洞的可靠性和可再现性高。（3）访问：利用漏洞通常可以为攻击者提供更高的访问权。（4）定位：攻击者必须要能与存在漏洞的应用程序交互，并访问到含该漏洞的代码。 编写安全的代码SD3Secure by Design, Default, Deployment 安全设计（1）安排具体的安全设计人员；进行安全教育；（2）确保威胁分析已经完成；符合安全设计和编码的指导原则；（3）尽可能修补安全编程指南上BUG；确保安全指南是逐步改进的；（4）回归测试已修复缺陷；简化代码和安全模型；打包前完成穿透测试。 缺省安全（1）缺省状态下，不要设置所有的特点和功能；（2）允许最小权限；恰当的资源保护。 安全提交（1）确认程序给管理员提供了安全功能；尽可能提供高质量补丁；（2）提供足够信息使用户安全的使用软件。 安全规则（1）学习错误；最小化攻击面；（2）使用深度防御；应用缺省安全；（3）使用最小权限；基于错误计划；（4）记住兼容性的倒退是痛苦的；（5）假定外部系统是不安全的；（6）切记安全的特性不等于安全特性；（7）不要混合编码和数据；正确修复安全问题。 学习错误学习错误从填写一个文档开始，文档内容：产品名称；产品版本；联系人；BUG数据库编号；脆弱性描述；脆弱性的隐含意义；在产品的缺省安装中，这个问题是否存在？设计，开发和测试人员能够做什么来防止这个缺陷？修复的细节，包括代码的区别，如果可以填写。 最小化攻击面需要计算下面的内容：（1）打开socket、命名管道、RPC端点的数量；（2）服务的数量；缺省运行服务的数量；服务以提高权限运行的数量；（3）ISAPI过滤器和应用的数量；动态WEB页面数量；加入管理员组帐号的数量；（4）文件，目录和注册键值的数量，带有弱访问控制列表。 一个信息系统的安全模型分析信息系统MIS及其特征借助自动化数据处理手段进行管理的系统由计算机硬件、软件（系统软件、应用软件、管理学软件包）、数据库规程和人共同组成。 由人、计算机等组成的能进行管理信息的收集、传递、加工的信息系统。主要特征：（1）一定是依赖于计算机的；（2）涉及了计算机的软件和硬件；（3）实现数据的采集、传递、加工、处理功能。主要特性：（1）整体性：系统各部分一定以整体目标为目标，追求全局最优；（2）目的性：一个系统一定具有明确目的标，并完成一定的功能；（3）层次性：一个系统可分为若干层次和子系统；（4）边界性：每个系统明显区别于其他系统，系统间有明确界限；（5）关联性：系统包括若干元素，元素间存在一定关联性；（6）环境性：系统处于一定的环境之中并受环境影响。主要类型：宏观的国家经济信息系统；面向基层的企事业管理信息系统；事务型管理信息系统；办公型管理信息系统；专业型管理信息系统等；运行环境要素：（1）物理世界；（2）管理者实体：拥有授权管理、变更、修复和使用系统的人或其他系统其中一些被授权人可能缺乏有效管理系统的能力或具有恶意的目的；（3）使用者：在使用界面接受系统服务的实体；（4）提供者：在系统使用界面提供服务的实体；（5）基础组织：对系统提供信息源、通信链接、能源、冷气等特定服务的实体；（6）入侵者：企图超越其权限来变更或阻止服务，变更系统功能、性能或存取秘密信息的实体。 信息系统的安全问题ISO7498-2五种安全服务类型：（1）身份鉴别；（2）访问控制；（3）数据保密；（4）数据完整性；（5）抗抵赖。 安全模型系统的使用者划分为：用户、系统管理员、信息主管（或企业主管）系统划分为：用户界面逻辑、业务逻辑、异常检测机系统安全功能：访问控制；抗抵赖；数据保密；身份鉴别；授权机制；日志审计；系统异常探测。 用户界面逻辑用户界面逻辑部分：数据访问、登录控制。系统启动时，用户首先登录系统，通过验证后才能数据访问。登录控制主要功能：口令验证；口令修改；口令数据的加密；登录时间记录。登录控制设计考虑：（1）用户ID：从权限数据中提取相应用户名，回避重名，简化输入用户号本身也可以增加一定的安全性。（2）用户修改口令，而非系统管理员：管理员授权，但口令由用户输入加密存到后端避免系统管理员获取用户口令造成泄密。（3）初始口令的安全：系统初次运行关键——初始口令的赋予，由信息主管（或其他高层）完成用户身份确认要求用户初次登录时必须更改初始口令。（4）口令安全：口令长度限制、字符集限制、有效期限制。（5）用户封锁：用户多次登录失败后系统锁定用户操作，解锁必须由系统管理员完成。 业务逻辑业务逻辑部分：（1）数据服务：完成特定数据的加密、解密，日志数据的存储，权限及用户信息的存储（2）权限管理：完成用户的授权，包括两个部分：系统管理员和信息主管。系统管理员负责日常权限管理、日志审计、系统状态监控、异常监测、用户锁定处理。信息主管负责系统启动和初始授权。（3）日志审计：对用户的操作行为进行跟踪，提供根据时间、用户、系统的检索手段。日志不能人工清除，怕被管理员在后台清掉。log周期性自动清旧信息，保留最新信息。信息系统提高安全可信性的重要手段：合理并具有一定强度的日志设计。 异常探测机主要功能：分析日志、网络状态的安全监测、提供一定的日志文件保护机制异常检测独立于业务逻辑的目的：（1）独立程序便于进一步发展，有较大发展空间；（2）位于业务和用户进程之外，能对其进行监控；（3）不对信息系统运行发生干扰；（4）作为系统的可选件而非强制。 异常行为探测 区别内容 异常探测机 IDS 检测范围 网络内部行为 网络外部行为 实现方式 软件 硬件 与系统关系 可以存取系统数据 不能存取系统数据 保护目标 信息系统 网路 异常行为用户身份的攻击：针对用户的ID猜测用户身份。采用ID登录原因：方便输入&amp;安全性。猜测三次后用户被封锁。口令攻击：已知用户身份来猜测口令。在系统设计中，当猜测三次后则封锁用户。服务器异常访问：Sever PC和Web Sever等专用服务程序。类似局域网内DOS攻击。数据库异常连接：主要通过特定端口访问DB，威胁来自合法或非法的客户端程序。 日志分析针对日志文件自身的攻击（1）日志数据的删除：系统不向用户提供日志修改功能。合法用户不具有删除log的能力，只有超过log过期时间的日志数据才可删除。非法用户受到异常探测机、操作系统、数据库系统的安全机制约束。针对非法用户的攻击——如果是单独的log，当系统启动后，该文件置于异常探测机的保护之下；如果是数据库数据，则依赖于操作系统和数据库本身的保护机制。（2）日志数据的修改：系统不向用户提供日志修改功能。日志浏览需要授权，当系统管理员身份浏览日志时，该操作本身也将被写入日志。由于单独的日志文件置于异常探测机的保护之下，所以可以避免合法用户的修改。非法用户试图修改数据库中日志数据时，会受到操作系统和数据库系统的约束。 日志数据审计和异常模式（1）手动审计：提供必要的数据检索和查询的手段可根据用户、授权人、时间段、功能等进行检索，以进行必要的分析和事件跟踪。（2）自动审计和报警：根据日志数据，系统可自动完成的审计和报警功能用户锁定；口令超期；异常时间访问；异常功能访问；异常授权 网络异常探测机网络异常探测机：针对来自网络的信息进行分析，提供对信息系统的保护报警。探测器并不是通用的系统异常探测器。（1）数据流量检测；（2）服务器连接异常：连接数量异常、连接主机异常、连接端口异常。（3）文件访问限制；（4）日志文件保护。 WEB应用安全WEB APPLICATION采用HTTP协议完成通信与后台WEB Server实现交互与互联网服务器，包括Web Server，database server进行交互位于中间层，进行数据交互或其他服务程序 WEB应用安全现状：WEB应用安全实现非常困难WEB应用环境包括多个系统WEB应用大部分运行于INTERNET，具有更广的攻击面WEB应用运行中，具有更多的临时决策以支持系统的运行，系统状态具有更多的可变性许多支持系统没有得到恰当的保护 微软WEB应用安全框架WEB应用安全建模活动：Web 应用程序的威胁建模目的：确定方案中相关威胁和漏洞，以帮助构建应用程序的安全设计。输入： 主要用例和使用方案 、数据流 、数据架构 、部署关系图输出： 威胁列表 、漏洞列表 威胁建模：步骤 1 ：确定安全目标。业务需求、安全策略、兼容性要求 -&gt; 主要安全目标步骤 2 ：创建应用程序概述。列出重要特征和参与者有助于步骤 4 确定威胁。步骤 3 ：分解应用程序。部署关系图、用例、功能说明、数据流关系图 -&gt; 信任边界、入口点、出口点、数据流（在网络层，每个服务代表了一个入口点）步骤 4 ：确定威胁。使用步骤2&amp;3中的详细信息来确定相关威胁。步骤 5 ：确定漏洞。检查应用程序的各层以确定与威胁有关的弱点。使用漏洞类别来帮助关注最常出现错误的区域。 输入验证：在进行其他处理前如何筛选、删除或拒绝输入身份验证：一个实体验证另一个实体身份的过程，通过如用户名和密码的凭据进行授权：提供对资源和操作的访问控制的方式配置管理：处理运行身份、数据库连接、应用程序管理、设置保护敏感数据：处理必须受到保护的所有数据，不管数据在内存、网络还是永久性存储中会话：用户与Web 应用程序之间的一系列相关交互加密：应用程序保证机密性和完整性的方式参数操作：既指保护这些值不被篡改的方式，也指处理输入参数的方式审核与记录：记录与安全相关的事件的方式 一个WEB应用安全模型威胁建模：一种用于理解和消除系统安全威胁的形式化的方法（1）信息收集：定位文档、访问相关人员、探查系统（2）分析：用户、构件，资产，动机、入口、弱点和威胁（3）威胁消除：建立预算、排序处理、确立针对威胁的工作消除的选择：大概率会忽略，毕竟入侵代价昂贵、安全代价昂贵，搞不好不能承受消除的策略：移除入口点、减少攻击面、区分、最小优先权原则消除的技巧：1.建立子模型分支来减小复杂性2.开发可重用的威胁模型库3.先不做假定，消除多数明显的威胁会大大简化 一个安全工程过程模型核心工作1.安全目标定义：形成规范的文档，作为工程过程中的指导原则2.敏感数据分析：确定涉及的敏感数据并分类，给出明确定义、敏感程度和保护措施3.威胁分析：确定系统威胁来源，明确关键工程，部署，各部分及功能用户4.安全设计：根据前三部分实现安全设计。包括架构，安全问题应对措施等5.受攻击面分析：与安全设计对应，根据架构及敏感信息，对受攻击面（端口，数据，文件）及攻击路径进行分析6.安全实现：掌握安全实现方法，库及安全编码原则。针对安全目标，敏感数据，明确威胁的主要应对措施。7.安全测试8.安全维护]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>大二</tag>
        <tag>软件安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[System Call]]></title>
    <url>%2Fpost%2F22713.html</url>
    <content type="text"><![CDATA[Linux内核中设置了一组用于实现各种系统功能的子程序，称为系统调用（system call）。同时它还提供些C语言函数库，这些库对系统调用进行包装和扩展。由于这些库函数与系统调用的关系非常紧密，习惯上把这些函数也称为系统调用。本文是作者在大二写OS作业时整理的，安利文章系统调用跟我学这篇写得真的很舒服，本文很多内容都源于此。一般情况下进程不能访问内核所占内存也不能调用内核函数，但有一般就会有例外，系统调用就是例外。它的原理是进程先用适当的值填充寄存器，然后调用一个特殊的指令，该指令会跳到事先定义的内核中的一个位置（该位置用户进程可读但不可写）。Intel CPU中，这个由中断0x80实现。硬件知道一旦你跳到这个位置（system_call），你就不再是限制模式下运行的用户，而“成为”了操作系统的内核接下来你就可以为所欲为。这个过程会检查系统调用号，该号码告诉内核进程请求哪种服务。然后查看系统调用表(sys_call_table)找到所调用的内核函数入口地址。接着调用函数，等返回后做一些系统检查，最后返回到进程（或到其他进程，如果这个进程时间用尽）。 进程是可并发执行的程序在一个数据集合上的运行过程。 为防止和正常的返回值混淆，系统调用并不直接返回错误码，而是将错误码放入一个名为errno的全局变量中。若系统调用失败，可以读出errno的值来锁定问题。errno不同数值所代表的错误消息定义在errno.h中，可以通过命令man 3 errno来察看它们。需要注意的是，errno的值只在函数发生错误时设置，如果函数不发生错误，errno的值就无定义，并不会被置为0。另外，在处理errno前最好先把它的值存入另一个变量，因为在错误处理过程中，即使像printf()这样的函数出错时也会改变errno的值。 getpid在2.4.4版内核中，getpid是第20号系统调用，其在Linux函数库中的原型是： #include&lt;sys/types.h&gt; /* 提供类型pid_t的定义 */#include&lt;unistd.h&gt; /* 提供函数的定义 */pid_t getpid(void); getpid的作用很简单，就是返回当前进程的进程ID123456#include&lt;unistd.h&gt;main()&#123; printf("The current process ID is %d\n",getpid());&#125; The current process ID is 1980 注意，该程序的定义里并没包含头文件sys/types.h，这是因为我们在程序中没有用到pid_t类型，pid_t类型即为进程ID的类型。事实上，在i386架构上（就是我们一般PC计算机的架构），pid_t类型是和int类型完全兼容的，我们可以就把它当做个整形，比如用”%d”把它打印出来。 fork在2.4.4版内核中，fork是第2号系统调用，其在Linux函数库中的原型是： #include&lt;sys/types.h&gt; /* 提供类型pid_t的定义 */#include&lt;unistd.h&gt; /* 提供函数的定义 */pid_t fork(void); fork系统调用的作用是复制一个进程。当一个进程调用它，完成后就出现两个几乎一模一样的进程，我们也由此得到了一个新进程。123456789101112131415#include&lt;sys/types.h&gt;#inlcude&lt;unistd.h&gt;main()&#123; pid_t pid; /*此时仅有一个进程*/ pid=fork(); /*此时已经有两个进程在同时运行*/ if(pid&lt;0) printf("error in fork!"); else if(pid==0) printf("I am the child process, my process ID is %d\n",getpid()); else printf("I am the parent process, my process ID is %d\n",getpid());&#125; I am the parent process, my process ID is 1991I am the child process, my process ID is 1992 看这个程序的时候，头脑中必须首先了解一个概念：在语句pid=fork()之前，只有一个进程在执行这段代码，但在这条语句之后，就变成两个进程在执行了，这两个进程的代码部分完全相同，将要执行的下一条语句都是if(pid==0)两个进程中，原先就存在的那个被称作“父进程”，新出现的那个被称作“子进程”。父子进程的区别除了进程标志符PID不同外，变量pid的值也不相同，pid存放的是fork的返回值。fork调用的一个奇妙之处就是它仅仅被调用一次，却能够返回两次，它可能有三种不同的返回值： 在父进程中，fork返回新创建子进程的进程ID； 在子进程中，fork返回0； 如果出现错误，fork返回一个负值(当前的进程数已经达到了系统规定的上限这时errno的值被设置为EAGAIN或系统内存不足这时errno的值被设置为ENOMEM）。 exit在2.4.4版内核中，exit是第1号调用，其在Linux函数库中的原型是： #include&lt;stdlib.h&gt;void exit(int status); 这个系统调用是用来终止一个进程的。无论在程序中的什么位置，只要执行到exit系统调用，进程就会停止剩下的所有操作，清除包括PCB在内的各种数据结构，并终止本进程的运行。12345678#include&lt;stdlib.h&gt;main()&#123; printf("this process will exit!\n"); exit(0); printf("never be displayed!\n");&#125; this process will exit! 进程在exit(0)处直接终止，并不会打印后面的printf。exit带有一个整型的参数status，可以用这个参数传递进程结束时的状态，比如正常结束的为0。 _exit_exit在Linux函数库中的原型是： #include&lt;unistd.h&gt;void _exit(int status); 和exit比较一下，exit()函数定义在stdlib.h中，而_exit()定义在unistd.h中。但两者真正的区别在于exit()函数在调用exit系统调用之前要检查文件的打开情况，把文件缓冲区中的内容写回文件，也就是所谓的“清理I/O缓冲”。举个例子，如果同样的两行printf，加上不同的终止调用，会发生什么： main(){printf(&quot;output begin\n&quot;);printf(&quot;content in buffer&quot;);exit(0); # _exit(0);} exit(0)会完成两句printf,但_exit(0)可能完成第一句printf就终止了。这应该不难理解，系统先把前两个printf存入buff，然后一边I/O一边往后执行，如果是_exit(0)不管缓存死活，那就直接结束了啊。但是exit后的进程并不是就灰飞烟灭了，它有个让人毛骨悚然的名字，僵尸进程（Zombie） 进程状态下面简单介绍下进程的状态：分linux内核代码定义的状态和常用状态。由Linux内核代码宏定义出的“TASK_REPORT”： /* get_task_state() */#define TASK_REPORT (TASK_RUNNING | TASK_INTERRUPTIBLE |TASK_UNINTERRUPTIBLE | __TASK_STOPPED |&gt;__TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD) 可以得到七个基本的进程状态，即：R运行状态TASK_RUNNING进程要么正在执行，要么正要准备执行。S可中断睡眠状态TASK_INTERRUPTIBLE进程被阻塞，直到某个条件变为真。条件一旦达成，进程的状态就被设置为TASK_RUNNING。D不可中断睡眠状态TASK_UNINTERRUPTIBLE进程不会立即响应信号。这种状态一般用于内核某些不能被打断的进程，比如等待磁盘或网络I / O的设备驱动程序使用。T停止状态__TASK_STOPPED可以通过发送SIGSTOP信号给进程来停止进程，可以发送SIGCONT信号让进程继续运行t追踪状态__TASK_TRACED进程被debugger等进程监视。处于TASK_TRACED状态的进程不能响应SIGCONT信号而被唤醒。Z僵尸状态EXIT_ZOMBIE进程的执行被终止，但是其父进程还没有使用wait()等系统调用来获知它的终止信息。此时进程几乎的所有资源将被回收，没有任何可执行代码，也不能被调度，仅留下task_struct结构（以及少数资源）记载了些供人凭吊的信息。X死亡状态EXIT_DEAD进程的最终状态，即将被销毁，ls都没了的那种。常用的状态转换图： waitwait的函数原型是： #include &lt;sys/types.h&gt; /* 提供类型pid_t的定义 */#include &lt;sys/wait.h&gt;pid_t wait(int *status) 进程一旦调用了wait，就立即阻塞自己，由wait自动分析是否当前进程的某个子进程已经退出，如果让它找到了这样一个已经变成僵尸的子进程，wait就会收集这个子进程的信息，并把它彻底销毁后返回；如果没有找到这样一个子进程，wait就会一直阻塞在这里，直到有一个出现为止。参数status用来保存被收集进程退出时的一些状态，它是一个指向int类型的指针。但如果我们对这个子进程是如何死掉的毫不在意，只想把这个僵尸进程消灭掉（多数情况如此），我们就可以设定这个参数为NULL:pid = wait(NULL);如果成功，wait会返回被收集的子进程的PID，如果调用进程没有子进程，调用就会失败，此时wait返回-1errno被置为ECHILD。如果想知道status，那就准备个int指针，也可以int status然后传wait(&amp;status)这时就可以调用专门的宏（macro）来获取信息： WIFEXITED(status) 这个宏用来指出子进程是否为正常退出的，如果是，它会返回一个非零值。 WEXITSTATUS(status) 这个宏用来提取子进程的返回值，如果子进程调用exit(5)退出，WEXITSTATUS(status)就会返回5（注意，如果进程不是正常退出的，也就是说，WIFEXITED返回0，这个值就毫无意义） waitpidwaitpid的函数原型是： #include &lt;sys/types.h&gt; /* 提供类型pid_t的定义 */#include &lt;sys/wait.h&gt;pid_t waitpid(pid_t pid,int *status,int options) just多了pid和options两个参数： 参数 pid 为欲等待的子进程的识别码： pid &lt; -1 ；等待进程组 ID 为 pid 绝对值的进程组中的任何子进程； pid = -1 ；等待任何子进程，此时 waitpid() 相当于 wait()。实际上，wait()就是 pid = -1、options = 0 的waitpid() pid = 0 ；等待进程组 ID 与当前进程相同的任何子进程（也就是等待同一个进程组中的任何子进程）； pid &gt; 0 ；等待任何子进程 ID 为 pid 的子进程，只要指定的子进程还没有结束，waitpid() 就会一直等下去。 参数 options提供一些额外的选项来控制 waitpid()： WNOHANG；如果没有任何已经结束了的子进程，则马上返回，不等待； WUNTRACED；如果子进程进入暂停执行的情况，则马上返回，但结束状态不予理会； 也可以将这两个选项组合起来使用，使用 OR 操作 如果不想使用这两个选项，也可以直接把 options 设为0，则 waitpid() 会一直等待，直到有进程退出 waitpid()的返回值，有三种： 正常返回时，waitpid() 返回收集到的子进程的PID； 如果设置了 WNOHANG，而调用 waitpid() 时，没有发现已退出的子进程可收集，则返回0； 如果调用出错，则返回 -1，这时erron 会被设置为相应的值以指示错误所在。（当 pid 所指示的子进程不存在，或此进程存在，但不是调用进程的子进程， waitpid() 就会返回出错，这时 erron 被设置为 ECHILD） 12345678910111213141516171819202122232425262728#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;main()&#123; pid_t pc, pr; pc=fork(); if(pc&lt;0) /* 如果fork出错 */ printf("Error occured on forking.\n"); else if(pc==0)&#123; /* 如果是子进程 */ sleep(10); /* 睡眠10秒 */ exit(0); &#125; /* 如果是父进程 */ do&#123; pr=waitpid(pc, NULL, WNOHANG); /* WNOHANG参数:waitpid不会在这等待 */ if(pr==0)&#123; /* 如果没有收集到子进程 */ printf("No child exited\n"); sleep(1); &#125; &#125;while(pr==0); /* 没有收集到子进程，就回去继续尝试 */ if(pr==pc) printf("successfully get child %d\n", pr); else printf("some error occured\n");&#125; No child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedsuccessfully get child 1526 我们让父进程和子进程分别睡眠了10秒钟和1秒钟，代表它们分别作了10秒钟和1秒钟的工作。父子进程都有工作要做，父进程利用工作的简短间歇察看子进程的是否退出，如退出就收集它。 execQuestion:既然所有新进程都是由fork产生的，而且由fork产生的子进程和父进程几乎完全一样，那岂不是意味着系统中所有的进程都应该一模一样了吗？而且，就我们的常识来说，当我们执行一个程序的时候，新产生的进程的内容应就是程序的内容才对。是我们理解错了吗？实际上在Linux中，exec指的是一组函数，一共有6个，分别是： #include &lt;unistd.h&gt;int execl(const char *path, const char *arg, ...);int execlp(const char *file, const char *arg, ...);int execle(const char *path, const char *arg, ..., char *const envp[]);int execv(const char *path, char *const argv[]);int execvp(const char *file, char *const argv[]);int execve(const char *path, char *const argv[], char *const envp[]); 其中只有execve是真正意义上的系统调用，其它都是在此基础上经过包装的库函数。 exec函数族的作用是根据指定的文件名找到可执行文件，并用它来取代调用进程的内容，换句话说，就是在调用进程内部执行一个可执行文件。这里的可执行文件既可以是二进制文件，也可以是任何Linux下可执行的脚本文件。 与一般情况不同，exec函数族的函数执行成功后不会返回，因为调用进程的实体，包括代码段，数据段和堆栈等都已经被新的内容取代，只留下进程ID等一些表面上的信息仍保持原样，颇有些神似”三十六计”中的”金蝉脱壳”。看上去还是旧的躯壳，却已经注入了新的灵魂。只有调用失败了，它们才会返回一个-1，从原程序的调用点接着往下执行。 现在我们应该明白了，Linux下是如何执行新程序的，每当有进程认为自己不能为系统和拥护做出任何贡献了，他就可以发挥最后一点余热，调用任何一个exec，让自己以新的面貌重生；或者，更普遍的情况是，如果一个进程想执行另一个程序，它就可以fork出一个新进程，然后调用任何一个exec，这样看起来就好像通过执行应用程序而产生了一个新进程一样。 事实上第二种情况被应用得如此普遍，以至于Linux专门为其作了优化，我们已经知道，fork会将调用进程的所有内容原封不动的拷贝到新产生的子进程中去，这些拷贝的动作很消耗时间，而如果fork完之后我们马上就调用exec，这些辛辛苦苦拷贝来的东西又会被立刻抹掉，这看起来非常不划算，于是人们设计了一种写时拷贝（copy-on-write）技术，使得fork结束后并不立刻复制父进程的内容，而是到了真正实用的时候才复制，这样如果下一条语句是exec，它就不会白白作无用功了，也就提高了效率。 在学习它们之前，先来了解一下我们习以为常的main函数。 int main(int argc, char *argv[], char *envp[]) 参数argc指出了运行该程序时命令行参数的个数，数组argv存放了所有的命令行参数，数组envp存放了所有的环境变量。环境变量指的是一组值，从用户登录后就一直存在，很多应用程序需要依靠它来确定系统的一些细节，我们最常见的环境变量是PATH，它指出了应到哪里去搜索应用程序，如/bin；HOME也是比较常见的环境变量，它指出了我们在系统中的个人目录。环境变量一般以字符串”XXX=xxx”的形式存在，XXX表示变量名，xxx表示变量的值。 值得一提的是，argv数组和envp数组存放的都是指向字符串的指针，这两个数组都以一个NULL元素表示数组的结尾。 我们可以通过以下这个程序来观看传到argc、argv和envp里的都是什么东西：1234567891011int main(int argc, char *argv[], char *envp[])&#123; printf("\n### ARGC ###\n%d\n", argc); printf("\n### ARGV ###\n"); while(*argv) printf("%s\n", *(argv++)); printf("\n### ENVP ###\n"); while(*envp) printf("%s\n", *(envp++)); return 0;&#125; 编译cc main.c -o main然后运行，故意加几个没有任何作用的命令行参数./main -xx 000 ### ARGC ###3### ARGV ###./main-xx000### ENVP ###PWD=/home/leiREMOTEHOST=dt.laser.comHOSTNAME=localhost.localdomainQTDIR=/usr/lib/qt-2.3.1LESSOPEN=|/usr/bin/lesspipe.sh %sKDEDIR=/usrUSER=leiLS_COLORS=MACHTYPE=i386-redhat-linux-gnuMAIL=/var/spool/mail/leiINPUTRC=/etc/inputrcLANG=en_USLOGNAME=leiSHLVL=1SHELL=/bin/bashHOSTTYPE=i386OSTYPE=linux-gnuHISTSIZE=1000TERM=ansiHOME=/home/leiPATH=/usr/local/bin:/bin:/usr/bin:/usr/X11R6/bin:/home/lei/bin_=./main 我们看到，程序将”./main”作为第1个命令行参数，所以共有3个命令行参数。这可能与平时习惯的说法有些不同。现在回过头来看一下exec函数族，先把注意力集中在execve上： int execve(const char *path, char *const argv[], char *const envp[]); 对比一下main函数的完整形式，就会发现这两个函数里的argv和envp是完全一一对应的关系。execve第1个参数path是被执行应用程序的完整路径，第2个参数argv就是传给被执行应用程序的命令行参数，第3个参数envp是传给被执行应用程序的环境变量。留心看一下这6个函数还可以发现，前3个函数都是以execl开头的，后3个都是以execv开头的，它们的区别在于，execv开头的函数是以”char *argv[]”这样的形式传递命令行参数，而execl开头的函数采用了我们更容易习惯的方式，把参数一个一个列出来，然后以一个NULL表示结束。这里的NULL的作用和argv数组里的NULL作用是一样的。 在全部6个函数中，只有execle和execve使用了char *envp[]传递环境变量，其它的4个函数都没有这个参数，这并不意味着它们不传递环境变量，这4个函数将把默认的环境变量不做任何修改地传给被执行的应用程序。而execle和execve会用指定的环境变量去替代默认的那些。 还有2个以p结尾的函数execlp和execvp，咋看起来，它们和execl与execv的差别很小，事实也确是如此，除execlp和execvp之外的4个函数都要求，它们的第1个参数path必须是一个完整的路径，如”/bin/ls”；而execlp和execvp的第1个参数file可以简单到仅仅是一个文件名，如”ls”，这两个函数可以自动到环境变量PATH制定的目录里去寻找。123456789101112131415161718192021222324252627282930#include &lt;unistd.h&gt;main()&#123; char *envp[]=&#123;"PATH=/tmp", "USER=lei", "STATUS=testing", NULL&#125;; char *argv_execv[]=&#123;"echo", "excuted by execv", NULL&#125;; char *argv_execvp[]=&#123;"echo", "executed by execvp", NULL&#125;; char *argv_execve[]=&#123;"env", NULL&#125;; if(fork()==0) if(execl("/bin/echo", "echo", "executed by execl", NULL)&lt;0) perror("Err on execl"); if(fork()==0) if(execlp("echo", "echo", "executed by execlp", NULL)&lt;0) perror("Err on execlp"); if(fork()==0) if(execle("/usr/bin/env", "env", NULL, envp)&lt;0) perror("Err on execle"); if(fork()==0) if(execv("/bin/echo", argv_execv)&lt;0) perror("Err on execv"); if(fork()==0) if(execvp("echo", argv_execvp)&lt;0) perror("Err on execvp"); if(fork()==0) if(execve("/usr/bin/env", argv_execve, envp)&lt;0) perror("Err on execve");&#125; 程序里调用了2个Linux常用的系统命令，echo和env。echo会把后面跟的命令行参数原封不动的打印出来，env用来列出所有环境变量。 由于各个子进程执行的顺序无法控制，所以有可能出现一个比较混乱的输出–各子进程打印的结果交杂在一起，而不是严格按照程序中列出的次序。 executed by execlPATH=/tmpUSER=leiSTATUS=testingexecuted by execlpexcuted by execvexecuted by execvpPATH=/tmpUSER=leiSTATUS=testing 果然不出所料，execle输出的结果跑到了execlp前面。 在平时的编程中，如果用到了exec函数族，一定记得要加错误判断语句。因为与其他系统调用比起来，exec很容易受伤，被执行文件的位置，权限等很多因素都能导致该调用的失败。最常见的错误是： 找不到文件或路径，此时errno被设置为ENOENT； 数组argv和envp忘记用NULL结束，此时errno被设置为EFAULT； 没有对要执行文件的运行权限，此时errno被设置为EACCES。 总结下面就让我用一些形象的比喻，来对进程短暂的一生作一个小小的总结： 随着一句fork，一个新进程呱呱落地，但它这时只是老进程的一个克隆。然后随着exec，新进程脱胎换骨，离家独立，开始了为人民服务的职业生涯。人有生老病死，进程也一样，它可以是自然死亡，即运行到main函数的最后一个”}”，从容地离我们而去；也可以是自杀，自杀有2种方式，一种是调用exit函数，一种是在main函数内使用return，无论哪一种方式，它都可以留下遗书，放在返回值里保留下来；它还甚至能可被谋杀，被其它进程通过另外一些方式结束他的生命。进程死掉以后，会留下一具僵尸，wait和waitpid充当了殓尸工，把僵尸推去火化，使其最终归于无形。 这就是进程完整的一生。]]></content>
      <categories>
        <category>Sys</category>
      </categories>
      <tags>
        <tag>OS</tag>
        <tag>大二</tag>
        <tag>进程</tag>
        <tag>系统调用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[System Call]]></title>
    <url>%2Fpost%2F22713.html</url>
    <content type="text"><![CDATA[Linux内核中设置了一组用于实现各种系统功能的子程序，称为系统调用（system call）。同时它还提供些C语言函数库，这些库对系统调用进行包装和扩展。由于这些库函数与系统调用的关系非常紧密，习惯上把这些函数也称为系统调用。本文是作者在大二写OS作业时整理的，安利文章系统调用跟我学这篇写得真的很舒服，本文很多内容都源于此。一般情况下进程不能访问内核所占内存也不能调用内核函数，但有一般就会有例外，系统调用就是例外。它的原理是进程先用适当的值填充寄存器，然后调用一个特殊的指令，该指令会跳到事先定义的内核中的一个位置（该位置用户进程可读但不可写）。Intel CPU中，这个由中断0x80实现。硬件知道一旦你跳到这个位置（system_call），你就不再是限制模式下运行的用户，而“成为”了操作系统的内核接下来你就可以为所欲为。这个过程会检查系统调用号，该号码告诉内核进程请求哪种服务。然后查看系统调用表(sys_call_table)找到所调用的内核函数入口地址。接着调用函数，等返回后做一些系统检查，最后返回到进程（或到其他进程，如果这个进程时间用尽）。 进程是可并发执行的程序在一个数据集合上的运行过程。 为防止和正常的返回值混淆，系统调用并不直接返回错误码，而是将错误码放入一个名为errno的全局变量中。若系统调用失败，可以读出errno的值来锁定问题。errno不同数值所代表的错误消息定义在errno.h中，可以通过命令man 3 errno来察看它们。需要注意的是，errno的值只在函数发生错误时设置，如果函数不发生错误，errno的值就无定义，并不会被置为0。另外，在处理errno前最好先把它的值存入另一个变量，因为在错误处理过程中，即使像printf()这样的函数出错时也会改变errno的值。 getpid在2.4.4版内核中，getpid是第20号系统调用，其在Linux函数库中的原型是： #include&lt;sys/types.h&gt; /* 提供类型pid_t的定义 */#include&lt;unistd.h&gt; /* 提供函数的定义 */pid_t getpid(void); getpid的作用很简单，就是返回当前进程的进程ID123456#include&lt;unistd.h&gt;main()&#123; printf("The current process ID is %d\n",getpid());&#125; The current process ID is 1980 注意，该程序的定义里并没包含头文件sys/types.h，这是因为我们在程序中没有用到pid_t类型，pid_t类型即为进程ID的类型。事实上，在i386架构上（就是我们一般PC计算机的架构），pid_t类型是和int类型完全兼容的，我们可以就把它当做个整形，比如用”%d”把它打印出来。 fork在2.4.4版内核中，fork是第2号系统调用，其在Linux函数库中的原型是： #include&lt;sys/types.h&gt; /* 提供类型pid_t的定义 */#include&lt;unistd.h&gt; /* 提供函数的定义 */pid_t fork(void); fork系统调用的作用是复制一个进程。当一个进程调用它，完成后就出现两个几乎一模一样的进程，我们也由此得到了一个新进程。123456789101112131415#include&lt;sys/types.h&gt;#inlcude&lt;unistd.h&gt;main()&#123; pid_t pid; /*此时仅有一个进程*/ pid=fork(); /*此时已经有两个进程在同时运行*/ if(pid&lt;0) printf("error in fork!"); else if(pid==0) printf("I am the child process, my process ID is %d\n",getpid()); else printf("I am the parent process, my process ID is %d\n",getpid());&#125; I am the parent process, my process ID is 1991I am the child process, my process ID is 1992 看这个程序的时候，头脑中必须首先了解一个概念：在语句pid=fork()之前，只有一个进程在执行这段代码，但在这条语句之后，就变成两个进程在执行了，这两个进程的代码部分完全相同，将要执行的下一条语句都是if(pid==0)两个进程中，原先就存在的那个被称作“父进程”，新出现的那个被称作“子进程”。父子进程的区别除了进程标志符PID不同外，变量pid的值也不相同，pid存放的是fork的返回值。fork调用的一个奇妙之处就是它仅仅被调用一次，却能够返回两次，它可能有三种不同的返回值： 在父进程中，fork返回新创建子进程的进程ID； 在子进程中，fork返回0； 如果出现错误，fork返回一个负值(当前的进程数已经达到了系统规定的上限这时errno的值被设置为EAGAIN或系统内存不足这时errno的值被设置为ENOMEM）。 exit在2.4.4版内核中，exit是第1号调用，其在Linux函数库中的原型是： #include&lt;stdlib.h&gt;void exit(int status); 这个系统调用是用来终止一个进程的。无论在程序中的什么位置，只要执行到exit系统调用，进程就会停止剩下的所有操作，清除包括PCB在内的各种数据结构，并终止本进程的运行。12345678#include&lt;stdlib.h&gt;main()&#123; printf("this process will exit!\n"); exit(0); printf("never be displayed!\n");&#125; this process will exit! 进程在exit(0)处直接终止，并不会打印后面的printf。exit带有一个整型的参数status，可以用这个参数传递进程结束时的状态，比如正常结束的为0。 _exit_exit在Linux函数库中的原型是： #include&lt;unistd.h&gt;void _exit(int status); 和exit比较一下，exit()函数定义在stdlib.h中，而_exit()定义在unistd.h中。但两者真正的区别在于exit()函数在调用exit系统调用之前要检查文件的打开情况，把文件缓冲区中的内容写回文件，也就是所谓的“清理I/O缓冲”。举个例子，如果同样的两行printf，加上不同的终止调用，会发生什么： main(){printf(&quot;output begin\n&quot;);printf(&quot;content in buffer&quot;);exit(0); # _exit(0);} exit(0)会完成两句printf,但_exit(0)可能完成第一句printf就终止了。这应该不难理解，系统先把前两个printf存入buff，然后一边I/O一边往后执行，如果是_exit(0)不管缓存死活，那就直接结束了啊。但是exit后的进程并不是就灰飞烟灭了，它有个让人毛骨悚然的名字，僵尸进程（Zombie） 进程状态下面简单介绍下进程的状态：分linux内核代码定义的状态和常用状态。由Linux内核代码宏定义出的“TASK_REPORT”： /* get_task_state() */#define TASK_REPORT (TASK_RUNNING | TASK_INTERRUPTIBLE |TASK_UNINTERRUPTIBLE | __TASK_STOPPED |&gt;__TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD) 可以得到七个基本的进程状态，即：R运行状态TASK_RUNNING进程要么正在执行，要么正要准备执行。S可中断睡眠状态TASK_INTERRUPTIBLE进程被阻塞，直到某个条件变为真。条件一旦达成，进程的状态就被设置为TASK_RUNNING。D不可中断睡眠状态TASK_UNINTERRUPTIBLE进程不会立即响应信号。这种状态一般用于内核某些不能被打断的进程，比如等待磁盘或网络I / O的设备驱动程序使用。T停止状态__TASK_STOPPED可以通过发送SIGSTOP信号给进程来停止进程，可以发送SIGCONT信号让进程继续运行t追踪状态__TASK_TRACED进程被debugger等进程监视。处于TASK_TRACED状态的进程不能响应SIGCONT信号而被唤醒。Z僵尸状态EXIT_ZOMBIE进程的执行被终止，但是其父进程还没有使用wait()等系统调用来获知它的终止信息。此时进程几乎的所有资源将被回收，没有任何可执行代码，也不能被调度，仅留下task_struct结构（以及少数资源）记载了些供人凭吊的信息。X死亡状态EXIT_DEAD进程的最终状态，即将被销毁，ls都没了的那种。常用的状态转换图： waitwait的函数原型是： #include &lt;sys/types.h&gt; /* 提供类型pid_t的定义 */#include &lt;sys/wait.h&gt;pid_t wait(int *status) 进程一旦调用了wait，就立即阻塞自己，由wait自动分析是否当前进程的某个子进程已经退出，如果让它找到了这样一个已经变成僵尸的子进程，wait就会收集这个子进程的信息，并把它彻底销毁后返回；如果没有找到这样一个子进程，wait就会一直阻塞在这里，直到有一个出现为止。参数status用来保存被收集进程退出时的一些状态，它是一个指向int类型的指针。但如果我们对这个子进程是如何死掉的毫不在意，只想把这个僵尸进程消灭掉（多数情况如此），我们就可以设定这个参数为NULL:pid = wait(NULL);如果成功，wait会返回被收集的子进程的PID，如果调用进程没有子进程，调用就会失败，此时wait返回-1errno被置为ECHILD。如果想知道status，那就准备个int指针，也可以int status然后传wait(&amp;status)这时就可以调用专门的宏（macro）来获取信息： WIFEXITED(status) 这个宏用来指出子进程是否为正常退出的，如果是，它会返回一个非零值。 WEXITSTATUS(status) 这个宏用来提取子进程的返回值，如果子进程调用exit(5)退出，WEXITSTATUS(status)就会返回5（注意，如果进程不是正常退出的，也就是说，WIFEXITED返回0，这个值就毫无意义） waitpidwaitpid的函数原型是： #include &lt;sys/types.h&gt; /* 提供类型pid_t的定义 */#include &lt;sys/wait.h&gt;pid_t waitpid(pid_t pid,int *status,int options) just多了pid和options两个参数： 参数 pid 为欲等待的子进程的识别码： pid &lt; -1 ；等待进程组 ID 为 pid 绝对值的进程组中的任何子进程； pid = -1 ；等待任何子进程，此时 waitpid() 相当于 wait()。实际上，wait()就是 pid = -1、options = 0 的waitpid() pid = 0 ；等待进程组 ID 与当前进程相同的任何子进程（也就是等待同一个进程组中的任何子进程）； pid &gt; 0 ；等待任何子进程 ID 为 pid 的子进程，只要指定的子进程还没有结束，waitpid() 就会一直等下去。 参数 options提供一些额外的选项来控制 waitpid()： WNOHANG；如果没有任何已经结束了的子进程，则马上返回，不等待； WUNTRACED；如果子进程进入暂停执行的情况，则马上返回，但结束状态不予理会； 也可以将这两个选项组合起来使用，使用 OR 操作 如果不想使用这两个选项，也可以直接把 options 设为0，则 waitpid() 会一直等待，直到有进程退出 waitpid()的返回值，有三种： 正常返回时，waitpid() 返回收集到的子进程的PID； 如果设置了 WNOHANG，而调用 waitpid() 时，没有发现已退出的子进程可收集，则返回0； 如果调用出错，则返回 -1，这时erron 会被设置为相应的值以指示错误所在。（当 pid 所指示的子进程不存在，或此进程存在，但不是调用进程的子进程， waitpid() 就会返回出错，这时 erron 被设置为 ECHILD） 12345678910111213141516171819202122232425262728#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;main()&#123; pid_t pc, pr; pc=fork(); if(pc&lt;0) /* 如果fork出错 */ printf("Error occured on forking.\n"); else if(pc==0)&#123; /* 如果是子进程 */ sleep(10); /* 睡眠10秒 */ exit(0); &#125; /* 如果是父进程 */ do&#123; pr=waitpid(pc, NULL, WNOHANG); /* WNOHANG参数:waitpid不会在这等待 */ if(pr==0)&#123; /* 如果没有收集到子进程 */ printf("No child exited\n"); sleep(1); &#125; &#125;while(pr==0); /* 没有收集到子进程，就回去继续尝试 */ if(pr==pc) printf("successfully get child %d\n", pr); else printf("some error occured\n");&#125; No child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedNo child exitedsuccessfully get child 1526 我们让父进程和子进程分别睡眠了10秒钟和1秒钟，代表它们分别作了10秒钟和1秒钟的工作。父子进程都有工作要做，父进程利用工作的简短间歇察看子进程的是否退出，如退出就收集它。 execQuestion:既然所有新进程都是由fork产生的，而且由fork产生的子进程和父进程几乎完全一样，那岂不是意味着系统中所有的进程都应该一模一样了吗？而且，就我们的常识来说，当我们执行一个程序的时候，新产生的进程的内容应就是程序的内容才对。是我们理解错了吗？实际上在Linux中，exec指的是一组函数，一共有6个，分别是： #include &lt;unistd.h&gt;int execl(const char *path, const char *arg, ...);int execlp(const char *file, const char *arg, ...);int execle(const char *path, const char *arg, ..., char *const envp[]);int execv(const char *path, char *const argv[]);int execvp(const char *file, char *const argv[]);int execve(const char *path, char *const argv[], char *const envp[]); 其中只有execve是真正意义上的系统调用，其它都是在此基础上经过包装的库函数。 exec函数族的作用是根据指定的文件名找到可执行文件，并用它来取代调用进程的内容，换句话说，就是在调用进程内部执行一个可执行文件。这里的可执行文件既可以是二进制文件，也可以是任何Linux下可执行的脚本文件。 与一般情况不同，exec函数族的函数执行成功后不会返回，因为调用进程的实体，包括代码段，数据段和堆栈等都已经被新的内容取代，只留下进程ID等一些表面上的信息仍保持原样，颇有些神似”三十六计”中的”金蝉脱壳”。看上去还是旧的躯壳，却已经注入了新的灵魂。只有调用失败了，它们才会返回一个-1，从原程序的调用点接着往下执行。 现在我们应该明白了，Linux下是如何执行新程序的，每当有进程认为自己不能为系统和拥护做出任何贡献了，他就可以发挥最后一点余热，调用任何一个exec，让自己以新的面貌重生；或者，更普遍的情况是，如果一个进程想执行另一个程序，它就可以fork出一个新进程，然后调用任何一个exec，这样看起来就好像通过执行应用程序而产生了一个新进程一样。 事实上第二种情况被应用得如此普遍，以至于Linux专门为其作了优化，我们已经知道，fork会将调用进程的所有内容原封不动的拷贝到新产生的子进程中去，这些拷贝的动作很消耗时间，而如果fork完之后我们马上就调用exec，这些辛辛苦苦拷贝来的东西又会被立刻抹掉，这看起来非常不划算，于是人们设计了一种写时拷贝（copy-on-write）技术，使得fork结束后并不立刻复制父进程的内容，而是到了真正实用的时候才复制，这样如果下一条语句是exec，它就不会白白作无用功了，也就提高了效率。 在学习它们之前，先来了解一下我们习以为常的main函数。 int main(int argc, char *argv[], char *envp[]) 参数argc指出了运行该程序时命令行参数的个数，数组argv存放了所有的命令行参数，数组envp存放了所有的环境变量。环境变量指的是一组值，从用户登录后就一直存在，很多应用程序需要依靠它来确定系统的一些细节，我们最常见的环境变量是PATH，它指出了应到哪里去搜索应用程序，如/bin；HOME也是比较常见的环境变量，它指出了我们在系统中的个人目录。环境变量一般以字符串”XXX=xxx”的形式存在，XXX表示变量名，xxx表示变量的值。 值得一提的是，argv数组和envp数组存放的都是指向字符串的指针，这两个数组都以一个NULL元素表示数组的结尾。 我们可以通过以下这个程序来观看传到argc、argv和envp里的都是什么东西：1234567891011int main(int argc, char *argv[], char *envp[])&#123; printf("\n### ARGC ###\n%d\n", argc); printf("\n### ARGV ###\n"); while(*argv) printf("%s\n", *(argv++)); printf("\n### ENVP ###\n"); while(*envp) printf("%s\n", *(envp++)); return 0;&#125; 编译cc main.c -o main然后运行，故意加几个没有任何作用的命令行参数./main -xx 000 ### ARGC ###3### ARGV ###./main-xx000### ENVP ###PWD=/home/leiREMOTEHOST=dt.laser.comHOSTNAME=localhost.localdomainQTDIR=/usr/lib/qt-2.3.1LESSOPEN=|/usr/bin/lesspipe.sh %sKDEDIR=/usrUSER=leiLS_COLORS=MACHTYPE=i386-redhat-linux-gnuMAIL=/var/spool/mail/leiINPUTRC=/etc/inputrcLANG=en_USLOGNAME=leiSHLVL=1SHELL=/bin/bashHOSTTYPE=i386OSTYPE=linux-gnuHISTSIZE=1000TERM=ansiHOME=/home/leiPATH=/usr/local/bin:/bin:/usr/bin:/usr/X11R6/bin:/home/lei/bin_=./main 我们看到，程序将”./main”作为第1个命令行参数，所以共有3个命令行参数。这可能与平时习惯的说法有些不同。现在回过头来看一下exec函数族，先把注意力集中在execve上： int execve(const char *path, char *const argv[], char *const envp[]); 对比一下main函数的完整形式，就会发现这两个函数里的argv和envp是完全一一对应的关系。execve第1个参数path是被执行应用程序的完整路径，第2个参数argv就是传给被执行应用程序的命令行参数，第3个参数envp是传给被执行应用程序的环境变量。留心看一下这6个函数还可以发现，前3个函数都是以execl开头的，后3个都是以execv开头的，它们的区别在于，execv开头的函数是以”char *argv[]”这样的形式传递命令行参数，而execl开头的函数采用了我们更容易习惯的方式，把参数一个一个列出来，然后以一个NULL表示结束。这里的NULL的作用和argv数组里的NULL作用是一样的。 在全部6个函数中，只有execle和execve使用了char *envp[]传递环境变量，其它的4个函数都没有这个参数，这并不意味着它们不传递环境变量，这4个函数将把默认的环境变量不做任何修改地传给被执行的应用程序。而execle和execve会用指定的环境变量去替代默认的那些。 还有2个以p结尾的函数execlp和execvp，咋看起来，它们和execl与execv的差别很小，事实也确是如此，除execlp和execvp之外的4个函数都要求，它们的第1个参数path必须是一个完整的路径，如”/bin/ls”；而execlp和execvp的第1个参数file可以简单到仅仅是一个文件名，如”ls”，这两个函数可以自动到环境变量PATH制定的目录里去寻找。123456789101112131415161718192021222324252627282930#include &lt;unistd.h&gt;main()&#123; char *envp[]=&#123;"PATH=/tmp", "USER=lei", "STATUS=testing", NULL&#125;; char *argv_execv[]=&#123;"echo", "excuted by execv", NULL&#125;; char *argv_execvp[]=&#123;"echo", "executed by execvp", NULL&#125;; char *argv_execve[]=&#123;"env", NULL&#125;; if(fork()==0) if(execl("/bin/echo", "echo", "executed by execl", NULL)&lt;0) perror("Err on execl"); if(fork()==0) if(execlp("echo", "echo", "executed by execlp", NULL)&lt;0) perror("Err on execlp"); if(fork()==0) if(execle("/usr/bin/env", "env", NULL, envp)&lt;0) perror("Err on execle"); if(fork()==0) if(execv("/bin/echo", argv_execv)&lt;0) perror("Err on execv"); if(fork()==0) if(execvp("echo", argv_execvp)&lt;0) perror("Err on execvp"); if(fork()==0) if(execve("/usr/bin/env", argv_execve, envp)&lt;0) perror("Err on execve");&#125; 程序里调用了2个Linux常用的系统命令，echo和env。echo会把后面跟的命令行参数原封不动的打印出来，env用来列出所有环境变量。 由于各个子进程执行的顺序无法控制，所以有可能出现一个比较混乱的输出–各子进程打印的结果交杂在一起，而不是严格按照程序中列出的次序。 executed by execlPATH=/tmpUSER=leiSTATUS=testingexecuted by execlpexcuted by execvexecuted by execvpPATH=/tmpUSER=leiSTATUS=testing 果然不出所料，execle输出的结果跑到了execlp前面。 在平时的编程中，如果用到了exec函数族，一定记得要加错误判断语句。因为与其他系统调用比起来，exec很容易受伤，被执行文件的位置，权限等很多因素都能导致该调用的失败。最常见的错误是： 找不到文件或路径，此时errno被设置为ENOENT； 数组argv和envp忘记用NULL结束，此时errno被设置为EFAULT； 没有对要执行文件的运行权限，此时errno被设置为EACCES。 总结下面就让我用一些形象的比喻，来对进程短暂的一生作一个小小的总结： 随着一句fork，一个新进程呱呱落地，但它这时只是老进程的一个克隆。然后随着exec，新进程脱胎换骨，离家独立，开始了为人民服务的职业生涯。人有生老病死，进程也一样，它可以是自然死亡，即运行到main函数的最后一个”}”，从容地离我们而去；也可以是自杀，自杀有2种方式，一种是调用exit函数，一种是在main函数内使用return，无论哪一种方式，它都可以留下遗书，放在返回值里保留下来；它还甚至能可被谋杀，被其它进程通过另外一些方式结束他的生命。进程死掉以后，会留下一具僵尸，wait和waitpid充当了殓尸工，把僵尸推去火化，使其最终归于无形。 这就是进程完整的一生。]]></content>
      <categories>
        <category>Sys</category>
      </categories>
      <tags>
        <tag>OS</tag>
        <tag>大二</tag>
        <tag>进程</tag>
        <tag>系统调用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[恋与L]]></title>
    <url>%2Fpost%2F16623.html</url>
    <content type="text"><![CDATA[第一章&emsp;&emsp;初遇L是在我中考后的那个暑假，那时候不知道是哪里来的好学之心，竟然去报了个初升高的衔接班。爸妈看我这么有志气的样子感觉也没什么太正当的理由来阻拦我，就顺着我的意交钱让我上了。当然后来我高考完再寻思这事（其实我真正寻思这事是在高一，那时候内心疯狂吐槽为什么不好好过个没作业的假期）真真切切觉得这事儿除了能提高你点自信心外好像就真的没什么用处。刚到个陌生环境大家都不太适应，但是你竟然把前几个月要学的东西草草学了一遍，哎这你就有了一个能吹水，哦不，是能提高自信的机会了，当然这事也是因人而异，可能我会这么想多半是因为我天赋有限，白白糟蹋了钱财和大好的假期机会。&emsp;&emsp;在那个衔接班上，坐在我前面的就是L。这里给L取个名字吧，随意一点取的话，就叫路嘉琪吧。（虽然我说是很随意，但其实还是经历过某种深思熟虑的，绝对不是因为输入法蹦出来就随便选了这么草率的，嗯）第一次见她就觉着这个女孩子的发质非常好（？？？我绝对没有什么特殊的恋发癖），当然这并不是在给我上课卷她头发找借口，只是陈述一下第一眼的直观感受。她那时扎了个干净利索的马尾，但长度绝对能自然垂到肩下，发量很足，长发在末梢倒不显散乱。稍显弧度的马尾下是张非常白净的脸，记忆里是种瓷白，像玉石的温润中透着股月牙的皙白，如果能轻轻戳一下的话，说不定会是凉凉微滑的瓷器手感。标准的杏脸加上不错的五官，再配上她显出的那种富养大小姐的感觉，大方里流露点江南水乡蕴出来的温婉味道……我现在倒也说不清楚这是那时真切的第一印象还是我后期回忆里不断修正加工层层滤镜后的“第一印象”，总之就是好感MAX吧。&emsp;&emsp;过了一开始的新奇劲儿，之后大家平常的介绍然后熟识，像这种衔接班也就上个一个月左右。这一个月的日子像是平时，只是换了批老师和同学，平日里该笑该闹还是继续。依稀记得上课的那个学校里栽了很多竹子，一节节地蹿上了好几层楼，视线偏转一下就能看见一墙又一墙的爬山虎，风吹的时候一层层撩在人心上，让心儿痒得直颤。等好不容易熬到下课，我一般会去走廊上摘几片够得到的竹叶，上课了就把弄着竹叶玩儿，顺便留一片别在路嘉琪的马尾上。这种情况一般是不会被发现的，毕竟我的手法愈趋娴熟，而她的发量又着实比较充足。但人生在世总会遇到些说不清道不明的意外，偶尔被发现后她也只会回过头瞪着我几秒，毕竟我算准了上课时间她也不能做出什么过激的举动，等下课了，大家也就忘了这事儿，还像以往那样随意地谈天说地，聊着各种奇奇怪怪的话题。而随着这个衔接班的结束，中考录取结果也下来了，我和她不在同一所高中，她被我高中所谓的兄弟学校（我一直很好奇为什么不叫姐妹花学校）录取了。当时一边在心里小声哼着“啊~好朋友，再见~再见”一边做着不深不浅的道别，只是想着大概是没什么机会再见了，咱们，山高路远，就此别过。浑然不知后面还会和路嘉琪再扯上点意料之外的事。 第二章&emsp;&emsp;刚踏入高中校园的我有点点激动，这个高中很符合我的设想。整个校园很大（和我大二所在的校区一样大）各种徽派的设计也很得体，那股单纯的兴奋劲儿比我刚进大学校园还要冲（第四声）还要足。虽然高中是寄宿制，但大部分学生还是会选择走读即只中午在校午休。寝室是个标准六人间的配置，一般只会住四个人左右不会住满。最初的室友有三个人，名字也随意点，分别是大鲍、涵姐和骚翔。简单介绍下，大鲍是个身材魁梧，肤色偏黑，沉默寡言的壮硕男生，为人老实，热爱看书。平时虽不多言，但一出口，必属骚中佳句。涵姐整个人就有些浪里滑条的感觉，平时虽然骚浪了一点，但人却不差。除了那张浪里浪气的脸，各方面也是均值往上跑了。骚翔人如其名，一个“骚”字，论骚我也认识了不少人，但至今没人能骚得过他，以至于给我幼小的纯洁心灵留下了一道道崩坏三观的刻印。骚翔看上去就像个新疆的混血，是偏烤羊肉串的那种，不过到底是哪一族的混血他不愿主动说，我们也不好意思问，这个问题便就此作罢，只能在心中随意想想。当熟悉了我这三位室友的作风，不禁仰头长叹，本来一个正值大好年华的青葱少年，却被命运给安排上了这三个明骚暗骚的人，竟然还稍微有些期待？总之就这样展开了我第一段激情四射的宿舍生活。就比如军训的时候，作为一名纯洁正直，不蔓不枝的男子高中生，午休期间我肯定是不愿和他们三个同流合污相互扯淡的，那么睡觉无疑是我可以选择的最佳回避方式。但谁料，哪怕我睡了也无法暂停他们恶魔般的骚气步伐。涵姐见我侧卧在床半天没有动静，“你这不会就睡觉了吧？”，见我毫无回应之后索性放开了连念我名字后两字数十遍，想来大概和某种复杂的传教索魂仪式相似，嗯“我喊你声你敢应吗”的那种。“咦？”只见涵姐俯卧在床摸了摸自己的脸颊“这喊多了就成‘学姐’了，以后就叫学姐好吧”。其余两人略一思索，不约而同地觉得这个称呼十分贴切，真真是一场美妙的缘分，呃不是，室友情。就这样，我在睡梦中就有了这个贯穿我高中直至现在的外号。这声“学姐”被各式各样的人吐槽调侃过，像同学朋友就不说了，被班主任和各科老师，父母和亲戚……他们发现的来源主要有以下几种：在校几乎所有学生都喊我“学姐”不论男女（微笑脸）上课总会有人嘴贱喊出声叫老师听见，就像自己只是喝了口水般理所理当（微笑脸X2）网上评论直接“学姐”来“学姐”去，我爸妈真的想不知道都难（微笑脸X3）就这样慢慢的，“学姐”这个称号已经完美融入到我的人设里，成为了我生命中不可或缺的一份子（微笑脸X4）。&emsp;&emsp;正式上课后，我和那么多在题海里沉浮的少男少女并没什么二异，一个没什么太大爱好也没什么太多想法的的高一学生，整天就像被洗脑般脑中只有个高考的模糊影子，然后告诉你照着这个影子疯狂追就完事了，但丝毫没有人来告诉你追到了后要干什么，也没人告诉你这个影子后面会有那么多更加张牙舞爪的影子要你一个人去独自面对。不过嘛，那个时候的我们是没那个心思去思虑未来的，每天只顾着和周边人扯淡，关心些乱七八糟的事，以及在杂乱的烦心之余尽力学习一下意思意思。就在这样的肆意挥发荷尔蒙，哦不是，是散发青春气息的步子里，我跌跌撞撞地走了一个学期，谈不上多开心但整个生活确是像色调明亮的油画般充满了少年气，好似有股糯黄的酥糖味。而就在这个中二的骑士想要翻开下一学期的日历前，因为一件小事儿，路嘉琪找到了我。 第三章&emsp;&emsp;是件很小的事，但借此为契机，我和她在网上慢慢熟络，渐渐抛开原先有的那点小拘谨，好感像颗种子在我尚未察觉之际已经慢慢吐出怒绿的幼芽。从原先的如果什么时候在线就随便聊聊（事实上她的QQ显示是全天在线的，至少每次不论我什么时候上线她都一定是在的）到每天固定的些时间点互相聊些什么，一般是我听她说，然后给点不温不火的建议与想法。啧现在想想我那个时候的回答应该可傻逼了，不过好在她应该只是想找个人可以安安静静地听她的各种烦忧，所以在这样的网聊节奏下，我和她的关系倒是越走越近，至少在我看来是更近些的。近到或者让我自以为是到，每天会不经意说出和她的聊天而觉得平常，这样的结果就是激起了我前座的狗血八卦之魂。我前座小黑是个偏瘦的女生，叫她小黑纯粹是因为她肤色偏黑一点，就这么随口叫下来了。小黑是那种人前疯来跑去，但内里有点忧郁寡淡，有时候还敏感事多的那种。不过她学了那么多年的舞，身材和气质还是有的。我这么帮她说话起源得归咎她给我的那张贺卡，那时候刚高中没什么事儿的孩子总喜欢互赠各种贺卡，别人的贺卡大都是些无关疼痒但微微暖心的小祝福，小黑的偏是什么“我也很喜欢纳兰容若的词”和“你平时都是笑的样子只是表面”这样的话，事实证明，千万不要和中二病说这些，如果你不想和他们扯上关系的话。这些话对中二病的杀伤力无疑是巨大的，所以之后关于她的事，初印象总还不错。她就追问了我路嘉琪的名字，然后诡异的事发生了，路嘉琪是小黑的初中闺蜜……嗯？？？这是什么神仙孽缘，啊不是，肯定是天造地设的缘分啊这次。&emsp;&emsp;就这样，白天找小黑聊着路嘉琪初中的事，晚上和路嘉琪随意地谈天说地，聊人生聊理想，当时只觉得日子过得像神仙一样，管他的高考学习，我只要能每天都和她聊上天就好了，再说我成绩也，也还过得去嘛，这种事情也肯定不会影响学习的啦。反正每天晚上找父母借到手机就往卫生间跑，用各种什么拉肚子啊泡澡怎么可能没有bgm这样的烂理由延长用手机的时间来和她聊天。 她发“我来当你的树洞好了，有什么话都可以对我说哦”， 我回“嗯嗯，你也是，有什么事情都直接和我说好了”。 “你想考什么学校啊”， “我啊，没怎么想过，应该是像北航这样的学校吧”。 “那很好啊”， “你呢？”。 “我想去上海或者江浙那边的学校，具体还没想好”， “不急，才高一嘛，像我想好了估计也考不上，北航好像挺难考的”。 “你可是学霸啊，要好好加油啊”， “嗯嗯，一定，你也是！”。 …… &emsp;&emsp;就这样，每晚我都躲在卫生间里偷着乐，现在想想，一个小男孩背靠着厕所的墙壁蹲在地上抱着宛若至宝的手机，看着屏幕上的字傻笑却不敢发出声的样子，真的是多亏了爸妈给我的无限信任。不过有点，和路嘉琪聊天的时候也不是没说过小黑的事，但她的回应就比较平淡了，和我想的有点不太一样，不过当时的我也没多想就继续乐呵着和她扯别的话题了。 第四章&emsp;&emsp;某个下午体育课我在操场问起小黑，“她的话应该比较受欢迎吧，初中有谈过恋爱吗？”，“嗯有”，“哎哎，是什么样的啊”，“嗯，她初中会经常性地用小刀割腕，留下了很多印子”，“？？？啊？”，小黑看了眼惊诧非常的我，淡淡地说“我觉得你看到的她只是很表面的她”小黑说完轻叹口气，撇过头继续回我的问题，“然后她的男朋友在她某次想用小刀划自己的时候，把刀抢过对着自己的手腕划了一刀，和她说以后如果再想割的话，我陪你一起割。她好像挺感动的，后来就在一起了。不过中考后发生了些事情，好像分手了，她也删掉了初中所有人的联系方式。”，“……”当时我有点说不出话来，所有的气力都卡在喉咙里发不出一点声音。我到现在为止的生活都很普通，普通地学习，普通地长大，普通地在老师父母设置好的轨迹上咕噜咕噜滚着。这样普通的我不能理解为什么会真的有人会选择自残，去尝试割腕。这对我的冲击一如多年后当我得知，有某个我很钦佩的很优秀的朋友选择用自杀结束自己年轻的生命和未知的前途的时候，从未想过这样的事会发生在自己的身边，所以当发生的时候所带来的冲击和动摇无疑是巨大的。小黑看我良久不说话，偏头看向我，“学姐，我觉得你做不到”，“为什么要选择和她一起割啊，不应该想办法让她不再伤害自己才对吗”我有点激动，夹着猝不及防的三观动摇，对她的心疼，还有自己的不服气。小黑回过头确是又叹了口气，“唉，所以你……不过或许你这样理想主义的真的能帮到她也不一定”，不过我当时的想法倒是：为什么你老是在叹气啊喂，我有那么不行吗喂。有些沮丧的我伸着头望了望周围，柔和偏橘的阳光打下来，整个操场以及操场上的人都好像被浇了层薄蜜，稍微有点闪，我下意识低了低头。“你喜欢她吗？”耳边突然传来小黑的声音，我有点慌乱，“啊？我不知道，应该没有吧，只是朋友而已，比较好的朋友而已”，“那如果她和别的男生走在一起，拥抱打kiss，你也不会难受？”，“可能会有一点吧，但……”小黑不等我说完就走开了，只留下一句“你慢慢想吧”，剩我一个呆呆站在操场上。慢慢想？我想个鬼啊，这种事我怎么知道。&emsp;&emsp;当晚，我还是和路嘉琪没心没肺地聊着天，只是在心里暗暗下决定，一定要帮她。（当时真的就是这么想的啊，现在想想人家根本就没什么好要我帮的，倒不如说每天聊聊天就已经很好了，但那个时候中二的我，嗯，总喜欢一厢情愿地沉浸在自己的想象里）第二天小黑倒也没再提那事，只是对我来说始终有个刺大大咧咧地刺在那儿。中午回到寝室，室友们正在用他们扯淡的想象展现着大鲍家是多么有钱，比如 “哎以后去当大鲍家的扫地嘚~给你配几把黄金镶钻扫把”，“啧，镶钻怎么配得上大鲍的身价，那肯定要全是钻石打造的才行”，“噢~这是我考虑不周了，再配个钻石拖把，拖出来的地那是要比钻石还要亮”……之后骚翔见我进来不说话，“哦豁，学姐你竟然不说话，是不是看不起大鲍的家产，你看你头顶马上就会有无数个红点”。我的内心：兄弟我正在思考一些很哲学，呃不，是很重要的问题，你这样我很跳戏啊喂。等我翻身爬上床后，大鲍看我靠墙的背影说“你们看学姐这个角度看好像个女生啊”，骚翔马上就接“学姐本来不就是个女生嘛”，在我刚想反驳的时候，我就看见涵姐直接过来一边爬我的梯子一边淫笑着“嘿嘿嘿那我们来确认一下不就好了”，骚翔看到后不甘示弱，因为我和他床在同一边，他在尝试着能不能直接跨过来……接下来，虽然我奋力抵抗，但终究是腹背受敌最后败下阵来。然后那天中午我们三个男生（大鲍还是比较好的，安安心心睡在自己床上）挤在一张小小寝室单人床里，竟然各自都睡着了，让我感受到了我高中宿舍床板质量之高。不过和他们这么闹过之后，心情倒是放开了很多。跟着楼梯和人流向上走的时候，我决定向路嘉琪表明自己的心意。（我后来发现我好像还蛮喜欢这种自爆式告白的哎（微笑X 尾声&emsp;&emsp;这是个中二病一厢情愿最后什么也没能做到的故事。没能让她变得开心一点，不仅什么都做不到还给她徒增了很多烦闷。对当时的我来说，尽力想去帮别人但最后适得其反，给了我很深的打击，加上之前三观上的动摇，我开始对我本应普通的生活产生了怀疑。看着自己还未开出花朵的幼苗灰败地枯死，像是游荡到了某条寂静没有生灵的河流中，随着水波慢慢浮沉。但哪怕心里难受得只想缩成一团藏进角落，还是有些回忆像荧星点点悬浮在脑海中，不忍让它们散去。在你触碰到美好的时候就应该想到，任何事情都不可能只有美好。此时的欢愉和彼时的苦痛不断交织，在你决定享受喜悦的时候，就该做好面对以后未知苦难的准备，就像“她那时候还太年轻，不知道所有命运馈赠的礼物，早已在暗中标好了价格”，出来混，迟早是要还的。还是希望L会更开心一点吧。总之，随着这场无声的号角吹响然后顷刻覆灭的戏剧过后，没多久就迎来了高一的暑假，假期混混模联，做做义卖，以及最本职最头疼的暑假作业，倒是给高一画上了个还挺正常兼正经的句号。]]></content>
      <categories>
        <category>恋爱物语</category>
      </categories>
      <tags>
        <tag>大二</tag>
        <tag>恋爱物语</tag>
        <tag>随记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[恋与序]]></title>
    <url>%2Fpost%2F9391.html</url>
    <content type="text"><![CDATA[序章&emsp;&emsp;14年，那时的我在中考之后脱离了低阶中二，怀着高阶的中二和以各种动漫和轻小说为蓝本构建的美好高中幻想（比如天降个超级好看的小姐姐什么的……当然这种事情我现在大二了也依旧在幻想就是了），踏入了所在城市最好的高中之一。所谓“高阶的中二”，考完中考的那个假期由于无聊，就随意找了部看上去还不错的动漫，见了里面粉毛的女主，顿时惊为天人，励志要做一个像男主那样“亚撒西”的人，然后去找一个像女主的小姐姐（好了不用滋醒我，我知道不可能了）。由此，就开始了我漫长却又极具“趣味性”和戏剧性的感情经历。&emsp;&emsp;其实还别说，去学着日漫男主的样子混，只要你能学得像，总会有世界剧本女主的人出现在你眼前，这时候大概会出现两种情况，一是鉴于各种原因，你在她的剧本里只是个路过的路人乙，而路人的戏份总不会有太多。二是鉴于各种原因，你并不会那么理所当然的认为她是女主或者说你的the one，但由于很符合你目前心中的各种设定，总是一层一层地荡着好感，然后好感一圈圈转成棉花糖把自己裹在里面。同时，可能是混得太像那么回事儿了吧，我收到了至今为止都是我觉得对我赞誉最高的一句话“那时候的你仿佛是闪着光的”，以至于哪怕后来我真的很想努力去做个能为某个人照点亮光的人的时候，没能做到所带来的挫败感是无比巨大的，巨大到后来的我再想从自己身上找到点光就像是从被拧干的毛巾里寻找能沁住自己的水流一般，百般无奈后只能靠铺开摊在脸上找点湿润聊以自慰与自嘲。]]></content>
      <categories>
        <category>恋爱物语</category>
      </categories>
      <tags>
        <tag>大二</tag>
        <tag>恋爱物语</tag>
        <tag>随记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hidden Markov Model]]></title>
    <url>%2Fpost%2F19116.html</url>
    <content type="text"><![CDATA[Hidden Markov Model(HMM)是一种关于时序的概率模型，最初由 L. E. Baum 和其它一些学者发表在一系列的统计学论文中，随后在语言识别，自然语言处理以及生物信息等领域体现了很大的价值。本文通过个人的理解以及《统计学习方法》 中的公式推导对HMM的定义及其三个基本问题进行了简单的介绍。 Markov model 在介绍隐马尔可夫模型前我们先来介绍一下基础的马尔可夫链。马尔可夫链是一个随机模型描述序列可能的事件，其中每个事件的概率仅取决于在先前事件获得的状态 。粗略地说，以系统的当前状态为条件，其未来和过去的状态是相互独立的。 马尔可夫链的节点是状态，边是转移概率，是template CPD（条件概率分布）的一种有向状态转移表达。马尔可夫过程可以看做是一个自动机 ，以一定的概率在各个状态之间跳转。接下来以一阶马尔可夫链（first-order Markov chain）举例，N 次观测的序列的联合概率分布为： $$p(x_1,…,x_n)=p(x_1)\prod_{i=2}^n p(x_n|x_{n-1}) \quad$$ 由于每个事件发生的概率仅于前一件事件有关，即有： $$p(x_n |x_1,…,x_{n-1} )=p(x_n |x_{n-1})$$ 下面介绍马尔可夫链的一个重要性质，当n趋向于无穷时，p（Xn）趋向于一个定值。首先将每个事件的转移概率相整合成一个转移概率矩阵，假设每个事件的发生概率只与前一个事件的状态有关，共计n个状态的话，我们可以用一个n*n的矩阵P来表示转移概率，即 当我们有初始状态矩阵 时（ 表示初始状态为第i个状态的概率）则第n个状态的概率矩阵为 通过线性代数知识将转移概率矩阵做对角化，可得无穷个P矩阵相乘为一个常数，即马尔可夫链的极限收敛定理，马尔可夫链的这一性质对于其实际运用有重要的意义与作用。 HMM定义 在正常的马尔可夫模型中，状态对于观察者来说是直接可见的。这样状态的转换概率便是全部的参数。而在实际情况中，较为本质的状态转换通常是较为隐性即无法实际观测的，但是受该状态影响的变量是我们所可以观测的，由此，我们来介绍隐马尔可夫模型。在隐马尔可夫模型中，状态并不是直接可见的，但受状态影响的某些变量则是可见的。每一个隐性状态对于可观测的显性状态都有一概率分布，我们将这个概率称之为发射概率。 由此，如果我们有n个隐性状态的状态集合Q和有m个显性状态的观测集合V的话，假设I是长度为T的状态序列，O是对应的观测序列。我们就可设置转移一个$n \times n$的转移概率矩阵A和一个$m \times n$的发射概率矩阵B，π是初始状态概率向量： $n \times n$的转移概率矩阵A: $ \qquad a_{ij}=P(i_{t+1}=q_j│i_t=q_i ) \qquad i=1,…,N;j=1,…,N$$m \times n$的发射概率矩阵B: $ \qquad b_j (k)=P(o_t=v_k│i_t=q_j ) \qquad k=1,…,M;j=1,…,N$π是初始状态概率向量: $ \qquad π_i=P(i_1=q_i) \qquad i=1,…,N;j=1,…,N$ 状态转移概率矩阵A与初始状态概率向量π确定了隐藏的马尔可夫链，生成不可观测的状态序列。发射概率矩阵B确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。隐马尔可夫模型就是由基本的A,B,π三个矩阵或向量加上具体的状态集合Q和观测序列V所构成的。 接下来我以维基百科上的一个经典例子来作为算法实际运用的实例 ： “Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.Alice believes that the weather operates as a discrete Markov chain. There are two states, “Rainy” and “Sunny”, but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: “walk”, “shop”, or “clean”. Since Bob tells Alice about his activities, those are the observations. The entire system is that of a hidden Markov model (HMM).Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. 。” 这段稍显繁琐的文字可以转换成一张较为简洁易懂的状态转换图，如下： HMM基本问题 概率计算问题：给定模型λ(A,B,π)和观测序列O,计算在模型λ下观测序列O出现的概率P(O|λ)为多少。 学习问题：己知观测序列O,估计模型参数λ(A,B,π)使得在该模型下观测序列概率P(O|λ)最大。即用极大似然估计的方法估计参数。 预测问题：也称为解码（decoding)问题。已知模型λ(A,B,π)和观测序列O，求对给定观测序列条件概率P(I|O)最大的状态序列I。即给定观测序列，求最有可能的对应的状态序列。 概率计算 直接计算给定模型，求给定长度为T的观测序列的概率，直接计算法思路是枚举所有的长度T的状态序列，计算该状态序列与观测序列的联合概率（隐状态发射到观测），再用全概率公式对所有枚举项求和。在状态种类为N的情况下，一共有N^T种排列组合，每种组合计算联合概率的计算量为T，总的复杂度为O(T*N^T)，并不可取。 前向计算前向算法的介绍前向计算中最为重要的一步的就是前向概率的设定，对于给定隐马尔可夫模型λ(A,B,π)，定义到时刻t为止的观测序列为O且状态为 的概率为前向概率，记作 $$α_t (i)=P(o_1,o_2,…,o_t,i_t=q_i |λ)$$ 对于每一个时间点上的状态，都是用一个长度为n的概率矩阵来标记，转移到下一个状态前需要满足当前状态的观测值和已知观测序列一致，达成这个条件后即可正常转移到下一个状态，依次下去我们便可以递推地求得下一个前向概率及观测序列概率P(O|λ) 初值$$α_1 (i)=π_i b_i (o_1 ) \qquad i=1,…,N$$前向概率的定义中一共限定了两个条件，一是到当前为止的观测序列，另一个是当前的状态。所以初值的计算也有两项（观测和状态），一项是初始状态概率，另一项是发射到当前观测的概率。 递推对t=1,2…T-1$$\alpha_{t+1} (i)=[\sum_{j=1}^N \alpha_t (j) a_{ji}] b_i(o_{t+1}) \qquad i=1,…,N$$每次递推同样由两部分构成，括号中是当前状态为i且观测序列的前t个符合要求的概率，括号外的是状态i发射观测t+1的概率。 终止$$P(O|λ)=\sum_{i=1}^N[α_T (i)] \qquad i=1,…,N $$ 由于到了时间T，一共有N种状态发射了最后那个观测，所以最终的结果要用全概率公式将这些概率加起来。由于每次递推都是在前一次的基础上进行的，所以降低了复杂度。 前向算法实例以之前天气的例子来说明前向算法，HMM模型可以写成：$$A=\begin{bmatrix} 0.7 &amp; 0.3 \\ 0.4 &amp; 0.6 \\ \end{bmatrix} $$$$B=\begin{bmatrix} 0.1 &amp; 0.4 &amp; 0.5 \\ 0.6 &amp; 0.3 &amp; 0.1 \\ \end{bmatrix} $$$$π=[0.6,0.4]^T$$$$O=（散步，购物，扫除）$$假如我们想计算在（0.6,0.4）的初始状态下鲍比在未来三天分别按顺序做散步，购物，扫除的概率是多大，按照前向概率法计算，步骤如下： 计算初值——初始状态下散步的前向概率：$\alpha_1 (1)=π_1 b_1 (o_1 )=0.06$$\alpha_1 (2)=π_2 b_2 (o_1 )=0.24$ 递推计算——本质还是转移概率乘上发射概率：$\alpha_2 (1)=\sum_{j=1}^2[\alpha_1 (j) a_{j1} ] b_1 (o_2 )=(0.042+0.096)×0.4=0.0552$$\alpha_2 (2)=\sum_{j=1}^2[\alpha_1 (j) a_{j2} ] b_2 (o_2 )=(0.018+0.144)×0.3=0.0198$$\alpha_3 (1)=\sum_{j=1}^2[\alpha_3 (j) a_{j1} ] b_1 (o_3 )=(0.03864+0.00792)×0.5=0.02328$$\alpha_3 (2)=\sum_{j=1}^2[\alpha_3 (j) a_{j2} ] b_2 (o_3 )=(0.01656+0.01188)×0.5=0.01422$ 终止：$P(O│λ)=\sum_{i=1}^N[\alpha_T (i)]=0.03750 $ 后向计算后向计算与前向计算相类似，只是定义了后向概率，再次不多做赘述。 Baum-Welch算法假设给定训练数据只包含S个长度为T的观测序列O而没有对应的状态序列，目标是学习隐马尔可夫模型λ(A,B,π)的参数。我们将观测序列数据看作观测数据O，状态序列数据看作不可观测的隐数据I，那么隐马尔可夫模型事实上是一个含有隐变量的概率模型：$$P(O│λ)=\sum [P(O│I,λ)P(I|λ)]$$它的参数学习可以由EM算法实现。这里简单介绍一下EM算法，EM算法即Expectation Maximization期望最大算法。这个算法的引入可以从极大似然估计入手，极大似然估计是对于单分布问题通过已经发生的事件来估计概率模型中的位置参数，但事实上存在很多多分布问题，你只有最后的观测序列结果，但对于其具体的隐藏状态一无所知，比如经典的高斯混合模型。这个时候我们所采取的措施是先假设一组隐藏状态概率，然后进行极大似然估计，再用极大似然估计后的参数将原参数更新，这样不断迭代直至最后估计值收敛，具体的公式推导与证明只给明出处 ，在此并不细说。 确定完全数据的对数似然函数所有观测数据写成$O=(o_1,o_2,…,o_T)$,所有隐数据写成$I=(i_1,i_2,…,i_T)$，完全数据是$(O,I)=(o_1,o_2,…,o_T,i_1,i_2,…,i_T)$,。完全数据的对数似然函数是logP(O,I|λ)。 EM算法的E步：求Q函数$$Q(\lambda,\hat\lambda)=\sum_I[logP(O,I|\lambda)P(O,I|\hat\lambda)]$$其中，λ ̅是隐马尔可夫模型参数的当前估计值，λ是要极大化的隐马尔可夫模型参数。$$P(O,I│\lambda)=\pi_{i_1} b_{i_1} (o_1 ) a_{i_1 i_2 } b_{i_2} (o_2 )…a_{i_{T-1} i_T } b_{i_T} (o_T ) $$这个式子从左到右依次是初始概率、发射概率、转移概率、发射概率……于是函数Q可以写成：$$Q(\lambda,\hat\lambda)=\sum_I[log\pi_(i_1 ) P(O,I|\lambda)]\\+\sum_I(\sum_{t=1}^{T-1}[loga_{i_{t+1} \, i_t } ]) P(O,I│\hat\lambda)+\sum_I(\sum_{t=1}^Tlogb_{i_t}(o_t))P(O,I│\hat\lambda)$$式中求和都是对所有训练数据的序列总长度T进行的。这个式子是将$$P(O,I│λ)=π_{i_1} b_{i_1} (o_1 ) a_{i_1 i_2} b_{i_2} (o_2 )…a_{i_(T-1) \, i_T } b_{i_T} (o_T )$$代入$Q(\lambda ,\hat\lambda)=\sum_I logP(O,I\mid\lambda)P(O,I\mid\hat\lambda)$后，将初始概率、转移概率、发射概率这三部分乘积的对数拆分为对数之和，所以有三项。 EM算法的M步:极大化Q函数求模型参数λ(A,B,π)，由于要极大化的参数在Q函数表达式中单独地出现在3个项中，所以只需对各项分别极大化。第1项可以写成：$$\sum_Ilog\pi_{i_1} P(O,I|\hat\lambda)=\sum_{i=1}^Nlog\pi_i P(O,i_1=i|\hat\lambda)$$ 注意到$\pi_i$满足约束条件利用拉格朗日乘子法，写出拉格朗日函数。这个方法更简单明了的说法就是求条件极值，与微积分下册第五章第九节 所说内容几乎一致： $$\sum_{i=1}^Nlog\pi_i P (O,i_1=i\mid\hat\lambda)+\gamma(\sum_{i=1}^N\pi_i-1)$$ 对其求偏导数并令结果为0 $$\frac{\partial}{\partial\pi_i} [\sum_{i=1}^Nlog\pi_i P (O,i_1=i\mid\hat\lambda)+\gamma(\sum_{i=1}^N\pi_i-1)]=0 $$ 得到: $$P(O,i_1=i│\hat\lambda)+\gamma\pi_i=0 $$ 这个求导是很简单的，求和项中非i的项对π_i求导都是0，logπ的导数是1/π，γ那边求导就剩下π_i自己对自己求导，也就是γ。等式两边同时乘以π_i就得到了上式。 对i求和得到γ： $$\gamma=-P(O|\hat\lambda) $$ 代入$P(O,i_1=i│\hat\lambda)+\gamma\pi_i=0$中得到： $$\pi_i=\frac{P(o,i_1=i│\hat\lambda)}{P(O|\hat\lambda)} $$ 同理可以求得： $$a_{ij}=\frac{\sum_{i=1}^{T-1}P (O,i_t=i,i_{t+1}=j\mid\hat\lambda)}{\sum_{i=1}^{T-1}P (O,i_t=i\mid\hat\lambda)}$$$$b_j (k)=\frac{\sum_{i=1}^TP (O,i_t=j\mid\hat\lambda )I(o_t=v_(k))}{\sum_{i=1}^{T-1}P (O,i_t=i\mid\hat\lambda ) } $$ 预测算法维特比算法 维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）。这时一条路径对应着一个状态序列。 根据动态规划原理，最优路径具有这样的特性：如果最优路径在时刻t通过结点 ,那么这一路径从结点 到终点 的部分路径，对于从 到 的所有可能的部分路径来说，必须是最优的。因为假如不是这样，那么从 到 就有另一条更好的部分路径存在，如果把它和从 到达 的部分路径连接起来，就会形成一条比原来的路径更优的路径，这是矛盾的。依据这一原理，我们只需从时刻t=l开始，递推地计算在时刻t状态为i的各条部分路径的最大概率，直至得到时刻t=T状态为i的各条路径的最大概率。时刻t=T的最大概率即为最优路径的概率P,最优路径的终结点 也同时得到。之后，为了找出最优路径的各个结点，从终结点 开始，由后向前逐步求得结点 ,得到最优路径——这就是维特比算法 。 换一种更加形象易懂的说法，假设将我们所有的状态拉成一个n*T的图，或者说一个每层有n个神经元，总共有T层的全连接神经网络。现在每一层的每个节点都会有来自上一层n个节点的连接，但我们只取其中概率最大的那一条边，将其他边全部“失活”，这样每个连接层都只剩n条边存在，不断递推直到最后一层为止，取最后一层概率最大的边为整个最优路径的概率，并不断回推得到最优路径。 维特比算法的实例 我们再以之前天气的例子来说明前向算法，将HMM模型写成： $$A=\begin{bmatrix} 0.7 &amp; 0.3 \\ 0.4 &amp; 0.6 \\ \end{bmatrix} $$ $$B=\begin{bmatrix} 0.1 &amp; 0.4 &amp; 0.5 \\ 0.6 &amp; 0.3 &amp; 0.1 \\ \end{bmatrix} $$ $$π=[0.6,0.4]^T$$ $$O=（散步，购物，扫除）$$ 假如在（0.6,0.4）的初始状态下，我们知道了Bob在接下来三天里分别按顺序做散步，购物，扫除，现在我们想知道那边的天气大概是什么样，按照维特比算法计算，步骤如下： 初始化：在t=1时，对每个状态i，求i观测o_1为散步的概率δ_1 (i)：$δ_1 (1)=0.6×0.1=0.06$$δ_1 (2)=0.4×0.6=0.24$ 在t=2时，对每个状态i，i=1,2，求在t=1时状态为j观测为散步并在t=2时状态为i观测为购物的路径的最大概率，记录此最大概率为δ_2 (i)，则：$δ_2 (i)=max⁡[\delta_1 (j) a_{ji}]b_i (o_2) \qquad (1≤j≤2)$ 通过该公式不断递推计算，我们可以很清楚的得到如下这张路径图：所以最优路径为（晴天，雨天，雨天）发生概率为（即最大概率）为0.01344]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>大二</tag>
        <tag>概率模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程入门]]></title>
    <url>%2Fpost%2F13592.html</url>
    <content type="text"><![CDATA[关于Java多线程的简单入门知识其中最为重要的就是协调不同线程驱动的任务之间对资源的使用（刚整理这篇的时候还没学OS，等OS学完后再回来修改&amp;对比总结一下）希望我能成功填坑orz 并发的概念 进程：每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含1–n个线程。（进程是资源分配的最小单位） 线程：同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小。（线程是cpu调度的最小单位） 更快的执行： 并行可以将任务分解到多个CPU上执行，但并发通常是提高运行在单处理器上的程序的性能。为什么实际运用会这么反直觉？——因为阻塞，当某个任务因为程序控制范围之外的条件（比如I/O）不能继续执行时，如果没有并发，那么整个主进程都将因此停止下来，直到外部条件发生变化。 Java的并发： Java的并发系统与操作系统不同，会共享类如内存和I/O这样的资源，因此Java多线程最基本的困难就在于协调不同线程驱动的任务之间对资源的使用，以使得这些资源不会同时被多个任务访问。 Java的线程机制是在由执行程序表示的单一进程中创建任务。 对于资源各个线程是抢占式的，调度机制会周期性地中断线程，将上下文切换到另一个线程。 多线程实现 继承Thread类 1234567891011121314151617181920212223242526class Thread1 extends Thread&#123; private String name; public Thread1(String name) &#123; this.name = name; &#125; public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(name + "运行: " + i); try &#123; sleep((int) Math.random() * 10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Thread1 mTh1=new Thread1("A"); Thread1 mTh2=new Thread1("B"); mTh1.start(); mTh2.start(); &#125;&#125; 程序启动运行main时候，java虚拟机启动一个进程，主线程main在main()调用时候被创建。随着调用两个对象的start方法，启动另外两个线程，这样整个应用就在多线程下成功运行了。注意：start()方法的调用后并不是立即执行多线程代码，而是使得该线程变为可运行态（Runnable），什么时候运行是由系统决定的。 实现java.lang.Runnable接口 12345678910111213141516171819202122232425class Thread2 implements Runnable&#123; private String name; public Thread2(String name) &#123; this.name=name; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(name + "运行 : " + i); try &#123; Thread.sleep((int) Math.random() * 10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; new Thread(new Thread2("C")).start(); new Thread(new Thread2("D")).start(); &#125;&#125; Thread2类通过实现Runnable接口，使得该类有了多线程类的特征。run（）方法是多线程程序的一个约定。所有的多线程代码都在run方法里面。在启动的多线程的时候，需要先通过Thread类的构造方法Thread(Runnable target) 构造出对象，然后调用Thread对象的start()方法来运行多线程代码。 实际上所有的多线程代码都是通过运行Thread的start()方法来运行的。因此，不管是扩展Thread类还是实现Runnable接口来实现多线程，最终还是通过Thread的对象的API来控制线程的。实现Runnable接口比继承Thread类所具有的优势： 适合多个相同的程序代码的线程去处理同一个资源 可以避免java中的单继承的限制 增加程序的健壮性，代码可以被多个线程共享，代码和数据独立 线程池只能放入实现Runable或callable类线程，不能直接放入继承Thread的类 线程状态切换 新建状态（New）：新创建了一个线程对象。 就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。 运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。 阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： 等待阻塞：运行的线程执行wait()方法，JVM会把该线程放入等待池中。(wait会释放持有的锁) 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。 其他阻塞：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。（注意,sleep是不会释放持有的锁） 死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。 线程调度 调整线程优先级：优先级高的线程会获得较多的运行机会。Java线程的优先级用整数表示，取值范围是1~10Thread类有以下三个静态常量： static int MAX_PRIORITY;//线程可以具有的最高优先级，取值为10。static int MIN_PRIORITY;//线程可以具有的最低优先级，取值为1。static int NORM_PRIORITY;//分配给线程的默认优先级，取值为5。 Thread类的setPriority()和getPriority()方法分别用来设置和获取线程优先级。每个线程都有默认优先级，主线程的默认优先级为Thread.NORM_PRIORITY线程的优先级有继承关系，比如A线程中创建了B线程，那么B将和A具有相同的优先级。JVM提供了10个线程优先级，但与常见的操作系统不能很好的映射。如果希望程序能移植到各个操作系统中，应该仅仅使用Thread类有以下三个静态常量作为优先级，这样能保证同样的优先级采用了同样的调度方式。 线程睡眠：Thread.sleep(long millis)方法，使线程转到阻塞状态。millis：设定睡眠的时间(ms)当睡眠结束后，转为就绪（Runnable）状态。 线程等待：Object类中的wait()方法，导致当前的线程等待，直到其他线程调用此对象的notify()方法或notifyAll()唤醒方法。这个两个唤醒方法也是Object类中的方法，行为等价于调用wait(0)一样。 线程让步：Thread.yield()方法，暂停当前正在执行的线程对象，把执行机会让给相同或者更高优先级的线程。 线程加入：join()方法，等待其他线程终止。在当前线程中调用另一个线程的join()方法，则当前线程转入阻塞状态，直到另一个进程运行结束，当前线程再由阻塞转为就绪状态。 线程唤醒：Object类中的notify()方法，唤醒在此对象监视器等待的单个线程。如果所有线程都在此对象上等待，则会选择唤醒其中一个线程（选择是任意性的，并在对实现做出决定时发生）线程通过调用其中一个 wait 方法，在对象的监视器上等待。 直到当前的线程放弃此对象上的锁定，才能继续执行被唤醒的线程。被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争。类似的方法还有一个notifyAll()，唤醒在此对象监视器上等待的所有线程。 常用函数 Thread.sleep(long millis)：在指定的毫秒数内让当前正在执行的线程休眠（暂停执行） Thread.join()：指等待该线程终止。该线程是指的主线程等待子线程的终止。也就是在子线程调用了join()方法后面的代码，只有等到子线程结束了才能执行。在很多情况下，主线程生成并起动了子线程，如果子线程里要进行大量的耗时的运算，主线程往往将于子线程之前结束，但是如果主线程处理完其他的事务后，需要用到子线程的处理结果，也就是主线程需要等待子线程执行完成之后再结束，这个时候就要用到join()方法了。 Thread.yield()：暂停当前正在执行的线程对象，并执行其他线程。yield()应该做的是让当前运行线程回到可运行状态，以允许具有相同优先级的其他线程获得运行机会。因此，使用yield()的目的是让相同优先级的线程之间能适当的轮转执行。但是，实际中无法保证yield()达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。 Thread.setPriority(): 更改线程的优先级。 Obj.wait()：暂停当前线程，释放CPU控制权，释放对象锁的控制。与Obj.notify()必须要与synchronized(Obj)一起使用，也就是wait,与notify是针对已经获取了Obj锁进行操作： 从语法角度来说就是Obj.wait(),Obj.notify必须在synchronized(Obj){…}语句块内。 从功能上来说wait就是说线程在获取对象锁后，主动释放对象锁，同时本线程休眠。直到有其它线程调用对象的notify()唤醒该线程，才能继续获取对象锁，并继续执行。但有一点需要注意的是notify()调用后，并不是马上就释放对象锁的，而是在相应的synchronized(){}语句块执行结束，自动释放锁后，JVM会在wait()对象锁的线程中随机选取一线程，赋予其对象锁，唤醒线程，继续执行。 线程类方法： sleep(): 强迫一个线程睡眠Ｎ毫秒。isAlive(): 判断一个线程是否存活。join(): 等待线程终止。activeCount(): 程序中活跃的线程数。enumerate(): 枚举程序中的线程。currentThread(): 得到当前线程。isDaemon(): 一个线程是否为守护线程。setDaemon(): 设置一个线程为守护线程。(用户线程和守护线程的区别在于，是否等待主线程依赖于主线程结束而结束)setPriority(): 设置一个线程的优先级。 线程同步 synchronized关键字的作用域： 是某个对象实例内，synchronized aMethod(){}可以防止多个线程同时访问这个对象的synchronized方法（如果一个对象有多个synchronized方法，只要一个线程访问了其中的一个synchronized方法，其它线程不能同时访问这个对象中任何一个synchronized方法）。这时，不同的对象实例的synchronized方法是不相干扰的。也就是说，其它线程照样可以同时访问相同类的另一个对象实例中的synchronized方法； 是某个类的范围，synchronized static aStaticMethod{}防止多个线程同时访问这个类中的synchronized static 方法。它可以对类的所有对象实例起作用。 synchronized关键字是不能继承的，继承类需要你显式的指定它的某个方法为synchronized方法； 线程同步的TIPS 线程同步的目的是为了保护多个线程反问一个资源时对资源的破坏。 线程同步方法是通过锁来实现，每个对象都有且仅有一个锁，这个锁与一个特定的对象关联，线程一旦获取了对象锁，其他访问该对象的线程就无法再访问该对象的其他非同步方法。 对于静态同步方法，锁是针对这个类的，锁对象是该类的Class对象。静态和非静态方法的锁互不干预。一个线程获得锁，当在一个同步方法中访问另外对象上的同步方法时，会获取这两个对象锁。 对于同步，要时刻清醒在哪个对象上同步，这是关键。 编写线程安全的类，需要时刻注意对多个线程竞争访问资源的逻辑和安全做出正确的判断，对“原子”操作做出分析，并保证原子操作期间别的线程无法访问竞争资源。 当多个线程等待一个对象锁时，没有获取到锁的线程将发生阻塞。 代码示例创造一个Object Lock来充当锁，用Lock类的变量lockon作为是否循环wait()语句的条件：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class Lock&#123; int lockon; Lock()&#123; this.lockon = 1; &#125;&#125;class ThTest implements Runnable&#123; Lock lock; ThTest(Lock lock)&#123; this.lock = lock; &#125; public void run() &#123; synchronized(lock) &#123; while(lock.lockon==1) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(lock.lockon+"Lock is down. Next it will be on."); try &#123; Thread.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.lockon = 1; lock.notify(); &#125; &#125;&#125;class ThRest implements Runnable&#123; Lock lock; ThRest(Lock lock)&#123; this.lock = lock; &#125; public void run() &#123; synchronized(lock) &#123; while(lock.lockon==0) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(lock.lockon+"Lock is on. Next it will be down."); try &#123; Thread.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.lockon = 0; lock.notify(); &#125; &#125;&#125;public class Test8_3 &#123; public static void main(String args[]) &#123; Lock lock = new Lock(); ThTest thtest = new ThTest(lock); ThRest threst = new ThRest(lock); new Thread(thtest).start(); new Thread(threst).start(); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大二</tag>
        <tag>Java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradient Descent]]></title>
    <url>%2Fpost%2F17408.html</url>
    <content type="text"><![CDATA[梯度下降算法是机器学习领域中非常常用的优化算法。本文通过对梯度下降算法、AdaGrad算法、SGD算法、动量算法以及牛顿动量算法的介绍，将比较基础的梯度下降算法变种介绍给读者。 目的对于一个含参的函数，通过不断求解当前点的梯度值，并以该梯度值成负比例的步长不断更新所求点，以求解出使其函数值最小的一组参数。 思想对于一个含参的函数，以与函数在当前点梯度（近似）成负比例的步长来不断更新该参数，使其不断向该参数偏导为0的点逼近，以找到函数的局部最小值。 当我们从损失函数的某一点出发，在该点附近做出以常数η为参数的一个邻域，根据微积分所学的知识可知，我们在该邻域上一定可以找出一个找到极小值。我们以该极小值点为下一步我们需要跟进的点，更新之后我们再在该点附近做出以常数η为参数的一个邻域并重复以上过程。通过这样的不断更新，我们理论上一定可以找出一个全局的极小值点或者是导数为0的驻点。关于在某一邻域上如何找到该邻域上的局部最小值点。我们可以通过泰勒展开，该邻域上的损失函数可以近似写成 $L(\theta)=L(a,b)+u(\theta_1-a)+v(\theta_2-b)$也就是我们所熟知的全微分方程。令 $\Delta\theta_1=\theta_1-a;\Delta\theta_2=\theta_2-b$ 因为L(a,b)为一个常数，所以L的极小值直接取决于 u+ v同时我们可以将其书写成向量的形式$(\Delta\theta_1,\Delta\theta_2)·(u,v)$显然$(\Delta\theta_1,\Delta\theta_2)$与(u,v)反向时L最小，即$$ \begin{bmatrix} \Delta\theta_1 \\ \Delta\theta_2 \end{bmatrix}=-\eta \begin{bmatrix} u \\ v \end{bmatrix}$$其中u和v分别为函数的偏导数值且η取值越小该结果越准确。 实现用均方误差构造一个以函数f为自变量的二元损失函数 ：(因为f = w*x + b所以也可以看出以w和b为自变量的函数)1L(f)=L(w,b)=((y - (w * x + b))**2).sum()/len(x) 首先考虑只有一个参数 w 的损失函数，随机的选取一个初始点，计算此时 L 对 w 的微分，然后顺着切线下降的方向更改 w 的值1w = w - grad(w)*learning_rate 此后，w会不断靠近微分近似为0的点以达成目标。同样方法求出所有参数的值。而实际上我们所需要的是不断同时更新损失函数所需要的所有参数的值，我们在编程中只需要让所有参数的更新在同一次迭代过程中更新即可，因为当迭代次数足够多的时候，每一次的迭代都可以看成在很短的一个小时间段中进行，因此每次迭代的不同参数更新可以近似看成时在同一时间段所更新的，即达到了我们要求梯度下降同时更新损失函数中含有的所有参数的要求。 问题学习率的不确定性所带来的一系列问题，比如若学习率过大，导致每次更新的步长过大，而过大的步长有可能并不符合实际更新的情况，导致了过大的学习率会使不断更新的参数在最值点上方震荡，甚至直接跨过最值点使损失函数值不断增大，无法逼近最值点；如果学习率过小，尽管较小的学习率符合我们梯度下降的数学原理，可以求得较为准确的符合我们预期的参数值。但是同时，过小的学习率会使更新的速度较慢，在计算中往往导致浪费计算机时间与性能，在我们的日常实际生活运用中可能会等不及出结果。 优化思路通常刚开始，初始点会距离极值点比较远，所以使用大一点的学习率。而更新好几次参数之后呢，此时的参数点比较靠近最低点，故而可适当减小学习率。即随着次数的增加，使学习率的大小与更新次数呈负相关，例如采取将学习率除以次数加一的开根号$lr/\sqrt{t+1} \quad$等方法。 优化算法AdaGrad算法（适应性梯度算法）：每个参数的学习率都除上之前该参数所有微分的均方根[。为每一个参数保留一个学习率以提升在稀疏梯度上的性能。12lr_b += b_grad ** 2b -= lr/np.sqrt(lr_b) * b_grad 适应性梯度算法中的学习率($\eta_t/\sigma_t$)由两部分组成，其中 $\eta_t=\eta/\sqrt{t+1} \quad$即随着迭代次数的增加，不断削减学习率使每次更新的步长不断变小。$\sigma_t=\sqrt{\frac{\sum_{i=1}^t(g^i)^2\quad}{(t+1)}} \quad$即之前该参数所有微分和的均方根，用此作为分母用意在：如果走到当前点的微分过小，可以控制学习率让其步长适当增大一点；如果走到当前点的微分过大，通过控制学习率使其步长适当减小一点。如此，得到我们的学习率$\frac{\eta_t}{\sigma_t}=\frac{\eta}{\sqrt{\sum_{i=1}^t(g^i)^2\quad} \quad}g^t$接下来给出更本质的解释：适应性梯度算法中学习率近似于(|一阶导数|/二阶导数)。对于只有一个自变量的函数，我们可以用其一阶导数来表示该点的下降速率。但是对于有多个自变量的函数，每更新一次，所选取合适点的标准如果只有一阶导数唯一一个衡量标准显然是不合适的。而二次微分可以在一定程度上反映出当前点到偏导为0的驻点的距离。综合考虑着这两个因子，可以较好地提供出一个符合我们期望的学习率。而对于二次微分，我们用一次微分的均方根来表示，它可以在一定程度上反映二次微分的大小。例如如果二次微分较小，则一次微分图像斜率较缓，那么一次微分的均方根相对而言会较小。适应性梯度算法最好的举例便是一元二次函数 ，其最佳步长为 即|2ax+b|/2a上下分别是该一元二次函数的一次微分和二次微分。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122import matplotlib.pyplot as pltimport numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport randomw_init = 4.2b_init = -150def f(x): return w_init * x + b_initdef initDate(): x_data = [] y_data = [] for i in range(10): x = random.randint(-99,99) x_data.append(x) y_data.append(f(x)+f(x)/10*random.randint(-1,1)) # 给数据加点噪声，使过程更加真实一点 return x_data,y_datadef exhaustion(x_data,y_data): # 通过穷举法试探出使均方误差最小的一组b&amp;w x = np.arange(-abs(1.5*b_init),abs(1.5*b_init),abs(b_init)/20) #bias Y= np.arange(-abs(1.5*w_init),abs(1.5*w_init),abs(w_init)/20) #weight Z = np.zeros((len(x),len(y))) X,Y = np.meshgrid(x,y) #return两个矩阵,X的行向量是向量x的简单复制,Y的列向量是向量y的简单复制 x_data = np.array(x_data) y_data = np.array(y_data) for i in range(len(x)): for j in range(len(y)): b = x[i] w = y[j] Z[j][i] = ((y_data - b - w * x_data)**2).sum()/len(x_data) # Z值最小的那一组x[i]y[j]就是我们所期望的使f值最小的b，w return X,Y,Zdef gradientDescent(x_data,y_data): #initial: b = 0 w = 0 lr = 1 # learning rate iteration = 100000 b_history = [b] w_history = [w]# 我们实际用gandient descent求出的b&amp;w lr_b = 0 lr_w = 0 for i in range(iteration): #不断向偏导为0的驻点靠拢，以获取均方误差最小的一组解w、b x = np.array(x_data) y = np.array(y_data) b_grad = -2.0*(y - b - w*x) w_grad = -2.0*(y - b - w*x)*x b_grad = b_grad.sum()/len(x) w_grad = w_grad.sum()/len(x) lr_b += b_grad ** 2 lr_w += w_grad ** 2 #update parameters: b -= lr/np.sqrt(lr_b) * b_grad w -= lr/np.sqrt(lr_w) * w_grad #store parameters for plotting: b_history.append(b) w_history.append(w) return b_history,w_historydef getPicture(b_history,w_history,X,Y,Z): plt.figure('gradient_descent') plt.contourf(X,Y,Z,50,alpha=0.5,cmap=plt.get_cmap('jet')) plt.plot([b_init],[w_init],'x',ms=12,markeredgewidth=3,color='orange') plt.plot(b_history,w_history,'o-',ms=3,lw=1.5,color='black') plt.xlim(-abs(1.5*b_init),abs(1.5*b_init)) plt.ylim(-abs(1.5*w_init),abs(1.5*w_init)) plt.xlabel(r'$b$',fontsize=16) plt.ylabel(r'$w$',fontsize=16)def get3D(b_history,w_history,X,Y,Z): #为了画好看的三维图并考虑到memory error强行又弄了组数据 b_h = b_history[::1000] w_h = w_history[::1000] B,W = np.meshgrid(b_h,w_h) Q = np.zeros(len(b_h)) for i in range(len(b_h)): for n in range(len(x_data)): Q[i] = Q[i] + (y_data[n] - b_h[i] - w_h[i] * x_data[n])**2 Q[i] = Q[i]/len(x_data) # 均方误差 ax = Axes3D(plt.figure('三维图')) ax.plot_surface(X,Y,Z,cmap = 'rainbow') ax.plot([b_init],[w_init],'x',ms=12,markeredgewidth=3,color = 'orange') ax.plot(b_h,w_h,Q,'o-',ms=3,lw=1.5,color='black') ax.set_xlabel('--b--') ax.set_ylabel('--w--') ax.set_zlabel('--z--') ax.set_title('3D')def initPicture(b,w): plt.figure('initial') x = np.linspace(-99,99) y_init = f(x) # 所求目标函数的函数值 y_grad = w*x+b # 梯度下降求出的函数所对应的函数值 plt.plot(x,y_init,'.') plt.plot(x,y_grad)if __name__ == '__main__': x_data,y_data = initDate() # 获取实验数据 X,Y,Z = exhaustion(x_data,y_data) # X&amp;Y为网格图，Z为其函数值 b_history,w_history = gradientDescent(x_data,y_data) b = b_history[-1];print(b) w = w_history[-1];print(w) getPicture(b_history,w_history,X,Y,Z) # 绘制图像 get3D(b_history,w_history,X,Y,Z) initPicture(b,w) plt.show() SGD算法（Stochastic Gradient Descent随机梯度下降算法）：随机梯度下降算法与基础的梯度下降极为相似，唯一的不同点在于它是从某一个样本点出发而并非像梯度下降算法那样每次都考虑整体的数值同时更新。随机梯度下降算法的优点是很明显的，计算的速度和效率要比普通的梯度下降快得多。而这样所带来的缺点就是随机梯度下降算法的路线可能会不断抖动，并且可能优化过程充满震荡，但同时，正是因为这种大幅度的震荡，有时可以解决大部分梯度下降算法都会面临的一个绝对难题，即“陷进”局部最小值点。大部分梯度下降算法在优化过程中很可能会被困在某个驻点，而随机梯度下降算法有时可以通过优化过程的抖动而逃离当前的局部最小值点。这也是很多自适应优化算法在实际训练出的结果都不如随机梯度下降算法的原因。 Momentum算法（动量算法）：momentum即动量，该算法在普通的梯度下降中引入了动量这个因子是因为普通的梯度下降算法，在每一次更新完成后在当前点的附近邻域上所求得的局部最小值点的方向很大可能是和之前一步的方向几乎相反的，这就导致了普通梯度下降算法在更新当前点时很容易不断震荡，效率低下不符合我们的期望。由此我们采用了动量算法，即增加一个变量v表示速度更新，在每次参数更新的时候加上计算出的当前的速度更新值v= γ· v - η·grad（其中参数γ为衰减权重，一般为0.9，可以让早期的梯度对当前梯度的影响越来越小）其中这个0.9倍上一个点的梯度下降方向就表示了当前该点的动量，让该动量与当前点的梯度方向做矢量和。之后将原参数更新θ = θ + v即可。这样每次更新就会多更新一部分上一次迭代的更新量，来平滑这一次迭代的梯度。从物理的角度来解释，就像是一个小球在滚落的过程中会受其自身的历史动量所影响，所以才称为动量算法。动量算法中学习率越大（实际情况中一般学习率比较小，例如0.001），当前梯度对现在更新的影响也就越大；v中含有所有速度更新值，可以反映历史时刻梯度方向。而由于动量积攒了历史的梯度，如点P前一刻的梯度与当前的梯度方向几乎相反。因此原本在P点原本要大幅徘徊的梯度，主要受到前一时刻的影响，而导致在当前时刻的梯度幅度减小。要是当前时刻的梯度与历史时刻梯度方向相似，这种趋势在当前时刻则会加强；要是不同，则当前时刻的梯度方向减弱。 NAG算法（牛顿动量算法Nesterov accelerated gradient）：由之前所说，动量算法每下降一步都是由前面下降方向的一个累积和当前点的梯度方向组合而成。那么既然每一步都要将两个梯度方向（历史梯度、当前梯度）做一个合并再下降，那为什么不先按照历史梯度往前走那么一小步，按照前面一小步位置的“超前梯度”来做梯度合并呢？所以牛顿动量算法和动量算法唯一的区别就在于它的梯度不是根据当前参数位置，而是根据先走了一步本来计划要走的一步后达到参数的位置计算出来的。具体做法为先临时更新θ’ = θ + γ· v然后计算临时点的梯度grad’并计算出速度更新v= γ· v - η·grad’最后应用更新θ = θ + v即可。牛顿动量算法其实是相比于动量算法多考虑了本次梯度相对于上次梯度的变化量，而这个变化量本质是对于目标函数二阶导的近似。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>大二</tag>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sort]]></title>
    <url>%2Fpost%2F56837.html</url>
    <content type="text"><![CDATA[常见的nlogn排序算法介绍与C语言实现：快速排序，希尔排序，归并排序，堆排序，排序树 快排（nlogn）：将列表以首元(a[0])为判断依据隔成两部分,第一部分全部小于首元，第二部分全部大于等于首元。（复杂度n）重复上述过程至所有元素（次数为logn） void quick_sort(int a[],int low,int high); int find_pos(int a[],int low,int high); 希尔（nlogn）：将列表以不同的步长进行划分，常用n/2不断划分至步长为1。每次划分后将各子列表排序即可。希尔复杂度最优时为1.3n，分析起来好像有点复杂orz void shell_sort(int a[],int n); void step_wise(int a[],int D,int n); 归并（nlogn）：将列表不断二分成两份列表，当分成每个子列表的长度都为1时显然每个子列表都有序（次数logn），然后将有序的两个子列表不断合并并使之有序（复杂度n） void merge_sort(int a[], int b[], int start, int end); void Merge(int a[],int b[], int start, int mid, int end); 堆排（nlogn）：大顶堆（二叉树）的性质是父节点大于子节点，先通过这个性质建个堆。（tips：建堆的时候用完全二叉树的性质a[i]d的左子树为a[2*i]，所以存值从a[1]开始，a[0]用来暂存值就好啦）此时a[1]一定是最大数，将a[1]（堆顶）与末元a[n-1]（最后一个叶节点）交换后将a[n-1]忽略，不进入下次堆排（n–）再将堆的性质恢复（logn），依次进行至堆顶（n） void heap_sort(int a[],int n); void sift(int a[],int r,int n); 排序树（nlogn）：右节点&gt;父节点&gt;左节点，建立后将该二叉树中序遍历即可。 void tree_sort(int a[],int n); void setting(TNode* tree,int x); void tree_order(TNode* tree); 基数排序 (d(n+r))：依次按不同位数（d）进行关键字的比较（需要构建队列（队列大小r）） 以下附实现的C语言代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179#include &lt;stdio.h&gt; #include&lt;stdlib.h&gt;#include&lt;time.h&gt; typedef struct node&#123; int data; struct node* lc; struct node* rc;&#125;TNode;void quick_sort(int a[],int low,int high);int find_pos(int a[],int low,int high);void shell_sort(int a[],int n);void step_wise(int a[],int D,int n);void choose_sort(int a[],int n);void merge_sort(int a[], int b[], int start, int end);void Merge(int a[],int b[], int start, int mid, int end);void heap_sort(int a[],int n);void sift(int a[],int r,int n);void tree_sort(int a[],int n); TNode* secrach(TNode* tree,int x);void setting(TNode* tree,int x);void tree_order(TNode* tree);int main()&#123; int a[10];int b[10]; srand( (unsigned int)(time(0)) ); for(int i=0;i&lt;10;i++)&#123; a[i]=rand()%100+1; printf("%02d\t",a[i]); &#125;printf("\n"); //quick_sort(a,0,9); //shell_sort(a,10); //choose_sort(a,10); //merge_sort(a,b,0,9); //heap_sort(a,10); //tree_sort(a,10); for(int i=0;i&lt;10;i++) printf("%02d\t",a[i]); &#125;void quick_sort(int a[],int low,int high)&#123; if(low&gt;=high) return; int idx = find_pos(a,low,high); quick_sort(a,low,idx-1); quick_sort(a,idx+1,high);&#125;int find_pos(int a[],int low,int high)&#123; int temp = a[low]; while(low&lt;high)&#123; while(low&lt;high&amp;&amp;a[high]&gt;temp) high--; a[low] = a[high]; while(low&lt;high&amp;&amp;a[low]&lt;=temp) low++; a[high] = a[low]; &#125; a[low] = temp; return low;&#125;void shell_sort(int a[],int n)&#123; int d = n/2; while(d&gt;=1)&#123; step_wise(a,d,n); d = d/2; &#125;&#125;void step_wise(int a[],int D,int n)&#123; for(int d=0;d&lt;D;d++)&#123; for(int i=d+D;i&lt;n;i=i+D)&#123; int temp = a[i];int j=i; for(j=i;j-D&gt;=0;j=j-D)&#123; if(temp&lt;a[j-D]) a[j] = a[j-D]; else break; &#125; a[j] = temp; &#125; &#125;&#125;void choose_sort(int a[],int n)&#123;//选择排序 for (int i = 0;i&lt;n;i++)&#123; for (int j = i;j&lt;n;j++)&#123; if (a[j]&lt;a[i])&#123; int ex = a[i]; a[i] = a[j]; a[j] = ex; &#125; &#125; &#125;&#125;void Merge(int a[],int b[], int start, int mid, int end)&#123;//合并a、b数组并排序 int i = start, j=mid+1, k = start; while(i!=mid+1 &amp;&amp; j!=end+1)&#123; if(a[i] &gt; a[j]) b[k++] = a[j++]; else b[k++] = a[i++]; &#125; while(i != mid+1) b[k++] = a[i++]; while(j != end+1) b[k++] = a[j++]; for(i=start; i&lt;=end; i++) a[i] = b[i];&#125;void merge_sort(int a[], int b[], int start, int end)&#123; if(start &lt; end)&#123; int mid = (start + end) / 2; merge_sort(a, b, start, mid); merge_sort(a, b, mid+1, end); Merge(a, b, start, mid, end); &#125;&#125;void sift(int p[],int r,int n)&#123;//树根p[r]（可能是子树的树根）最值性质被破坏的堆 int k = 2*r; int temp = p[r]; while(k&lt;=n)&#123;//尽力将p[r]沉到最底，以确保整体性质的正确 if(k+1&lt;n &amp;&amp; p[k+1]&gt;p[k]) k++;//防止k+1越界 if(k==n || temp&gt;=p[k]) break; p[r] = p[k];r = k;//先不急交换完p[r]，我们看看p[r]最后能沉到哪 k = 2*r; &#125; p[r] = temp; &#125;void heap_sort(int a[],int n)&#123; n = n+1;int p[n]; for(int i=1;i&lt;n;i++) p[i]=a[i-1]; for(int i=n/2;i&gt;=1;i--) sift(p,i,n); for(int i=n-1;i&gt;=2;i--)&#123; p[0] = p[1]; p[1] = p[i]; p[i] = p[0]; sift(p,1,i-1); &#125; for(int i=1;i&lt;n;i++) a[i-1]=p[i]; &#125;void tree_sort(int a[],int n)&#123; TNode* tree = (TNode*)malloc(sizeof(TNode)); tree-&gt;lc=NULL;tree-&gt;rc=NULL;tree-&gt;data=a[0]; for(int i=1;i&lt;n;i++) setting(tree,a[i]); tree_order(tree);&#125;TNode* secrach(TNode* tree,int x)&#123; TNode* p; while(tree!=NULL)&#123; if(x&lt;tree-&gt;data)&#123; p=tree;tree=tree-&gt;lc; &#125; else&#123; p=tree;tree=tree-&gt;rc; &#125; &#125; return p;&#125;void setting(TNode* tree,int x)&#123; TNode* p = secrach(tree,x); TNode* q = (TNode*)malloc(sizeof(TNode)); q-&gt;data=x;q-&gt;lc=NULL;q-&gt;rc=NULL; if(p==NULL)&#123; p=q;return; &#125; if(x&lt;p-&gt;data) p-&gt;lc=q; else p-&gt;rc=q; return;&#125;void tree_order(TNode* tree)&#123; if(tree==NULL) return; tree_order(tree-&gt;lc); printf("%02d\t",tree-&gt;data); tree_order(tree-&gt;rc);&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小生成树]]></title>
    <url>%2Fpost%2F57313.html</url>
    <content type="text"><![CDATA[Prime：从点0开始不断拉相邻权重最小且不构成回路的点进来Kruskal：在原图中不断找权重最小的边“记录在案”，保证不构成回路 Prime算法（稠密矩阵；邻接矩阵）从点0开始不断拉相邻权重最小且不构成回路的点进来（形成一个超点） 1234567891011121314151617181920212223242526272829303132​void prime(int G[N][N])&#123; Edge edges[N]; for(int i=0;i&lt;N;i++)&#123; edges[i].vex = 0;//表示"0"到各个节点 edges[i].cost = G[0][i];//距离 &#125; edges[0].cost = 0;//距离"0"表示不可达 printf("%d ",edges[0].vex); for(int a=1;a&lt;N;a++)&#123; int k = select(edges); printf("%d(last: %d) ",k,edges[k].vex); edges[k].cost = 0; for(int i=0;i&lt;N;i++)&#123; if(G[k][i]&lt;edges[i].cost)&#123; edges[i].vex = k;//表示到i这个点是从k走更近一点 edges[i].cost = G[k][i]; &#125; &#125; &#125;&#125;int select(Edge edges[])&#123; int val = INT_MAX;int idx = -1; for(int i=0;i&lt;N;i++)&#123; if(edges[i].cost&lt;val&amp;&amp;edges[i].cost&gt;0)&#123; val = edges[i].cost; idx = i; &#125; &#125; return idx;&#125;​ Kruskal（稀疏矩阵；邻接表）在原图中不断找权重最小的边“记录在案”，保证不构成回路（通过与标记过的点不属于同一连通分量gno实现） 123456789101112131415161718192021​void kruskal(TGraph* G)&#123; heapsort(G);//稍微排个序 int idx=1;int num=0; for(int i=1;i&lt;G-&gt;nv;i++)&#123; while(1)&#123; if(G-&gt;pv[G-&gt;pe[idx].va].gno!=G-&gt;pv[G-&gt;pe[idx].vb].gno)&#123; int a=G-&gt;pv[G-&gt;pe[idx].va].gno,b=G-&gt;pv[G-&gt;pe[idx].vb].gno; for(int j=1;j&lt;idx;j++)&#123; if(G-&gt;pv[G-&gt;pe[j].va].gno==a) G-&gt;pv[G-&gt;pe[j].va].gno=b; if(G-&gt;pv[G-&gt;pe[j].vb].gno==a) G-&gt;pv[G-&gt;pe[j].vb].gno=b; &#125; G-&gt;pv[G-&gt;pe[idx].va].gno=b; printf("%d-%d ",G-&gt;pe[idx].va,G-&gt;pe[idx].vb);break; &#125; else idx++; &#125; idx++; &#125;&#125;​]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的最短路径]]></title>
    <url>%2Fpost%2F11574.html</url>
    <content type="text"><![CDATA[Dijkstra : 从点v0开始，不断把距V0距离最小的点拉进超点（权重值一定要是正的）Floyd : 如果i-&gt;k-&gt;j的距离比直接i-&gt;j要短的话，更新一下dist的距离和path的路径 Dijkstra从点v0开始，不断把距V0距离最小的点拉进超点（注意：权重值一定要是正的） 1234567891011121314151617181920212223242526272829​void dijkstra(int G[N][N],int v0,int path[],int s[])&#123; int dist[N]; for(int i=0;i&lt;N;i++)&#123; dist[i]=G[v0][i]; if(dist[i]&lt;INT_MAX) path[i]=v0;//记录前驱，即“0” else path[i] = -1; &#125;dist[v0]=-1; for(int a=0;a&lt;N;a++)&#123; int idx=-1,min=INT_MAX; for(int i=0;i&lt;N;i++)&#123; if(dist[i]&gt;0&amp;&amp;dist[i]&lt;min)&#123; idx = i;min = dist[i]; &#125; &#125; if(idx&lt;0) break;s[idx]=min; dist[idx] = -1; for(int i=0;i&lt;N;i++)&#123; if(dist[i]&gt;0&amp;&amp;G[idx][i]!=INT_MAX&amp;&amp;min+G[idx][i]&lt;dist[i])&#123; dist[i]=min+G[idx][i]; path[i] = idx; &#125; &#125; &#125;&#125;void print(int path[N],int s[N])&#123; for(int i=0;i&lt;N;i++) if(path[i]&gt;=0) printf("node :%2d(pre:%2d) s:%d \n",i,path[i],s[i]);&#125;​ Floyd如果i-&gt;k-&gt;j的距离比直接i-&gt;j要短的话，更新一下dist的距离和path的路径。 （特别注意：关于循环的顺序，k一定在最外层循环，不然程序会出现差错，以下给出解释：k在最外层保证了每次k变动后会遍历图上所有的点以达成完备的更新。） 1234567891011121314151617181920212223​void dist_init(int dist[N][N])&#123; freopen("SP_edge.txt","r",stdin); for(int i=0;i&lt;N;i++) for(int j=0;j&lt;N;j++) dist[i][j]=INT_MAX; int va,vb,cost; while(scanf("%d %d %d\n",&amp;va,&amp;vb,&amp;cost)==3) dist[va][vb] = cost; for(int i=0;i&lt;N;i++) dist[i][i] = 0;&#125;void path_init(int path[N][N])&#123; for(int i=0;i&lt;N;i++) for(int j=0;j&lt;N;j++) path[i][j]=i;&#125;void floyd(int dist[N][N],int path[N][N])&#123; for(int k=0;j&lt;N;j++)&#123; for(int i=0;i&lt;N;i++)&#123; for(int j=0;k&lt;N;k++)&#123; if(dist[i][k]+dist[k][j]&lt;dist[i][j]&amp;&amp;dist[i][k]!=INT_MAX&amp;&amp;dist[k][j]!=INT_MAX)&#123; dist[i][j]=dist[i][k]+dist[k][j];path[i][j]=k; &#125; &#125; &#125; &#125;&#125;​]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的遍历]]></title>
    <url>%2Fpost%2F42854.html</url>
    <content type="text"><![CDATA[这是一个以邻接表为基础的遍历……包括DFS的递归非递归实现以及BFS队列实现 递归DFS：12345678910111213141516171819202122​int main()&#123; TGraph* G = (TGraph*)malloc(sizeof(TGraph)); graph_init(G);//稍微初始化一下我们的图 int visited[N];for(int i=0;i&lt;N;i++) visited[i]=0; for(int i=0;i&lt;N;i++)&#123; if(visited[i]==0)&#123; DFS(G,visited,i); printf("\n"); &#125; &#125;&#125; void DFS(TGraph* G,int visited[N],int i)&#123; printf("%d ",G-&gt;adjlist[i].vex);visited[i]=1; ENode* p = G-&gt;adjlist[i].next; while(p!=NULL)&#123; if(visited[p-&gt;vex]==0)&#123; DFS(G,visited,p-&gt;vex); &#125; p = p -&gt; next; &#125;&#125; 非递归DFS：12345678910111213141516171819202122232425​void dfs(TGraph* G,int visited[N])&#123; Tqueue* stack = stack_init(G-&gt;nv); for(int i=0;i&lt;G-&gt;nv;i++)&#123; if(visited[i]==1) break; else&#123; stack-&gt;qu[stack-&gt;rear++] = i; visited[i]=1; while(stack-&gt;front!=stack-&gt;rear)&#123; ENode* temp = G-&gt;adjlist[stack-&gt;qu[stack-&gt;rear-1]].next; printf("%d ",stack-&gt;qu[--stack-&gt;rear]); while(1)&#123; if(temp==NULL) break; if(visited[temp-&gt;vex]==0)&#123; stack-&gt;qu[stack-&gt;rear++] = temp-&gt;vex; visited[temp-&gt;vex]=1;break; &#125; temp = temp-&gt;next; &#125; &#125; &#125; &#125; free(stack);&#125; BFS：123456789101112131415161718192021​void bfs(TGraph* G,int visited[N])&#123; Tqueue* queue = queue_init(G-&gt;nv); for(int i=0;i&lt;G-&gt;nv;i++)&#123; if(visited[i]==1) break; queue-&gt;qu[queue-&gt;rear++%G-&gt;nv] = i;visited[i]=1;//简单地借用一下循环队列的思想 while(queue-&gt;front%G-&gt;nv!=queue-&gt;rear%G-&gt;nv)&#123; ENode* temp = G-&gt;adjlist[queue-&gt;qu[(queue-&gt;front)%G-&gt;nv]].next; printf("%d ",queue-&gt;qu[(queue-&gt;front++)%G-&gt;nv]); while(1)&#123; if(temp==NULL) break; if(visited[temp-&gt;vex]==0)&#123; queue-&gt;qu[queue-&gt;rear++%G-&gt;nv] = temp-&gt;vex; visited[temp-&gt;vex]=1;break; &#125; temp = temp-&gt;next; &#125; &#125; &#125; free(queue);&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的储存结构]]></title>
    <url>%2Fpost%2F16857.html</url>
    <content type="text"><![CDATA[图的储存结构：邻接矩阵，邻接表，十字链表，邻接多重表，边集数组实现方式：矩阵，链表构成的数组*3，三元组 邻接矩阵（n个结点，构造n*n的矩阵，各点值可达为1，不可达为0）当我们需要将权重赋在矩阵上时，各点值若可达，则为其权重；不可达则为∞（INT_MAX）；对角元为0 邻接表（n个结点，构造有n个首结点的数组，每个首结点拉出一条含该首结点所有可达结点的链表） 123456789101112131415​typedef struct node&#123; int vex; int cost;//对应VNode到该点的权重 struct node* next;&#125;ENode;typedef struct&#123; int vex; ENode* next;&#125;VNode;typedef struct&#123; VNode adjlist[N]; int nv,ne;&#125;TGraph;​ 十字链表（有向图）相比于邻接表，VNode多开了个指针域让你指向以该点为弧尾的第一个弧结点，便于同时求出度和入度，将原先的TNode变成有两个结点编号和两个指针域（一个指向弧头相同的下一条弧，一个指向弧尾相同的下一条弧）的Vex（弧结点）。 12345678910111213141516​typedef struct vex&#123; int headvex,tailvex;//弧头（尾）顶点编号 int cost; struct vex* hlink,tlink;//指向弧头（尾）相同的下一条弧&#125;Vex;typedef struct&#123; int data;//存放顶点信息 Vex* firstin;//以该点为弧头的第一个弧结点 Vex* firstout;//以该点为弧尾的第一个弧结点&#125;VNode;typedef struct&#123; VNode orthlist[N]; int nv,ne;&#125;OrthGraph;​ 邻接多重表（无向图）相比于邻接表，VNode不变，将原先的TNode变成有两个结点编号和两个指针域（一个指向下一条依附）的Vex（弧结点） 12345678910111213141516​typedef struct vex&#123; int ivex,jvex;//依附于该边的两个顶点编号 int cost; struct vex* ilink;//指向下一条依附于顶点ivex的边 struct vex* jlink;//指向下一条依附于顶点jvex的边&#125;Vex;typedef struct&#123; int data;//存放顶点信息 Vex* firstedge;//指向第一条依附于该顶点的边结点&#125;VNode;typedef struct&#123; VNode orthlist[N]; int nv,ne;&#125;OrthGraph;​ （TIPS_对比：十字链表注重对边的操作，而邻接多重表更注重对点的操作） 边集数组（类似于三元组）12345678910111213141516​typedef struct&#123; int vex; int gno;//连通分量，用于kruskal算法&#125;TVex;typedef struct&#123; int va,vb; int cost; &#125;TEdge;typedef struct&#123; TVex* pv; TEdge* pe; int nv,ne;&#125;TGraph;​]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AVLtree]]></title>
    <url>%2Fpost%2F27676.html</url>
    <content type="text"><![CDATA[平衡二叉搜索树(Self-balancing binary search tree)又被称为AVL树（有别于AVL算法）它左右的两个子树高度差的绝对值不超过1，且左右两个子树都是平衡二叉树。 引入：在说平衡二叉树之前，我们先需要谈谈二叉排序树，详见我之前在《排序》中所写。当排序树构建出来过于“畸形”，两边不对称，极端一点就是一路下来毫无分叉，那么显然，它的时间复杂度就是n，为了避免这种情况或者说，为了找寻最优情况，我们希望我们构建出来的二叉树可以两边对称平衡一下，使其树的深度为lonn而非n，为此，我们引入平衡二叉树的概念。 调整(旋转)：当每次插入一个结点后，我们根据这棵树的现状对它进行旋转操作，可以简单的分为： 单旋： 在A结点的左孩子的左子树或者右孩子的右子树上插入结点。 直接将B结点提上来，再把离A最近的子树滑给A。 双旋： 在A结点的左孩子的右子树或者右孩子的左子树上插入结点。 直接将C提到A、B之间，再把C的子树分别分给A、B所缺的位置。 删除：查找：现在平衡树中查找到关键字相同的结点p 删除：分以下几种情况： 叶结点：直接删除就好 单分支结点：把后面那个孩子结点接上来就好啦 双分支结点：把这个双分支结点p的值，用其中序前驱结点q的值替代后，直接删掉结点q]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tree]]></title>
    <url>%2Fpost%2F20571.html</url>
    <content type="text"><![CDATA[这里是关于二叉树的一系列函数C的代码二叉树的编程我觉着可以帮助你学习递归 : )123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef int ElemType;typedef struct node&#123; ElemType data; struct node* lc; struct node* rc; int visit;//真的，如果不是这样写最方便，我也不想直接改掉TNode结构体orz &#125;TNode;typedef struct&#123; TNode** qu; int n; int front; int rear;&#125;Tqueue;TNode* tree_init();//树的初始化void preorder(TNode* tree); //递归遍历void tree_pre(TNode* tree); //非递归遍历void inorder(TNode* tree); void tree_in(TNode* tree); void postorder(TNode* tree); void tree_post(TNode* tree); void level_order(TNode* tree);//层次遍历int tree_nodes(TNode* tree);int tree_leaves(TNode* tree);int tree_depth(TNode* tree);int tree_level(TNode* tree,int x,int* j);//查找int tree_width(TNode* tree);Tqueue* queue_init(int n)&#123; Tqueue* queue = (Tqueue*)malloc(sizeof(Tqueue)); queue-&gt;qu = (TNode**)malloc(sizeof(TNode*)*n); queue-&gt;front = 0; queue-&gt;rear = 0; queue-&gt;n = n; return queue;&#125;int main()&#123; freopen("tree.txt","r",stdin); TNode* tree = tree_init(); preorder(tree);printf("\n");tree_pre(tree);printf("\n"); inorder(tree);printf("\n");tree_in(tree);printf("\n"); postorder(tree);printf("\n");tree_post(tree);printf("\n"); level_order(tree); printf("\nTree leaves: %d\tTree depth: %d\tTree nodes: %d",tree_leaves(tree),tree_depth(tree),tree_nodes(tree)); int num;tree_level(tree,'K',&amp;num);printf("\t'K': %d",num+1); printf("\tTree width: %d",tree_width(tree));&#125; TNode* tree_init()&#123; TNode* tree = (TNode*)malloc(sizeof(TNode)); if(tree==NULL)&#123; printf("tree_init:malloc error."); exit(0); &#125; ElemType a; scanf("%d",&amp;a); if(a==0) tree=NULL; else&#123; tree-&gt;data = a; tree-&gt;visit= 0; tree-&gt;lc = tree_init(); tree-&gt;rc = tree_init(); &#125; return tree; &#125;void preorder(TNode* tree)&#123; if(tree==NULL) return; else&#123; printf("%c ",tree-&gt;data); preorder(tree-&gt;lc); preorder(tree-&gt;rc); &#125;&#125;void tree_pre(TNode* tree)&#123; Tqueue* stack = queue_init(tree_nodes(tree));//假装这是初始化栈的函数 while((stack-&gt;rear!=0)||(tree!=NULL))&#123; if(tree!=NULL)&#123; stack-&gt;qu[stack-&gt;rear++] = tree; printf("%c ",tree-&gt;data); tree = tree-&gt;lc; &#125; else&#123; tree = stack-&gt;qu[--stack-&gt;rear]; tree = tree-&gt;rc; &#125; &#125; free(stack);&#125; void inorder(TNode* tree)&#123; if(tree==NULL) return; else&#123; inorder(tree-&gt;lc); printf("%c ",tree-&gt;data); inorder(tree-&gt;rc); &#125;&#125;void tree_in(TNode* tree)&#123; Tqueue* stack = queue_init(tree_nodes(tree)); while((stack-&gt;rear!=0)||(tree!=NULL))&#123; if(tree!=NULL)&#123; stack-&gt;qu[stack-&gt;rear++] = tree; tree = tree-&gt;lc; &#125; else&#123; tree = stack-&gt;qu[--stack-&gt;rear]; printf("%c ",tree-&gt;data); tree = tree-&gt;rc; &#125; &#125; free(stack);&#125; void postorder(TNode* tree)&#123; if(tree==NULL) return; else&#123; postorder(tree-&gt;lc); postorder(tree-&gt;rc); printf("%c ",tree-&gt;data); &#125;&#125;void tree_post(TNode* tree)&#123; Tqueue* stack = queue_init(tree_nodes(tree)); while((stack-&gt;rear!=0)||(tree!=NULL))&#123; if(tree!=NULL)&#123; if(tree-&gt;visit==0) stack-&gt;qu[stack-&gt;rear++] = tree; tree = tree-&gt;lc; &#125; else&#123; tree = stack-&gt;qu[--stack-&gt;rear]; if(tree-&gt;visit==1) printf("%c ",tree-&gt;data); else&#123; tree-&gt;visit = 1; stack-&gt;qu[stack-&gt;rear++] = tree; &#125; tree = tree-&gt;rc; &#125; &#125; free(stack);&#125; void level_order(TNode* tree)&#123; Tqueue* queue = queue_init(tree_nodes(tree)); queue-&gt;qu[queue-&gt;rear++%queue-&gt;n] = tree; while(queue-&gt;front%queue-&gt;n!=queue-&gt;rear%queue-&gt;n)&#123; if(queue-&gt;qu[(queue-&gt;front)%queue-&gt;n]-&gt;lc!=NULL) queue-&gt;qu[queue-&gt;rear++%queue-&gt;n] = queue-&gt;qu[(queue-&gt;front)%queue-&gt;n]-&gt;lc; if(queue-&gt;qu[(queue-&gt;front)%queue-&gt;n]-&gt;rc!=NULL) queue-&gt;qu[queue-&gt;rear++%queue-&gt;n] = queue-&gt;qu[(queue-&gt;front)%queue-&gt;n]-&gt;rc; printf("%c ",queue-&gt;qu[(queue-&gt;front++)%queue-&gt;n]-&gt;data); &#125; free(queue);&#125;int tree_nodes(TNode* tree)&#123; if(tree==NULL) return 0; int a,b; a = tree_nodes(tree-&gt;lc); b = tree_nodes(tree-&gt;rc); return a+b+1;&#125;int tree_leaves(TNode* tree)&#123; if(tree==NULL) return 0; if(tree-&gt;lc==NULL&amp;&amp;tree-&gt;rc==NULL) return 1; return tree_leaves(tree-&gt;lc)+tree_leaves(tree-&gt;rc);&#125;int tree_depth(TNode* tree)&#123; int dl = 0; int dr = 0; if(tree==NULL) return 0; if(tree-&gt;lc==NULL&amp;&amp;tree-&gt;rc==NULL) return 1; dl = tree_depth(tree-&gt;lc); dr = tree_depth(tree-&gt;rc); return 1+((dl&gt;dr)?dl:dr);&#125;int tree_level(TNode* tree,int x,int *j)&#123; if(tree==NULL) return -1; // 没找到 返回-1； if(tree-&gt;data==x) return *j; int t=++*j; // 层数加1 int ret=tree_level(tree-&gt;lc,x,j); if(ret&gt;0)return ret; //在左子树分支找到 *j=t; //恢复层数 return tree_level(tree-&gt;rc,x,j);&#125;int tree_width(TNode* tree)&#123; if(tree==NULL) return 0; Tqueue* queue = queue_init(tree_nodes(tree)); queue-&gt;qu[queue-&gt;rear++%queue-&gt;n] = tree; int width=1; while(true)&#123; int size=queue-&gt;rear-queue-&gt;front;//当前层的节点数 if(size&gt;width) width = size; if(size==0) break; while(size&gt;0)&#123;//如果当前层还有节点就进行下去 TNode* node = queue-&gt;qu[(queue-&gt;front++)%queue-&gt;n];size--; if(node-&gt;lc) queue-&gt;qu[queue-&gt;rear++%queue-&gt;n] = node-&gt;lc; if(node-&gt;rc) queue-&gt;qu[queue-&gt;rear++%queue-&gt;n] = node-&gt;rc; &#125; &#125; free(queue); return width;&#125;​]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Huffman Tree]]></title>
    <url>%2Fpost%2F32862.html</url>
    <content type="text"><![CDATA[哈夫曼树也称为最优二叉树，是一种带权路径长度最短的完全二叉树。最经典的用法就是做无损数据压缩的huffman coding 概念：哈夫曼树也称为最优二叉树，是一种带权路径长度最短的完全二叉树（Complete Binary Tree）树的带权路径长度：树中所有的叶结点的权值乘上其深度（到根结点的路径长度），然后进行加和的结果。哈夫曼树中根结点为第0层，N个权值构成一棵有N个叶结点的二叉树，相应的叶结点的路径长度为其层数。树的路径长度是从树根到每一结点的路径长度之和，记为WPL 易证哈夫曼树的WPL是最小的。 原理：编码表是通过对源符号出现的概率进行评估的方式得到，出现概率高的符号使用较短的编码，反之则使用较长的编码，由此实现编码后字符串的平均长度的期望值降低，从而达到无损压缩数据的目的。 举例：根据文本文件得出45个不同字符，通过所给的函数初始化哈夫曼树，根据45个初始节点完善填充整个（2*n-1 = 89）哈夫曼数组，再根据哈夫曼数组制作密码本，进行文件的压缩（加密）与解压（解密）的功能实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209​#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define LEN 100typedef struct&#123; char ch; int weight;int parent; int lchild;int rchild;&#125;TNode;typedef struct&#123; char ch; char code[LEN];&#125;TCode;typedef struct&#123; char ch; int weight;&#125;TW;void select_subtree(TNode* pht,int n,int* pa,int* pb)&#123; for(int i=0;i&lt;n;i++)&#123; if(pht[i].parent==-1)&#123; *pa = i; break; &#125; &#125; for(int j=*pa+1;j&lt;n;j++)&#123; if(pht[j].parent==-1)&#123; *pb = j; break; &#125; &#125; //printf("\n%d\t%d\t%d\t%d\t",*pa,*pb,pht[*pa].parent,pht[*pb].parent); int temp = *pa; *pa = (pht[*pa].weight&gt;pht[*pb].weight)?*pb:*pa; *pb = (pht[*pb].weight&lt;pht[temp].weight)?temp:*pb; //printf("\n%d\t%d",*pa,*pb); for(int i=0;i&lt;n;i++)&#123; if(pht[i].parent==-1)&#123; if(pht[i].weight &lt; pht[*pa].weight&amp;&amp;i!=*pa&amp;&amp;i!=*pb) &#123;*pb = *pa;*pa = i;&#125; else if(pht[i].weight &lt; pht[*pb].weight&amp;&amp;i!=*pa&amp;&amp;i!=*pb) &#123;*pb = i;&#125; &#125; &#125; //printf("\n%d\t%d",*pa,*pb); &#125;TNode* create_htree(TW weights[],int n)&#123; TNode* pht = (TNode*)malloc(sizeof(TNode)*(2*n-1)); for(int i=0;i&lt;n*2-1;i++)&#123; pht[i].ch = (i&lt;n)?weights[i].ch:' '; pht[i].weight = (i&lt;n)?weights[i].weight:0; pht[i].parent = -1; pht[i].lchild = -1;pht[i].rchild = -1; &#125; int pa,pb; for(int i=n;i&lt;n*2-1;i++)&#123; select_subtree(pht,i,&amp;pa,&amp;pb); pht[pa].parent=i;pht[pb].parent=i; pht[i].lchild = pa;pht[i].rchild = pb; pht[i].weight = pht[pa].weight+pht[pb].weight; //printf("\n%d\t%02d\t%02d\t%02d\t%02d\n",i,pht[i].weight,pht[i].parent,pht[i].lchild,pht[i].rchild); //printf("%d\t%d\n",pht[pht[i].lchild].parent,pht[pht[i].rchild].parent); &#125; return pht;&#125;void encoding(TNode* pht,TCode book[],int n)&#123; char* str = (char*)malloc(n+1); str[n] = '\0'; for(int i=0;i&lt;n;i++)&#123; int idx = i;int j = n; while(pht[idx].parent!=-1)&#123; if(pht[pht[idx].parent].lchild==idx)&#123; j--;str[j]='0'; &#125; if(pht[pht[idx].parent].rchild==idx)&#123; j--;str[j]='1'; &#125; idx = pht[idx].parent; &#125; book[i].ch = pht[i].ch; strcpy(book[i].code,&amp;str[j]); printf("%c : ",book[i].ch); puts(book[i].code); &#125;&#125;void decoding(TNode* pht,char codes[],int n)&#123; freopen("your_love.txt","w",stdout); int i=0,p = 2*n-2; while(codes[i]!='\0')&#123; while(pht[p].lchild!=-1&amp;&amp;pht[p].rchild!=-1)&#123; if(codes[i]=='0') p = pht[p].lchild; else p = pht[p].rchild; i++; &#125; printf("%c",pht[p].ch); p = 2*n-2; &#125; printf("\n"); fclose(stdout); &#125;// 统计字符串text中字符出现的频率，参数n为字符串长度// 返回值为：text中出现的不同种类的字符个数// 副作用：通过指针参数间接返回两个数组，其中：// dict：字符数组，存放 text中出现的不同种类的字符// freq：整型数组，存放 text中出现的不同种类的字符的出现频率 int calc_freq(char text[], int **freq, char **dict, int n)&#123; int i, k, nchar = 0; int * pwght; char * pch; int tokens[256] = &#123;0&#125;; // 根据输入的文本字符串逐一统计字符出现的频率 for(i = 0; i &lt; n; ++i)&#123; tokens[text[i]]++; &#125; // 统计共有多少个相异的字符出现在文本串中 for(i = 0; i &lt; 256; i++)&#123; if( tokens[i] &gt; 0 )&#123; nchar++; &#125; &#125; // 为权重数组分配空间 pwght = (int*)malloc(sizeof(int)*nchar); if( !pwght )&#123; printf("为权重数组分配空间失败！\n"); exit(0); &#125; // 为字符数组（字典）分配空间 pch = (char *)malloc(sizeof(char)*nchar); if( !pch )&#123; printf("为字符数组（字典）分配空间失败！\n"); exit(0); &#125; k = 0; for(i = 0; i &lt; 256; ++i)&#123; if( tokens[i] &gt; 0 )&#123; pwght[k] = tokens[i]; pch[k] = (char)i; //强制类型转换 k++; &#125; &#125; *freq = pwght; *dict = pch; return nchar;&#125; int main()&#123; freopen("love_letter.txt","r",stdin); char* str = (char*)malloc(2000); gets(str); fclose(stdin); int** ch_f = (int**)malloc(4); char**ch_c = (char**)malloc(4); int n = calc_freq(str,ch_f,ch_c,strlen(str)); free(str); TW weights[n]; for(int i=0;i&lt;n;i++)&#123; weights[i].weight = (*ch_f)[i]; weights[i].ch = (*ch_c)[i]; &#125; free(ch_f);free(ch_c); TNode* pht = create_htree(weights,n); TCode codebook[n]; encoding(pht,codebook,n); freopen("love_letter.txt","r",stdin); char* word = (char*)malloc(2000); gets(word); fclose(stdin); freopen("codebook.txt","w",stdout); while(*word!='\0')&#123; for(int i=0;i&lt;n;i++)&#123; if(*word==codebook[i].ch)&#123; for(int j=0;j&lt;strlen(codebook[i].code);j++)&#123; printf("%c",codebook[i].code[j]); &#125; &#125; &#125; word++; &#125; fclose(stdout); //free(word); freopen("codebook.txt","r",stdin); char* codes = (char*)malloc(6000); gets(codes); fclose(stdin); decoding(pht,codes,n); return 0;&#125;​]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP]]></title>
    <url>%2Fpost%2F31301.html</url>
    <content type="text"><![CDATA[普通的查找匹配需要在失配时回溯到失配串第二位继续开始查找是否匹配，复杂度过高。于是我们想着能不能一次不回头的走到底，针对模式串创建了一个辅助数组（next数组）next数组各个元的值：固定字符串的最长真前缀（第一个字符伊始，但不含最后一个）和最长真后缀相同的长度，以下举例。 n e x t 数 组 各元固定字符串 相同最长前后缀 前后缀相同长度 n e x t [0] a “ ” 0 n e x t [1] ab “ ” 0 n e x t [2] aba “a” 1 n e x t [3] abab “ab” 2 n e x t [4] ababa “aba” 3 n e x t [5] ababab “abab” 4 n e x t [6] abababc “ ” 0 n e x t [7] abababca “a” 1 a b a b a b c a 的 n e x t 数 组 为 ： -1 0 0 1 2 3 4 0 1 因为你的目标串没有必要完全回溯，可以把前面相同的位置掠过，因此有了基于next数组的KMP算法，以下举例： 目标串T：ababaacababbabababca 模式串P：abababca 指针i指向目标串，指针j指向模式串，if（T[i]==P[j]）//匹配成功 i++ , j++//往后继续尝试是否匹配 如果失配，i指针不回溯，j指针回溯（j = next[j]），当回溯到首元时，无法再回溯，继续往后试探 i++ , j++ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748​#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;typedef struct&#123; char *pch; int len;&#125;Tstr;int* KMP(Tstr* T,Tstr* P,int *next)&#123; int i=0,j=0,k=0,m = T-&gt;len,n = P-&gt;len; int num[m]; for(int i=0;i&lt;m;i++) num[i]=0; while(i&lt;=m-n)&#123; //不超过目标串的长度，且i&gt;m-n时不可能存在匹配 while(j==-1||(j&lt;n&amp;&amp;T-&gt;pch[i]==P-&gt;pch[j]))&#123; //只要相同且不超过模式串长度就继续往前走 i++; j++; &#125; if(j==n) num[k++] = (i-n+1);//成功找到一个匹配 j = next[j];//通过next数组回溯模式串，继续寻找匹配 &#125; return num;&#125;int* KMP_next(Tstr* P)&#123; int *next = (int*)malloc(sizeof(int)*(P-&gt;len)); int j=0,k=-1,n = P-&gt;len; next[0] = -1; while(j&lt;n)&#123; if(k==-1||P-&gt;pch[j]==P-&gt;pch[k])&#123; next[j+1] = k+1; j++; k++; &#125; else k = next[k]; &#125; return next; &#125;int* KMP_nextval(Tstr* P)&#123; int* nextval = KMP_next(P); for(int j=1;j&lt;P-&gt;len;j++)&#123; if(P-&gt;pch[j]==P-&gt;pch[nextval[j]]) nextval[j]=nextval[nextval[j]]; &#125; return nextval;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>大一</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[方寸间的茶，骨子里的美]]></title>
    <url>%2Fpost%2F62922.html</url>
    <content type="text"><![CDATA[喜欢，从来都是简单随性的，它可能是雨天里脚踏在水坑上清脆的回响和扬起奇妙弧度的水花；也可能是夏日里偶然撞见的白色衬衫和“刺啦”一声打开的汽水罐……这些，是生活在这青葱岁月的我们所定义的喜欢，而在唐宋元明清，一扇又一扇翩跹舞开的帘幕下藏着的，是那个时代的人们心底最敏感、最柔软却又最热烈的喜欢——诗词。便如茶，元稹的“铫煎黄蕊色，婉转曲尘花”是喜欢；高鹗的“瓦铫煮春雪，淡香生古瓷”是喜欢；刘禹锡的“骤雨松声入鼎来，白云满碗花徘徊”是喜欢。每个人的喜欢都是不同的，一千种茶，在品茶者眼里是一千种人生、一千种欢喜。这接下来，我就借用朱海燕老师在《中国茶美学研究》里的一篇文章——《唐宋诗词中茶之“美”的刻画》，浅述一下我个人对于“茶”与“美”的认知与看法，以及流淌在诗词中的那一份微醉的喜欢。 形色之美 细如粟粒柔如蕊,肥如云腴壮似笋——茶芽之美 枪旗鸟爪劲姿爽,鹰嘴雀舌展芽叶——芽梢之美 圆如皓月润似玉,方比珪璧芳胜兰——团饼茶之美 茶芽，刚吐露的嫩芽无处不炫耀着她娇嫩诱人的初生姿态，这理当是茶这一植株在其生长的一生中最欢愉最激情向上的阶段，这个阶段表现最突出的便是生命力——那种绿的逼人的翠色，让人生怕它会在你转身的那一刹那“噗啦”一声滴下来，令你不得不着迷似的定在那里，移不开视线。而作为人类文明演变发展的长河中，思绪最为敏感的那一圈人中，在我国唐宋时期，体现的最为明显的便是或风花雪月，或忧国忧民的诗人。他们用他们传世的诗、词、曲勾画着他们当时心底那份敏感纯粹被挑拨得激动不已的时刻。这是个感性的区域，从来就没有个方程中唯一解这样让人觉着安稳，却不禁扫兴的说法。每个人在那一瞬所呈现的感觉是不一样的，同时也是最能展示和区分他们波澜壮阔的内心世界的东西，那就是——想象。想象，这是自人类出生开始就赐予人一生的珍贵的天赋：它是天生的，孩子的想象往往最为干净纯粹，包含了最初始最接近“0”的美好烂漫，而随着世俗的沾染，想象这种东西渐渐被束缚，被怀疑，被丢弃……我个人，或者说，在我这个时期，刚感受着离家两千公里的大一生活的我，愈发地觉得，失去了想象的人，是最为可悲的，因为这应是一个不可逆的过程。但是在这个社会转型的大过渡时期，一个大弯甩过来，荡平了多少人纯真，不切实际却美好异常的想象；甩掉多少人那心底小小的仅存的遵守和天真；一个弯所圈出来的怀疑与自我否定，与其说是辩证性地思考，倒不如说是失去自信地辩解。当桌上的茶叶全部被速溶咖啡取代的时候，不敢说这种前进我能有多讨厌，但我敢说我绝不会喜欢。我更希望的是另一种前进，一种精神上或者说人性上的前进，给我的不再是昂不起的头颅和直不起的脊梁的感觉，不再是没有自信没有底气却还要不断争辩的无力与腻歪，而是一种像茶芽一般喷涌着惊人生命力的饱满姿态，那种骨子里的自信、自知与谦和，真正做到心细如粟，心柔如蕊；神壮似笋，肥如云腴。同时，想象也是后天所培育发展的，一个人所能想象出的事物，起源于这个人从他出生至今所经历的事情，所感悟的美好，所体会的失落，所思考的问题，所接触的人群，所读过的书籍……这一切的一切所综合起来构成了他想象的基石。所以在那个信息不流通的年代，他们游山玩水，他们体验人生；他们“不羡朝入省”，他们“不羡慕登台”；他们感悟不同的风土人情，他们把玩不同的文物古籍；这样的他们用他们一生的所见所闻，所思所感筑成了这座瑰丽的想象宫殿，供我们观赏感叹，找寻在快节奏下的闹市里心灵的一片净土，甚至说是归宿。 接下来，就是两种不同方向的意象美，芽梢和团饼茶。芽梢较尖，一般所代表的是一种灵动的个性；而饼茶较为圆润，讲究的是一种温润的淳善。 枪旗、鸟爪、鹰嘴、雀舌，古人对芽梢的形容无一不将浓郁的生命力赋给了这小巧的尖角，擦亮了一抹惊艳的生命色彩。明明只有两字，却极鲜明地将芽梢的那种“尖”劲儿脆生生地呈现在我们的眼中，茶叶本身的形状只是种客观存在的形态，并不包含融在这两个字里的“灵魂”，无疑，这迸发出强烈生机与共鸣的感受源于诗人自身的灵魂，在诗人的一字一句中，融入了他对于这个世界自然而又感性的认识，每一种感性的认知都单纯而又坦诚地给读者打开了一扇了解作者根源的门，他幼年的纯真或残酷，他少年的幻想或青涩，他成年的欢愉或失落，构成了他对这个自然世界的认知，一字一句中我们都能看见掠过的只属于他的浮窗剪影。由此，在我个人看来，感性的或者说个性的创作是会流传不朽，待人发现的。这样的创作会勾起读者的情绪与回忆，人只有对自己共鸣的作品才会真正的爱不释手，产生一种微妙的专有感与占有欲，先不论对错与后续的影响，但是这个作品无疑会因为这样微妙的感情而传递下去。而这之中传承的不仅是茶叶神态中的神韵，更有一种流淌在华夏血脉里，刻进骨子里的精神，如茶一般，千姿百态，各有各的个性与傲骨。 “圆如皓月润似玉,方比珪璧芳胜兰”，说来这和前两种有着本质上的区别。前两种是茶自然的、原生态的情况，如果说这前两种是原汁原味的生命力的展现，那么，团饼茶就是体现我们中华先辈劳动智慧与蕙质兰心的引喻了。若是按我的想象来看，这团饼茶所代表的，就是古人对“君子”的美好向往与期待了：“圆如皓月润似玉”，象征着君子的温润如玉，包容沉稳；而“方比珪璧芳胜兰”，则象征着君子的清远宁人，秀智兰香。在朱海燕女士的原文中有提到：“形色产生美感其根源是人类在社会实践时对自然形状(包括运动、结构)的把握和运用的过程中，使形色与主体知觉结构的相互适应，从而引起审美愉悦。品读茶诗时，可以感受诗人们对茶形色的审美情趣，而形象的刻画更能激发人们无尽的遐想：麦粒的纤小，枪旗的英姿，雀舌的灵巧，琼蕊的秀丽，圭璧的圆润，紫的高贵、绿的生机、白的纯洁、黄的温馨、黑的凝重，每个意象仿佛就是一幅精妙绝伦的工笔画。”可以说，在茶的形色之美上，古人的诗词将自己的美好祝愿与自身情感全部寓情于“茶”，在品茶的形色中感受诗词的妙韵，在品读诗词中欣赏着茶的形色，将声色与想象融于美之中，清幽的茶与沉香的诗，相互交织构成了现在我们所品味研究的“茶美学”。 茶香之美 香飘九畹清若兰,幽薄芳草得天真——茶香之清幽美 疏香皓齿有余味,更觉鹤心通杳冥——茶香之悠远美 风流气味未染尘,不是人间香味色——茶香之脱俗美 “茶的香气,分为真香和混和香两种,真香是茶自身所具有的香味,混和香是加入香味物与茶的真香混合而形成。不同的茶香各有区别,或甜润馥郁,或清幽淡雅,或高爽持久,或鲜灵沁心,因茶之别而变化无穷。” 著名茶叶专家施兆鹏在其主编的《茶叶审评与检验》一书中，将成品茶香气归纳为九种香气类型：毫香型、嫩香型、花香型、果香型、清香型、甜香型、火香型、陈醇香型、松烟香型。正是因为这些纷繁的香气，些微的不同往往会带给品茶者全然不同的崭新感受，这也是茶香的无穷魅力所在，引起文人墨客的争相赞颂。而除此之外，诗人当时所处的环境和心境对茶香的品味也有很大的影响，想来，吵杂市井里的茶香和幽静庭院里的茶香，给人的感受是绝对不同的。“花笺茗碗香千载，云影波光活一楼”茶香是一个很玄妙的东西，用嗅觉触发想象，便比“形色”来得更为飘忽。 唐代诗人李德裕描写茶香为：“松花飘鼎泛，兰气入瓯轻”。“轻”字形象地表达如兰花般极为清雅的茶香随着茶的烹煮而逐渐散发出来的过程。“香于九畹芳兰气”，茶香清幽如兰，不浓烈，不艳俗，香虽清淡却能随风飘送至数里之外，如此清风傲骨的象征，又有哪一个文人雅士能不受她的诱惑?在我家乡有种被称作“汀溪兰香”的茶叶，因为在生长过程中长期与空谷幽兰相伴随生长，便渐渐浸染上了兰花的那一股幽香，颇受一些文人雅士所推崇，当然也因此这种茶叶的珍品售价极高。先不论其他，就单单这一点便不难看出品茗者们对于兰香是何等的推崇，大概都是向往着《离骚》中“余既滋兰之九畹兮，又树蕙之百亩”生活的吧。 朱海燕老师在原文中所引用的这句“疏香皓齿有余味，更觉鹤心通杳冥”，取自于温庭筠的《西陵道士茶歌》，原诗这样写道： 乳窦溅溅通石脉，绿尘愁草春江色。 涧花入井水味香，山月当人松影直。仙翁白扇霜鸟翎，拂坛夜读黄庭经。 疏香皓齿有余味，更觉鹤心通杳冥。 茶香悠远，这份悠远如烟般萦绕在诗人温庭筠的皓齿舌尖，逐渐在精神上更加地贴近一种通明的无为而治的感觉，在诗人主观上与心中所念的“道”相为贴近。按照我个人的理解，“道法自然”，道修的是自身与自然的连接，并不主张用世间的条条框框来束缚自己。他们的随性而为，他们的无为而治，不是说随意、无节制地放出自身的欲望破坏人常纲理，他们的心不是随意散乱的，而更偏向于一种浑圆透彻的状态，在我个人的想象中，便是种“太极”的姿态，周而往返、浑圆剔透。他们所修的是一种智慧的自然，他们去探寻自然而不改变自然，故因此，与自然更加亲近。这或许也是为什么道家和儒家一样推崇着茶道，甚至有着“以茶入道”的说法与故事。也不知道是茶本身就如此玄妙，还是在人为赋予自身的幻想与寄托之后，才那么地令人着迷。但不管怎样，茶，早已成为我国传统文化一面无可或缺的明镜，映照着华夏五千年的历史变迁与文化发展。 茶味之美 流华无尘净肌骨,疏瀹清味涤心源——茶味之清美 琼蕊甘露贵流霞,灵芽云液胜醍醐——茶味之甘美 茗饮醇滑齿颊香,消尽酲醲爽气来——茶味之爽美 香茗一盏甘与苦,人生百味寓其中——“味外之味” “清”，这个在中国古代美学上留下浓墨重彩的一笔的字。“清”字，《说文解字》之中将其释为“澄水之貌”，意指水澄澈明净的样子。茶味之清，体现于淡。这种淡不是苍白的寡淡，而是“清淡”与“轻淡”。借用朱海燕老师在《中国茶美学研究》中所说，“其味觉是丰富的，味感是微妙的”。我们为了感受清美不妨设想一下，说到清，想到了什么？清雨？这个词最先映入我的脑海，接着便是随之而来的意想。清雨？为什么不是微雨？“落花人独立，微雨燕双飞”第一眼见到小山的词，便觉惊艳，不是李商隐那般华美的惊艳，而是扑面而来的清爽与干净利索。但微雨成丝，连绵不绝，与心中所期待的清美相比，总觉着是差了些味道。不知为何，说到“清雨”，总会有花浮现，大概是那句“清明时节雨纷纷”在脑中洗脑般的刻印所致。“清明时节雨纷纷”的清雨，是带着踏青的美好与春日的生机的。或者说，是带着我对于春雨踏青的印象。在玉珠连丝的春日，伴着雨线划过脸颊的清凉，看着雨水洗过的绿色，陷在其中的我大概分不出到底是“青”还是“清”，但这里的一切和我脑海中浮现的一切，一定都是“美”的概念作为我内心里的某种具象化。 “甘”这一点自不必说，想来在大家心中甜美都是很幸福很美好的存在。而对于茶叶而言，茶味之甘美却又与我们正常情况下所提及的甘美不同，我们在日常生活中喜爱甜食，用化学的眼光来看待是因为甜食中的糖分在于人体接触后发生化学变化造成大量的多巴胺，通过不断的吸收与溶解，多巴胺通过血管流至全身，最后刺激神经致使人体产生亢奋状态，这可能也是我对巧克力那么痴迷的原因之一吧。而茶味的甘美是在于它的回味，先苦后甜，而这种苦后回甘之味才是受古代文人雅士所推崇的，这倒也符合我们中国传统的文化思想观念与美德。“爽”，这里的爽同样和我们现在常说的有些区别，这里所表示的更多是“神清气爽”，现在我们也常听到长辈们说关于醒酒茶的功效，虽然现代科学证明了用茶醒酒是错误的做法，但这也是古代人将茶作为提神醒脑的良药的佐证。按文中所说便是“好茶往往滋味醇爽，入口润滑而不紧涩，饮过之后齿颊留香，提神醒脑，畅意不已，唐宋时任将这种让人通体舒泰的美感谓之‘爽’”。而所谓“味外之味”，简单来说就是借品茶来品味人生。比如苏轼一生坎坷，在“乌台诗案”之后，在儒、道、佛思想的共同影响之下，有着复杂的处事观念，既有儒家忧国忧民的入世之心，又有佛家放空自我的出世之心，同时又在道家的影响下向往自然，寻求与自然的超脱。这三种思想集中在一个人身上，不由显得矛盾而又分裂。为了调和改善这种情况，他把品茶作为沟通自然、内省自性、品味人生、超越自我的载体，将不同的思想通过“茶”这一纽带相互串连。也因如此，茶渐渐成为了他生命中寄思不可缺少的物品。这便是茶之“味”，茶之美的某种体现吧。 茶叶，在唐宋词人的眼中，是寓于自身丰沛情感的寄托，是自己内心世界的写照。茶如明镜，一滴茶水滴落下去，“叮”地激起一圈水纹，荡起一层清绿，随之平静无褶。诗人走到茶水边，先看上那么几眼，看看茶水中荡漾的自己的“形色”，然后不用动上鼻尖，便会闻到自心里溢出来的“茶香”，接着便一头栽进这茶水里，让茶水洗涤着自己的身子和心，去体会最回归与最纯粹的“茶味”，纯粹到令人艳羡。这些，便构成了唐宋诗词中对于“茶”之美的刻画。而这些美，是需要寻着诗词品赏茶、感受茶，将茶带进自己的人生与思考才能真正体会到的美学。希望看到这篇文章的人，都能尝试着去体会中国的茶文化，去感悟这之中蕴藏着的茶美学。]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>大一</tag>
        <tag>茶美学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[书单]]></title>
    <url>%2Fpost%2F32430.html</url>
    <content type="text"><![CDATA[不知道现在的自己所思考的究竟和以前的自己所想的有多少出入。也不知道现在自己所坚持的和以前的自己有多少区别。能做的，只有坚信着未来不断前进。未来是一个闪着光的字眼，闪着一种叫希望的光。怀着希望是最悲伤的事情，因为希望，所以要不断支撑着自己向前爬行。怀着希望是最幸福的事情，因为希望，所以赌上一切只是为了几近虚妄的“光明的未来”。仿佛抓住了那片幻想中的光明，就能得到救赎，可以宽恕一切不希望发生的过去，或者说，给那些过去安排了一个合适到完美的理由。但每个人心底都清楚得很，光，哪是人可以抓住的？于是乎，又给了自己一个超脱“人”这个范畴的动机与理由。直到最终面目全非，披着“人”的幌子留给光鲜而已。 Book List锵锵~这是份隐藏彩蛋：听学姐的话，从2020开始，在过往和未来的书单里，挑一些印象深刻的书记录下来。 To Do《时间的秩序》 《每个人都会死，但我总以为自己不会》 《昨日之旅》 《相约星期二》 2020《心灵的焦灼》茨威格的心理描写之细腻总能让我直接陷进去，这本是他唯一的长篇，特别安利。同情是一系列复杂情绪的化合物，作为共情者的悲伤、施舍者的优越、攀比者的虚伪……“看到别人的不幸，急于尽快脱身出来，以免受到感动，陷入难堪的境地，这种同情根本不是对别人的痛苦抱有同感，而只是本能地予以拒绝，免得它触及自己的心灵”，像打发叫花子一般，只希望速战速决，让自己的情绪继续当前，而不被缠于被同情者。同情常常伴随着同情者的牺牲。人们常用高位者的姿态进行给予来换回心里的平复，告诉自己我是施舍者，我已经做出了牺牲，我可以心安理得。但这本质不变，依旧是急于脱身。还有一种，同情心泛滥。这种人一见着别人的不幸就难受得不行，他们扮演着“圣人”的角色，心底的柔弱使他们沉溺在他人的痛苦中不能自拔，他们在最后恐惧于越陷越深，却在开始时一沾则陷。“同情就跟吗啡一样，只在刚开头的时候对病人是行善，是灵药，是帮助，可如果你不会掌握分寸，剂量不当，不及时停药，就会变成凶险的毒药”，可惜人无论对于同情还是对于玛咖，都有着习以为常的适应性。一步步加深，一步步套牢，“做人得好好控制自己的同情心，否则比麻木不仁危害更甚。——这点，我们做大夫的知道，当法官的、担任法院执行官的和开当铺的也都知道。倘若大家都动不动同情心大发，那么地球就静止不动了”。大众对于不合自己判断的事，始于好奇继而猜疑，最终夹杂着种种恶意被揉捏成大众最喜闻乐见的谣言。 《资本论》绝对的著作啊，一步步推理论证揭示资本是如何血淋淋地将劳动者生吞活剥再榨干。资本家用货币购买人力物力生产商品，再将商品出售获取货币，简单的说就是用钱生钱，同时将钱再用于购买人力物力扩大生产，循环往复。中间的差价即资本所赚取的利润，人力越便宜，利润值越高，于是想方设法压低人力榨取剩余价值就成了资本家最朝思暮想的事，比如996工资一定会比理应要低，毕竟你要和资本家对线，那资本家一定不会亏，他们只希望你保留最基本的生存以供它持续压榨。同时为了让劳动者甘于被压榨，就需要设置起他们难以支付的刚需，比如高昂得需要工作一辈子才能还完的房贷。资本的不断扩张会导致人力的紧缺，为使人力重新轻贱下来，资本家无疑希望人口过剩。人口过剩导致劳动力价格下跌，同时产生更多的商品需求，进一步推动资本的再生产。即劳动者内卷越严重，资本家剥削压迫起来越厉害，越有底气。 《我是猫》 《时间简史》 《书店日记》 Earlier《一个陌生女人的来信》被茨威格圈粉了，故事特别感性细腻，不愧被称为“最懂女性的作家”。这是个中短篇，建议一口气读完，一个人沉浸在那些浮掠的狭小剪影缝隙里，感受关于爱情那种纯粹的心灵上的震撼。可以和《少年维特的烦恼》做个对比，完全不同的风味，好比一个是踏在中欧古堡旋转的楼梯，一个是走在乡下小镇的田园小道。 《断头王后》支教时借千荀kindle看完的，茨威格的传记，当时觉得玛丽 安托瓦内特真的是火一样炙烈的女子，可能是受FGO影响，总是有尼禄的代入感。最经典的，“她那时还太年轻，不知道所有命运赠予的礼物，早已在暗中标好了价格”。作为篇传记，可能会和想象的有些差别，它没有什么宏大的视角或者以小见大的缩影，至始至终主观追随着玛丽一人，能感受出茨威格这篇传记其实是夹了私货的。如果撇开她的王后身份，她可能会出彩地过完她浪漫炙热的一生；同时也正因她的身份，她才可以过早而轻易地得到她想要的一切。不过撇开这个身份，今天也不会有这段话的存在了。 《娱乐至死》支教结束后在西宁看完的，倒是记得一个人跑去西宁最高的电视塔顶的旋转餐厅，点上小食茶水，一边看书一边等日落（因为有最低消费必须要恰晚饭）蛮有意思的一本书：你能看到的听到的，一切都刻意筛选过，打上滤镜，你不在意真相，毕竟只是娱乐；你也无法一一去探寻真像，你只是个普通人，没有上帝视角。同时你还是个么得智商么得脑子的工具人，一个巨大的电视把你包裹进去，不断给你快速更新的、喜闻乐见的刺激，你连鱼的记忆都不如，只会嘻嘻哈哈就完事了。只要你的目光无时无刻不被新的刺激占据，他们就赢了。 《乌合之众》人一旦融入群体就会狂掉智商，感性的煽动会被数倍放大，而把理性的气焰踩灭在脚下，可能这是群居生物的本能or特性？我一直猜测放弃思考追随群体释放兽性发泄欲望是刻在人类DNA里的，人一旦有机会就不会错过这几件事。平时仅存的那点理智只是为了自保，不断地压抑和克制，直到有一天融入群体，风险被分散的那一刻起，理性也被消融得差不多，他们主动抛掉了理智，放弃了思考，服从于别人，顺从于欲望，可能服从和臣服也被刻进了本能里，相比于自由更倾向于当奴才的欲望吧。 《哈姆雷特》本来我只徒有其表了解点常识性剧情就敬而远之的，直到大一英语课我从原著中挑合适的情节作剧本，班级演出作期中考核。因时长考虑要删减剧本，当把目光从情节放到文字上，才感受到莎士比亚惊心动魄的勾人魔力。那时候我看了很多版本的演绎，电影、戏剧，心思一点一滴都被勾在演出上，起承转合，酣畅淋漓。让我意识到了戏剧的魅力，也真正对这部剧有了自己的认知和理解。 《旅行的意义》特别安静的一本书，但不会像《瓦尔登湖》那么静（最佳睡前读物提名）我想，旅途中翻开这本书，偶尔再看看窗外，一定非常应景愉悦。 《解忧杂货店》那一年我的某份生日礼物，像夜晚庭屋的风铃声，静谧美好。 《1984》因《美丽新世界》入反乌托邦的坑，相比而言，1984直观上有科技感多了，但氛围也更加冰冷。 《谈美》朱光潜先生的美学启蒙作品，吃了高中语文课本书后的安利。读来就像收到朋友的来信一样，倍感温和亲切，虽然书中所述我不是全都赞同，但这本和朱光潜先生的另一本书《给青年的十二封信》，为高中的我解了不少困惑，作为一本美学入门书，真的是很推荐的。 Film List记录下我印象深刻、难以忘怀的电影，也是从2020开始。 2020《小妇人》感性细腻得像一幅中世纪的画卷，在眼前直铺开来，如果以后我有女儿，一定会送给她这部原著。 《安尼亚拉号》诺奖作品《Aniara》改编，原作是一首一首诗连起来的科幻故事。本来是想快进看完的科幻消遣，结果被切段的时间怔住了。过于真实的发展=过于压抑的现实，越往后看越心冷，最后一串数字浮现我人都没了，想缓会儿又被片尾曲拉回去，这怕是部恐怖片吧。不过对比疫情下的自己，真的逼人思考一些东西。 Earlier《寻梦环游记》本来感触不深，直到去美国交流时候接触到拉丁文化，才在心里刻下了重重一笔。 《爱乐之城》真的爱死了la la land这种梦幻感性带点慵懒的节奏氛围和爱情，就算最后泡沫炸了也能掀起水花滋润下我这颗渴求浪漫梦幻的心嘛。 《若能与你共乘海浪之上》清澈简洁的爱情童话，这种干净清新的作画和风格特别勾我，合唱的主题曲虐哭我 。 《降临》冷色调的科幻童话，配合音乐有种飘飘然的虚幻美感，电影结束甚至有梦醒的错觉。 《朗读者》给我印象至深，前半段几近想放弃，但被后半段压得死死的，把矛盾赤裸裸揭开，对立双方都能得到第三方视角的理解，但这不代表能得到解决，只能眼睁睁看到那些东西一步步碎开，看完后压抑难受了半天。 《楚门的世界》如果人生真的是别人写死的定论，希望你有勇气打破第四堵墙。 《傲慢与偏见》真的是美感爆棚又充满罗曼蒂克氛围的美好电影，让我吹一下女主的颜。 《大鱼》有时候刻板的真相没有那么重要，藏在心照不宣的故事和谎言里的深情才是真正支撑我们走下去的美好。 《彗星来的那一夜》彗星来之前，什么东西啊，杂乱日记本吗？彗星来之后，嗯？有点意思……我靠牛批啊。但为什么要在私人影院看剧情片，看名字谁能想到这是个剧情片啊，每每想到都觉得血亏。]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>大一</tag>
        <tag>随记</tag>
      </tags>
  </entry>
</search>
